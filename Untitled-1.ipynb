{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 — Age Regression con MLP “from scratch” + SciPy\n",
    "\n",
    "Obiettivo: minimizzare\n",
    "E(ω) = (1/N) Σ_i (y_i - ŷ_i)^2 + λ Σ_l ||W^(l)||^2\n",
    "senza strumenti di automatic differentiation. L’ottimizzazione è full‑batch con SciPy (L‑BFGS‑B).\n",
    "\n",
    "Perché questa scelta:\n",
    "- L‑BFGS‑B è efficiente per problemi con numero di parametri medio‑grande e loss liscia (tanh/sigmoid); funziona bene in full‑batch.\n",
    "- Backprop manuale: calcoliamo i gradienti analitici di MSE + L2 su pesi (niente autograd).\n",
    "- Standardizzazione X e (opzionale) y riducono malcondizionamento, favorendo convergenza.\n",
    "- K‑fold CV per selezionare L (2..4), width, λ e activation (tanh/relu), con metrica MAPE media sui fold (come richiesto dal testo per il tracciamento delle performance).\n",
    "\n",
    "Architettura:\n",
    "- Input: vettori di feature (ResNet) da `AGE_REGRESSION.csv`.\n",
    "- Hidden: L-1 strati con stessa width (hyperparam). Attivazioni: `tanh` o `relu` (derivate incluse).\n",
    "- Output: 1 neurone lineare (regressione).\n",
    "- Inizializzazione: Xavier per tanh/sigmoid, He per ReLU.\n",
    "\n",
    "Forward:\n",
    "- z_l = a_{l-1} W_l + b_l\n",
    "- a_l = φ(z_l) per hidden, a_L = z_L (lineare)\n",
    "\n",
    "Loss:\n",
    "- data_loss = mean((ŷ - y)^2)\n",
    "- reg_loss = λ Σ ||W_l||^2 (no bias)\n",
    "- totale = data_loss + reg_loss\n",
    "\n",
    "Backward:\n",
    "- δ_L = dE/dyhat = 2/N (ŷ - y)\n",
    "- δ_l = (δ_{l+1} W_{l+1}^T) ⊙ φ'(z_l)\n",
    "- ∂E/∂W_l = a_{l-1}^T δ_l + 2λ W_l\n",
    "- ∂E/∂b_l = Σ_n δ_l\n",
    "\n",
    "Ottimizzazione:\n",
    "- SciPy `minimize(..., method=\"L-BFGS-B\", jac=True)` con callback che salva objective e norma del gradiente (grafico `train_history.png`).\n",
    "- Convergenza controllata da `gtol`.\n",
    "\n",
    "Cross‑Validation:\n",
    "- `kfold_indices` semplice con shuffle (seed fisso).\n",
    "- Grid su {L, width, λ, activation}; per ogni combinazione si addestra su K-1 fold e si valuta su 1 fold; media MAPE valida il best setting.\n",
    "- Salviamo `cv_results.json` e un bar‑plot ordinato (`cv_results.png`).\n",
    "\n",
    "Metriche:\n",
    "- MSE e MAPE (%) con epsilon sul denominatore per gestire target 0.\n",
    "\n",
    "Output “catchy”:\n",
    "- `train_history.png`: andamento objective e ||grad|| (aiuta a raccontare l’ottimizzazione).\n",
    "- `scatter_train.png` e `scatter_test.png`: Predetto vs Vero con bisettrice.\n",
    "- `cv_results.png`: confronto immediato degli iperparametri.\n",
    "\n",
    "Riproducibilità:\n",
    "- Seed globale su splits, inizializzazioni e CV.\n",
    "\n",
    "Come lanciare:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "python scripts/train_mlp_age_regression.py --data ./AGE_REGRESSION.csv --k 5 --test-size 0.2 --seed 42 --scale-y\n",
    "```\n",
    "Flag utili:\n",
    "- `--L 2 3 4` `--width 64 128 256` `--l2 0.0 0.0001 0.001 0.01` `--activation tanh relu`\n",
    "- `--max-iter 800` `--tol 1e-6`\n",
    "\n",
    "Note pratiche:\n",
    "- `relu` è veloce ma non liscia in 0; per L‑BFGS di solito `tanh` converge più regolare. La grid decide.\n",
    "- `--scale-y` spesso migliora la stabilità numerica; per la MAPE riportiamo sempre nella scala originale (inverse transform).\n",
    "- MAPE penalizza molto errori relativi con età piccole; l’epsilon evita divisioni per zero ma va menzionato nel report.\n",
    "\n",
    "Estensioni immediate:\n",
    "- Early‑stopping non è standard con L‑BFGS; si può basare su valid MAPE e salvare il best theta per fold.\n",
    "- Width per layer non tutti uguali: generalizzabile aggiungendo un parser “128x64x32”.\n",
    "- Aggiungere gradient‑check a campione (finite differences) per debugging nel report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column: gt\n",
      "AUDIT: {\n",
      "  \"rows\": 20475,\n",
      "  \"cols\": 33,\n",
      "  \"nan_total\": 0,\n",
      "  \"inf_total\": 0,\n",
      "  \"duplicated_rows\": 169,\n",
      "  \"y_min\": 10.0,\n",
      "  \"y_max\": 89.0,\n",
      "  \"y_out_of_[0,100]\": 0,\n",
      "  \"zero_var_features\": []\n",
      "}\n",
      "AUDIT: {\n",
      "  \"rows\": 20306,\n",
      "  \"cols\": 33,\n",
      "  \"nan_total\": 0,\n",
      "  \"inf_total\": 0,\n",
      "  \"duplicated_rows\": 0,\n",
      "  \"y_min\": 10.0,\n",
      "  \"y_max\": 89.0,\n",
      "  \"y_out_of_[0,100]\": 0,\n",
      "  \"zero_var_features\": []\n",
      "}\n",
      "CLEANING_LOG: {\n",
      "  \"rows_removed_invalid_y\": 0,\n",
      "  \"imputation\": \"median_per_feature\",\n",
      "  \"duplicates_removed\": 169,\n",
      "  \"zero_var_features_dropped\": [],\n",
      "  \"feature_cols_kept\": [\n",
      "    \"feat_1\",\n",
      "    \"feat_2\",\n",
      "    \"feat_3\",\n",
      "    \"feat_4\",\n",
      "    \"feat_5\",\n",
      "    \"feat_6\",\n",
      "    \"feat_7\",\n",
      "    \"feat_8\",\n",
      "    \"feat_9\",\n",
      "    \"feat_10\",\n",
      "    \"feat_11\",\n",
      "    \"feat_12\",\n",
      "    \"feat_13\",\n",
      "    \"feat_14\",\n",
      "    \"feat_15\",\n",
      "    \"feat_16\",\n",
      "    \"feat_17\",\n",
      "    \"feat_18\",\n",
      "    \"feat_19\",\n",
      "    \"feat_20\",\n",
      "    \"feat_21\",\n",
      "    \"feat_22\",\n",
      "    \"feat_23\",\n",
      "    \"feat_24\",\n",
      "    \"feat_25\",\n",
      "    \"feat_26\",\n",
      "    \"feat_27\",\n",
      "    \"feat_28\",\n",
      "    \"feat_29\",\n",
      "    \"feat_30\",\n",
      "    \"feat_31\",\n",
      "    \"feat_32\"\n",
      "  ]\n",
      "}\n",
      "Dati puliti pronti: N=20306, D=32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "CSV_PATH = \"AGE_PREDICTION.csv\"  # modificare se necessario\n",
    "\n",
    "def load_csv_auto(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    # Coerce numerico (stringhe -> NaN)\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def detect_target_column(df: pd.DataFrame) -> str:\n",
    "    return \"gt\" if \"gt\" in df.columns else df.columns[-1]\n",
    "\n",
    "def audit_dataframe(df: pd.DataFrame, target_col: str) -> dict:\n",
    "    n, d = df.shape\n",
    "    y = df[target_col].to_numpy(dtype=float)\n",
    "    nan_total = int(df.isna().sum().sum())\n",
    "    inf_total = int(np.isinf(df.to_numpy(dtype=float)).sum())\n",
    "    dup_rows = int(df.duplicated().sum())\n",
    "    y_min = float(np.nanmin(y)) if n > 0 else float(\"nan\")\n",
    "    y_max = float(np.nanmax(y)) if n > 0 else float(\"nan\")\n",
    "    out_of_range = int(np.sum((y < 0) | (y > 100)))\n",
    "    zero_var_features = [c for c in df.columns if c != target_col and df[c].std(skipna=True) == 0]\n",
    "    summary = {\n",
    "        \"rows\": n, \"cols\": d,\n",
    "        \"nan_total\": nan_total,\n",
    "        \"inf_total\": inf_total,\n",
    "        \"duplicated_rows\": dup_rows,\n",
    "        \"y_min\": y_min, \"y_max\": y_max,\n",
    "        \"y_out_of_[0,100]\": out_of_range,\n",
    "        \"zero_var_features\": zero_var_features,\n",
    "    }\n",
    "    print(\"AUDIT:\", json.dumps(summary, indent=2))\n",
    "    return summary\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame, target_col: str, impute: bool = True) -> tuple[pd.DataFrame, dict]:\n",
    "    log: dict = {}\n",
    "    # Inf -> NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    # Drop righe con y NaN o fuori range [0,100]\n",
    "    before = len(df)\n",
    "    df = df[df[target_col].notna()]\n",
    "    df = df[(df[target_col] >= 0) & (df[target_col] <= 100)]\n",
    "    log[\"rows_removed_invalid_y\"] = int(before - len(df))\n",
    "    # Imputazione feature NaN con mediana (robusta, veloce)\n",
    "    feat_cols = [c for c in df.columns if c != target_col]\n",
    "    if impute:\n",
    "        med = df[feat_cols].median()\n",
    "        df[feat_cols] = df[feat_cols].fillna(med)\n",
    "        log[\"imputation\"] = \"median_per_feature\"\n",
    "    else:\n",
    "        before = len(df)\n",
    "        df = df.dropna(subset=feat_cols)\n",
    "        log[\"rows_removed_nan_features\"] = int(before - len(df))\n",
    "        log[\"imputation\"] = \"none_drop_rows\"\n",
    "    # Drop duplicati esatti\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    log[\"duplicates_removed\"] = int(before - len(df))\n",
    "    # Drop feature a varianza zero\n",
    "    zero_var = [c for c in feat_cols if df[c].std(skipna=True) == 0]\n",
    "    if zero_var:\n",
    "        df = df.drop(columns=zero_var)\n",
    "    log[\"zero_var_features_dropped\"] = zero_var\n",
    "    # Report colonne usate\n",
    "    log[\"feature_cols_kept\"] = [c for c in df.columns if c != target_col]\n",
    "    return df.reset_index(drop=True), log\n",
    "\n",
    "# ---- Esecuzione: load -> audit -> clean -> audit ----\n",
    "df_raw = load_csv_auto(CSV_PATH)\n",
    "target_col = detect_target_column(df_raw)\n",
    "print(f\"Target column: {target_col}\")\n",
    "\n",
    "audit_before = audit_dataframe(df_raw, target_col)\n",
    "df_clean, clean_log = clean_dataframe(df_raw, target_col, impute=True)\n",
    "audit_after = audit_dataframe(df_clean, target_col)\n",
    "\n",
    "print(\"CLEANING_LOG:\", json.dumps(clean_log, indent=2))\n",
    "\n",
    "# ---- Output numpy arrays ----\n",
    "y = df_clean[target_col].to_numpy(dtype=float)\n",
    "X = df_clean.drop(columns=[target_col]).to_numpy(dtype=float)\n",
    "n, d = X.shape\n",
    "print(f\"Dati puliti pronti: N={n}, D={d}\")\n",
    "\n",
    "# X, y, df_clean, audit_before, audit_after, clean_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, root)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Ora importa dai TUOI moduli ma con il prefisso del package\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_age_regression_csv, StandardScaler, ScalarStandardizer, train_test_split\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MLPRegressor\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m grid_search_cv, build_hidden_layers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "# Assicurati che la root che contiene 'dataset/' sia nel sys.path (in genere già vero in Jupyter)\n",
    "import sys, os\n",
    "root = os.getcwd()  # cartella del notebook; deve contenere la cartella 'dataset'\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)\n",
    "\n",
    "# Ora importa dai TUOI moduli ma con il prefisso del package\n",
    "from dataset.data import load_age_regression_csv, StandardScaler, ScalarStandardizer, train_test_split\n",
    "from dataset.model import MLPRegressor\n",
    "from dataset.cv import grid_search_cv, build_hidden_layers\n",
    "from dataset.metrics import mse, mape\n",
    "from dataset.plotting import plot_training_history, plot_scatter_pred_vs_true, plot_cv_bars\n",
    "\n",
    "print(\"Import OK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
