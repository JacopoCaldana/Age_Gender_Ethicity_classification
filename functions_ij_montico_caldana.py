# -*- coding: utf-8 -*-
"""Functions_ij_Montico_Caldana.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AYc926gn3K49dp0fvOoRngDlm51TzF82
"""

import numpy as np

class MLP_Regression:
    def __init__(self, input_size, hidden_layers, output_size, lambda_reg, activation='tanh'):
        """
        Multi-Layer Perceptron for Regression.
        Implements manual gradient calculation for L2 Regularized Loss.
        """
        self.input_size = input_size
        # Layer sizes includes Input, Hidden Layers, and Output
        self.layer_sizes = [input_size] + hidden_layers + [output_size]
        self.lambda_reg = lambda_reg
        self.activation_name = activation

        # --- SMART INITIALIZATION ---
        self.params = []
        for i in range(len(self.layer_sizes) - 1):
            n_in = self.layer_sizes[i]
            n_out = self.layer_sizes[i+1]

            if self.activation_name == 'relu':
                # He Initialization (Optimal for ReLU)
                # Std = sqrt(2 / n_in)
                std = np.sqrt(2.0 / n_in)
                W = np.random.normal(0.0, std, (n_in, n_out))

            else:
                # Xavier/Glorot Initialization (Optimal for Tanh/Sigmoid)
                # Uniform distribution within [-limit, limit]
                limit = np.sqrt(6.0 / (n_in + n_out))
                W = np.random.uniform(-limit, limit, (n_in, n_out))

            # Biases initialized to zero
            b = np.zeros((1, n_out))

            self.params.append(W)
            self.params.append(b)

    def get_activation(self, Z):
        if self.activation_name == 'tanh':
            return np.tanh(Z)
        elif self.activation_name == 'relu':
            return np.maximum(0, Z)
        elif self.activation_name == 'sigmoid':
            return 1 / (1 + np.exp(-Z))
        return Z

    def get_activation_derivative(self, A):
        if self.activation_name == 'tanh':
            return 1 - np.power(A, 2)
        elif self.activation_name == 'relu':
            return (A > 0).astype(float)
        elif self.activation_name == 'sigmoid':
            return A * (1 - A)
        return np.ones_like(A)

    def flatten_params(self):
        return np.concatenate([p.ravel() for p in self.params])

    def unflatten_params(self, flat_params):
        params = []
        start = 0
        for i in range(len(self.layer_sizes) - 1):
            n_in = self.layer_sizes[i]
            n_out = self.layer_sizes[i+1]
            w_size = n_in * n_out
            W = flat_params[start : start + w_size].reshape((n_in, n_out))
            start += w_size
            b_size = n_out
            b = flat_params[start : start + b_size].reshape((1, n_out))
            start += b_size
            params.append(W)
            params.append(b)
        return params

    def forward(self, X, params=None):
        if params is None:
            params = self.params
        else:
            params = self.unflatten_params(params)
        activations = [X]
        current_input = X
        num_layers = len(self.layer_sizes) - 1

        for i in range(num_layers):
            W = params[2*i]
            b = params[2*i+1]
            Z = np.dot(current_input, W) + b

            # Apply activation to Hidden Layers only
            if i < num_layers - 1:
                A = self.get_activation(Z)
            else:
                A = Z # Linear output for regression

            activations.append(A)
            current_input = A
        return activations, params

    def cost_function_and_grads(self, flat_params, X, y):
        activations, params_struct = self.forward(X, flat_params)
        y_hat = activations[-1]
        N = X.shape[0]

        # Loss (MSE + L2)
        mse_loss = np.mean((y - y_hat) ** 2)

        reg_loss = 0
        for i in range(0, len(params_struct), 2): # Regularize Weights (W) only
            reg_loss += np.sum(params_struct[i] ** 2)

        total_loss = mse_loss + self.lambda_reg * reg_loss

        # Gradients (Backpropagation)
        grads = []
        delta = (2 / N) * (y_hat - y)

        num_layers = len(self.layer_sizes) - 1

        for i in reversed(range(num_layers)):
            W = params_struct[2*i]
            A_prev = activations[i]

            dW = np.dot(A_prev.T, delta) + (2 * self.lambda_reg * W)
            db = np.sum(delta, axis=0, keepdims=True)

            grads.insert(0, db)
            grads.insert(0, dW)

            if i > 0:
                d_act = self.get_activation_derivative(A_prev)
                delta = np.dot(delta, W.T) * d_act

        return total_loss, np.concatenate([g.ravel() for g in grads])

def calculate_mape(y_true, y_pred):
    epsilon = 1e-10
    return 100 * np.mean(np.abs((y_true - y_pred) / (y_true + epsilon)))