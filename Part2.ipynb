{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:\n",
    "* Caricamento \n",
    "* Analisi\n",
    "* Conversione per target duale\n",
    "* Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.900846</td>\n",
       "      <td>0.102587</td>\n",
       "      <td>-0.397814</td>\n",
       "      <td>0.112796</td>\n",
       "      <td>2.588096</td>\n",
       "      <td>-0.192754</td>\n",
       "      <td>-0.968311</td>\n",
       "      <td>-0.490886</td>\n",
       "      <td>-0.872099</td>\n",
       "      <td>-0.288411</td>\n",
       "      <td>...</td>\n",
       "      <td>2.541431</td>\n",
       "      <td>1.739102</td>\n",
       "      <td>0.166066</td>\n",
       "      <td>4.584869</td>\n",
       "      <td>-0.107031</td>\n",
       "      <td>-0.913990</td>\n",
       "      <td>-0.686416</td>\n",
       "      <td>-0.368085</td>\n",
       "      <td>-0.870545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.838868</td>\n",
       "      <td>0.039976</td>\n",
       "      <td>-0.387101</td>\n",
       "      <td>0.055413</td>\n",
       "      <td>2.066874</td>\n",
       "      <td>-0.226948</td>\n",
       "      <td>-0.947416</td>\n",
       "      <td>-0.472817</td>\n",
       "      <td>-0.855387</td>\n",
       "      <td>-0.207101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.991721</td>\n",
       "      <td>1.259745</td>\n",
       "      <td>0.065058</td>\n",
       "      <td>3.019790</td>\n",
       "      <td>-0.110633</td>\n",
       "      <td>-0.890023</td>\n",
       "      <td>-0.611625</td>\n",
       "      <td>-0.298235</td>\n",
       "      <td>-0.855208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.814961</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>-0.397147</td>\n",
       "      <td>0.092713</td>\n",
       "      <td>1.897454</td>\n",
       "      <td>-0.269387</td>\n",
       "      <td>-0.945285</td>\n",
       "      <td>-0.449579</td>\n",
       "      <td>-0.849705</td>\n",
       "      <td>-0.151179</td>\n",
       "      <td>...</td>\n",
       "      <td>1.822978</td>\n",
       "      <td>1.105511</td>\n",
       "      <td>0.065353</td>\n",
       "      <td>2.500681</td>\n",
       "      <td>-0.052730</td>\n",
       "      <td>-0.885691</td>\n",
       "      <td>-0.583346</td>\n",
       "      <td>-0.218140</td>\n",
       "      <td>-0.856456</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.110470</td>\n",
       "      <td>0.027849</td>\n",
       "      <td>-0.044310</td>\n",
       "      <td>-0.005343</td>\n",
       "      <td>0.177831</td>\n",
       "      <td>-0.232092</td>\n",
       "      <td>-0.562700</td>\n",
       "      <td>-0.400713</td>\n",
       "      <td>-0.552356</td>\n",
       "      <td>0.037349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098367</td>\n",
       "      <td>-0.370318</td>\n",
       "      <td>-0.123008</td>\n",
       "      <td>-0.861314</td>\n",
       "      <td>0.106840</td>\n",
       "      <td>-0.483669</td>\n",
       "      <td>-0.224164</td>\n",
       "      <td>0.147321</td>\n",
       "      <td>-0.615051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.626313</td>\n",
       "      <td>-0.091985</td>\n",
       "      <td>-0.373756</td>\n",
       "      <td>-0.005083</td>\n",
       "      <td>1.172486</td>\n",
       "      <td>-0.314868</td>\n",
       "      <td>-0.885046</td>\n",
       "      <td>-0.412587</td>\n",
       "      <td>-0.818729</td>\n",
       "      <td>-0.012022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.030348</td>\n",
       "      <td>0.421886</td>\n",
       "      <td>-0.068029</td>\n",
       "      <td>0.258984</td>\n",
       "      <td>-0.057158</td>\n",
       "      <td>-0.834079</td>\n",
       "      <td>-0.441066</td>\n",
       "      <td>-0.099874</td>\n",
       "      <td>-0.829539</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.826631</td>\n",
       "      <td>0.031593</td>\n",
       "      <td>0.266672</td>\n",
       "      <td>-0.347612</td>\n",
       "      <td>-0.639248</td>\n",
       "      <td>0.340627</td>\n",
       "      <td>1.614477</td>\n",
       "      <td>0.483605</td>\n",
       "      <td>0.995385</td>\n",
       "      <td>-0.196342</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.661917</td>\n",
       "      <td>-0.498242</td>\n",
       "      <td>-0.010401</td>\n",
       "      <td>-0.689971</td>\n",
       "      <td>-0.100364</td>\n",
       "      <td>1.194464</td>\n",
       "      <td>0.184993</td>\n",
       "      <td>0.036553</td>\n",
       "      <td>0.949231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.963062</td>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.254258</td>\n",
       "      <td>-0.392125</td>\n",
       "      <td>-0.696457</td>\n",
       "      <td>0.429470</td>\n",
       "      <td>1.911828</td>\n",
       "      <td>0.625015</td>\n",
       "      <td>1.223570</td>\n",
       "      <td>-0.205733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.701267</td>\n",
       "      <td>-0.500958</td>\n",
       "      <td>-0.013814</td>\n",
       "      <td>-0.636859</td>\n",
       "      <td>-0.137050</td>\n",
       "      <td>1.428118</td>\n",
       "      <td>0.218145</td>\n",
       "      <td>0.017752</td>\n",
       "      <td>1.211529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.780911</td>\n",
       "      <td>-0.070232</td>\n",
       "      <td>0.276407</td>\n",
       "      <td>-0.558690</td>\n",
       "      <td>-0.886186</td>\n",
       "      <td>0.918438</td>\n",
       "      <td>3.540307</td>\n",
       "      <td>1.309418</td>\n",
       "      <td>2.546124</td>\n",
       "      <td>-0.276382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.838080</td>\n",
       "      <td>-0.503847</td>\n",
       "      <td>0.097946</td>\n",
       "      <td>-0.214809</td>\n",
       "      <td>-0.257925</td>\n",
       "      <td>2.670643</td>\n",
       "      <td>0.408064</td>\n",
       "      <td>-0.022937</td>\n",
       "      <td>2.525592</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1.168703</td>\n",
       "      <td>-0.024039</td>\n",
       "      <td>0.271217</td>\n",
       "      <td>-0.418862</td>\n",
       "      <td>-0.759834</td>\n",
       "      <td>0.560393</td>\n",
       "      <td>2.332132</td>\n",
       "      <td>0.760089</td>\n",
       "      <td>1.564775</td>\n",
       "      <td>-0.211580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.745087</td>\n",
       "      <td>-0.493854</td>\n",
       "      <td>0.069935</td>\n",
       "      <td>-0.557275</td>\n",
       "      <td>-0.160119</td>\n",
       "      <td>1.710489</td>\n",
       "      <td>0.272429</td>\n",
       "      <td>0.025845</td>\n",
       "      <td>1.510238</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.145126</td>\n",
       "      <td>0.025884</td>\n",
       "      <td>0.114698</td>\n",
       "      <td>-0.093961</td>\n",
       "      <td>-0.157402</td>\n",
       "      <td>-0.089719</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>-0.164875</td>\n",
       "      <td>-0.223452</td>\n",
       "      <td>-0.092394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.384568</td>\n",
       "      <td>-0.474988</td>\n",
       "      <td>-0.153925</td>\n",
       "      <td>-0.837694</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>-0.060032</td>\n",
       "      <td>-0.059799</td>\n",
       "      <td>0.076589</td>\n",
       "      <td>-0.244711</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0   -0.900846  0.102587 -0.397814  0.112796  2.588096 -0.192754 -0.968311   \n",
       "1   -0.838868  0.039976 -0.387101  0.055413  2.066874 -0.226948 -0.947416   \n",
       "2   -0.814961 -0.010184 -0.397147  0.092713  1.897454 -0.269387 -0.945285   \n",
       "3   -0.110470  0.027849 -0.044310 -0.005343  0.177831 -0.232092 -0.562700   \n",
       "4   -0.626313 -0.091985 -0.373756 -0.005083  1.172486 -0.314868 -0.885046   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.826631  0.031593  0.266672 -0.347612 -0.639248  0.340627  1.614477   \n",
       "996  0.963062  0.017975  0.254258 -0.392125 -0.696457  0.429470  1.911828   \n",
       "997  1.780911 -0.070232  0.276407 -0.558690 -0.886186  0.918438  3.540307   \n",
       "998  1.168703 -0.024039  0.271217 -0.418862 -0.759834  0.560393  2.332132   \n",
       "999  0.145126  0.025884  0.114698 -0.093961 -0.157402 -0.089719  0.003851   \n",
       "\n",
       "       feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26  \\\n",
       "0   -0.490886 -0.872099 -0.288411  ...  2.541431  1.739102  0.166066   \n",
       "1   -0.472817 -0.855387 -0.207101  ...  1.991721  1.259745  0.065058   \n",
       "2   -0.449579 -0.849705 -0.151179  ...  1.822978  1.105511  0.065353   \n",
       "3   -0.400713 -0.552356  0.037349  ... -0.098367 -0.370318 -0.123008   \n",
       "4   -0.412587 -0.818729 -0.012022  ...  1.030348  0.421886 -0.068029   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.483605  0.995385 -0.196342  ... -0.661917 -0.498242 -0.010401   \n",
       "996  0.625015  1.223570 -0.205733  ... -0.701267 -0.500958 -0.013814   \n",
       "997  1.309418  2.546124 -0.276382  ... -0.838080 -0.503847  0.097946   \n",
       "998  0.760089  1.564775 -0.211580  ... -0.745087 -0.493854  0.069935   \n",
       "999 -0.164875 -0.223452 -0.092394  ... -0.384568 -0.474988 -0.153925   \n",
       "\n",
       "      feat_27   feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0    4.584869 -0.107031 -0.913990 -0.686416 -0.368085 -0.870545   0  \n",
       "1    3.019790 -0.110633 -0.890023 -0.611625 -0.298235 -0.855208   0  \n",
       "2    2.500681 -0.052730 -0.885691 -0.583346 -0.218140 -0.856456   0  \n",
       "3   -0.861314  0.106840 -0.483669 -0.224164  0.147321 -0.615051   0  \n",
       "4    0.258984 -0.057158 -0.834079 -0.441066 -0.099874 -0.829539   0  \n",
       "..        ...       ...       ...       ...       ...       ...  ..  \n",
       "995 -0.689971 -0.100364  1.194464  0.184993  0.036553  0.949231   1  \n",
       "996 -0.636859 -0.137050  1.428118  0.218145  0.017752  1.211529   1  \n",
       "997 -0.214809 -0.257925  2.670643  0.408064 -0.022937  2.525592   1  \n",
       "998 -0.557275 -0.160119  1.710489  0.272429  0.025845  1.510238   1  \n",
       "999 -0.837694  0.008155 -0.060032 -0.059799  0.076589 -0.244711   1  \n",
       "\n",
       "[1000 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path= '/Users/jacopocaldana/Desktop/Università/Optimization/final_project/dataset/GENDER_CLASSIFICATION.csv'\n",
    "df=pd.read_csv('/Users/jacopocaldana/Desktop/Università/Optimization/final_project/dataset/GENDER_CLASSIFICATION.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Info Dataset ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 33 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   feat_1   1000 non-null   float64\n",
      " 1   feat_2   1000 non-null   float64\n",
      " 2   feat_3   1000 non-null   float64\n",
      " 3   feat_4   1000 non-null   float64\n",
      " 4   feat_5   1000 non-null   float64\n",
      " 5   feat_6   1000 non-null   float64\n",
      " 6   feat_7   1000 non-null   float64\n",
      " 7   feat_8   1000 non-null   float64\n",
      " 8   feat_9   1000 non-null   float64\n",
      " 9   feat_10  1000 non-null   float64\n",
      " 10  feat_11  1000 non-null   float64\n",
      " 11  feat_12  1000 non-null   float64\n",
      " 12  feat_13  1000 non-null   float64\n",
      " 13  feat_14  1000 non-null   float64\n",
      " 14  feat_15  1000 non-null   float64\n",
      " 15  feat_16  1000 non-null   float64\n",
      " 16  feat_17  1000 non-null   float64\n",
      " 17  feat_18  1000 non-null   float64\n",
      " 18  feat_19  1000 non-null   float64\n",
      " 19  feat_20  1000 non-null   float64\n",
      " 20  feat_21  1000 non-null   float64\n",
      " 21  feat_22  1000 non-null   float64\n",
      " 22  feat_23  1000 non-null   float64\n",
      " 23  feat_24  1000 non-null   float64\n",
      " 24  feat_25  1000 non-null   float64\n",
      " 25  feat_26  1000 non-null   float64\n",
      " 26  feat_27  1000 non-null   float64\n",
      " 27  feat_28  1000 non-null   float64\n",
      " 28  feat_29  1000 non-null   float64\n",
      " 29  feat_30  1000 non-null   float64\n",
      " 30  feat_31  1000 non-null   float64\n",
      " 31  feat_32  1000 non-null   float64\n",
      " 32  gt       1000 non-null   int64  \n",
      "dtypes: float64(32), int64(1)\n",
      "memory usage: 257.9 KB\n",
      "None\n",
      "\n",
      "--- Head ---\n",
      "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
      "0 -0.900846  0.102587 -0.397814  0.112796  2.588096 -0.192754 -0.968311   \n",
      "1 -0.838868  0.039976 -0.387101  0.055413  2.066874 -0.226948 -0.947416   \n",
      "2 -0.814961 -0.010184 -0.397147  0.092713  1.897454 -0.269387 -0.945285   \n",
      "3 -0.110470  0.027849 -0.044310 -0.005343  0.177831 -0.232092 -0.562700   \n",
      "4 -0.626313 -0.091985 -0.373756 -0.005083  1.172486 -0.314868 -0.885046   \n",
      "\n",
      "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
      "0 -0.490886 -0.872099 -0.288411  ...  2.541431  1.739102  0.166066  4.584869   \n",
      "1 -0.472817 -0.855387 -0.207101  ...  1.991721  1.259745  0.065058  3.019790   \n",
      "2 -0.449579 -0.849705 -0.151179  ...  1.822978  1.105511  0.065353  2.500681   \n",
      "3 -0.400713 -0.552356  0.037349  ... -0.098367 -0.370318 -0.123008 -0.861314   \n",
      "4 -0.412587 -0.818729 -0.012022  ...  1.030348  0.421886 -0.068029  0.258984   \n",
      "\n",
      "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
      "0 -0.107031 -0.913990 -0.686416 -0.368085 -0.870545   0  \n",
      "1 -0.110633 -0.890023 -0.611625 -0.298235 -0.855208   0  \n",
      "2 -0.052730 -0.885691 -0.583346 -0.218140 -0.856456   0  \n",
      "3  0.106840 -0.483669 -0.224164  0.147321 -0.615051   0  \n",
      "4 -0.057158 -0.834079 -0.441066 -0.099874 -0.829539   0  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "--- Distribuzione Target 'gt' ---\n",
      "gt\n",
      "0    500\n",
      "1    500\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance Ratio: 1.00\n",
      "\n",
      "Dimensioni Features Matrix X: (1000, 32)\n",
      "Dimensioni Target Vector y: (1000,)\n",
      "\n",
      "--- Statistiche Features (Describe) ---\n",
      "             mean       std       min       max\n",
      "feat_1   0.369742  1.193100 -0.970982  3.382229\n",
      "feat_2  -0.026694  0.077630 -0.239500  0.278737\n",
      "feat_3  -0.042664  0.312194 -0.419281  0.302920\n",
      "feat_4  -0.207897  0.280292 -0.761153  0.278908\n",
      "feat_5   0.401260  1.262918 -0.983220  3.850973\n",
      "feat_6   0.220031  0.587774 -0.335672  1.873458\n",
      "feat_7   0.961636  2.172455 -0.991013  6.764083\n",
      "feat_8   0.268475  0.866674 -0.553851  2.684002\n",
      "feat_9   0.572921  1.652436 -0.903364  5.091131\n",
      "feat_10 -0.168164  0.123819 -0.448568  0.141840\n",
      "\n",
      "Totale valori mancanti (NaN): 0\n"
     ]
    }
   ],
   "source": [
    "# 2. Ispezione Struttura\n",
    "print(\"\\n--- Info Dataset ---\")\n",
    "print(df.info())\n",
    "\n",
    "# Verifica prime righe per capire il formato delle colonne 'feat'\n",
    "print(\"\\n--- Head ---\")\n",
    "print(df.head())\n",
    "\n",
    "# 3. Analisi della Variabile Target (ground truth 'gt') [cite: 35]\n",
    "target_col = 'gt'\n",
    "print(f\"\\n--- Distribuzione Target '{target_col}' ---\")\n",
    "class_counts = df[target_col].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Visualizzazione bilanciamento\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=target_col, data=df)\n",
    "plt.title('Distribuzione Classi (Gender)')\n",
    "plt.show()\n",
    "\n",
    "# Calcolo ratio di sbilanciamento\n",
    "imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "# Se < 0.5, potremmo dover considerare pesi nelle classi o metriche diverse dall'accuracy.\n",
    "\n",
    "# 4. Analisi delle Features\n",
    "# Le colonne feature sono quelle che non sono 'gt'\n",
    "feature_cols = [c for c in df.columns if c != target_col]\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values\n",
    "\n",
    "print(f\"\\nDimensioni Features Matrix X: {X.shape}\")\n",
    "print(f\"Dimensioni Target Vector y: {y.shape}\")\n",
    "\n",
    "# Check statistiche descrittive per vedere se serve scaling\n",
    "print(\"\\n--- Statistiche Features (Describe) ---\")\n",
    "print(df[feature_cols].describe().T[['mean', 'std', 'min', 'max']].head(10)) # Mostra solo le prime 10 features\n",
    "\n",
    "# Verifica valori mancanti\n",
    "nan_check = df.isnull().sum().sum()\n",
    "print(f\"\\nTotale valori mancanti (NaN): {nan_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target originali: [0 1]\n",
      "Target convertiti in {-1, 1} per l'ottimizzazione duale.\n",
      "\n",
      "--- Check post-scaling (Prima feature) ---\n",
      "Mean: -0.0000, Std: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# A) Conversione Target per SVM Duale: {0, 1} -> {-1, 1}\n",
    "# Verifichiamo i valori attuali\n",
    "unique_targets = np.unique(y)\n",
    "print(f\"Target originali: {unique_targets}\")\n",
    "\n",
    "if set(unique_targets) == {0, 1}:\n",
    "    y_svm = np.where(y == 0, -1, 1)\n",
    "    print(\"Target convertiti in {-1, 1} per l'ottimizzazione duale.\")\n",
    "else:\n",
    "    y_svm = y # Assumiamo siano già corretti o richiedano altra logica\n",
    "    print(\"Nessuna conversione target effettuata (verificare).\")\n",
    "\n",
    "# B) Scaling\n",
    "# Per SVM con kernel RBF, StandardScaler (media=0, std=1) è solitamente raccomandato per far si che tutte le feature influenzino in egual misura\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\n--- Check post-scaling (Prima feature) ---\")\n",
    "print(f\"Mean: {np.mean(X_scaled[:, 0]):.4f}, Std: {np.std(X_scaled[:, 0]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementazione "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cvxopt\n",
      "  Downloading cvxopt-1.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Downloading cvxopt-1.3.2-cp310-cp310-macosx_11_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: cvxopt\n",
      "Successfully installed cvxopt-1.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cvxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, y, gamma):\n",
    "    \"\"\"\n",
    "    Calcola il Kernel Gaussiano (RBF) tra due matrici di feature.\n",
    "    K(x, y) = exp(-gamma ||x-y||^2)\n",
    "    Usa l'identità: ||x-y||^2 = ||x||^2 + ||y||^2 - 2x.T @ y\n",
    "    \"\"\"\n",
    "    # Calcolo efficiente della distanza euclidea quadrata usando broadcasting\n",
    "    sq_dists = np.sum(x**2, axis=1).reshape(-1, 1) + np.sum(y**2, axis=1) - 2 * np.dot(x, y.T)\n",
    "    return np.exp(-gamma * sq_dists)\n",
    "\n",
    "def polynomial_kernel(x, y, p):\n",
    "    \"\"\"\n",
    "    Calcola il Kernel Polinomiale.\n",
    "    K(x, y) = (x.T @ y + 1)^p\n",
    "    \"\"\"\n",
    "    return (np.dot(x, y.T) + 1) ** p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold e Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: (800, 32)\n",
      "Test Set: (200, 32)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('gt', axis=1).values\n",
    "y = df['gt'].values\n",
    "\n",
    "# 2. Conversione Target per SVM Duale {0, 1} -> {-1, 1}\n",
    "# Necessario per la formulazione matematica corretta del problema\n",
    "y_svm = np.where(y == 0, -1, 1)\n",
    "\n",
    "# 3. Split Train/Test (Hold-out set)\n",
    "# Teniamo da parte un 20% dei dati per il test finale (Question 2 richiede test accuracy) [cite: 114]\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y_svm, test_size=0.2, random_state=42, stratify=y_svm\n",
    ")\n",
    "\n",
    "print(f\"Training Set: {X_train_full.shape}\")\n",
    "print(f\"Test Set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio Grid Search (5-fold CV)...\n",
      "C=0.1, Gamma=0.01 -> Val Acc: 0.9150\n",
      "C=0.1, Gamma=0.1 -> Val Acc: 0.9175\n",
      "C=0.1, Gamma=1 -> Val Acc: 0.9088\n",
      "C=1, Gamma=0.01 -> Val Acc: 0.9163\n",
      "C=1, Gamma=0.1 -> Val Acc: 0.9138\n",
      "C=1, Gamma=1 -> Val Acc: 0.9100\n",
      "C=10, Gamma=0.01 -> Val Acc: 0.9138\n",
      "C=10, Gamma=0.1 -> Val Acc: 0.9138\n",
      "C=10, Gamma=1 -> Val Acc: 0.8863\n",
      "\n",
      "--- Migliori Iperparametri Trovati ---\n",
      "{'C': 0.1, 'gamma': 0.1}\n",
      "Best Validation Accuracy: 0.9175\n"
     ]
    }
   ],
   "source": [
    "# --- Impostazione Iperparametri ---\n",
    "# Griglia di ricerca (puoi estenderla se necessario)\n",
    "C_values = [0.1, 1, 10]          # Parametro di regolarizzazione\n",
    "gamma_values = [0.01, 0.1, 1]    # Parametro del Kernel Gaussiano\n",
    "# p_values = [2, 3]              # Se usassi il polinomiale\n",
    "\n",
    "kernel_chosen = 'gaussian'\n",
    "k_folds = 5\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "best_acc = 0\n",
    "best_params = {}\n",
    "results = [] # Per salvare i dati per il report\n",
    "\n",
    "print(f\"Inizio Grid Search ({k_folds}-fold CV)...\")\n",
    "\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        fold_accuracies = []\n",
    "        \n",
    "        # Loop sui K-Fold\n",
    "        for train_idx, val_idx in skf.split(X_train_full, y_train_full):\n",
    "            # Split dati per il fold corrente\n",
    "            X_fold_train, X_fold_val = X_train_full[train_idx], X_train_full[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train_full[train_idx], y_train_full[val_idx]\n",
    "            \n",
    "            # --- SCALING (Fondamentale farlo QUI) ---\n",
    "            scaler = StandardScaler()\n",
    "            X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "            X_fold_val_scaled = scaler.transform(X_fold_val) # Usa la media/std del train\n",
    "            \n",
    "            # --- TRAINING ---\n",
    "            # Nota: train_svm_cvxopt deve essere importata dal tuo file\n",
    "            try:\n",
    "                alphas, b, sv, sv_y, sv_a = train_svm_cvxopt(\n",
    "                    X_fold_train_scaled, y_fold_train, C, kernel_chosen, gamma\n",
    "                )\n",
    "                \n",
    "                if alphas is None: continue # Skip se l'ottimizzazione fallisce\n",
    "                \n",
    "                # --- PREDICTION ---\n",
    "                preds = predict_svm(\n",
    "                    X_fold_val_scaled, sv, sv_y, sv_a, b, kernel_chosen, gamma\n",
    "                )\n",
    "                \n",
    "                acc = accuracy_score(y_fold_val, preds)\n",
    "                fold_accuracies.append(acc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Errore nel fold con C={C}, gamma={gamma}: {e}\")\n",
    "\n",
    "        # Media Accuracy per questa configurazione\n",
    "        if fold_accuracies:\n",
    "            avg_acc = np.mean(fold_accuracies)\n",
    "            print(f\"C={C}, Gamma={gamma} -> Val Acc: {avg_acc:.4f}\")\n",
    "            \n",
    "            results.append((C, gamma, avg_acc))\n",
    "            \n",
    "            if avg_acc > best_acc:\n",
    "                best_acc = avg_acc\n",
    "                best_params = {'C': C, 'gamma': gamma}\n",
    "\n",
    "print(\"\\n--- Migliori Iperparametri Trovati ---\")\n",
    "print(best_params)\n",
    "print(f\"Best Validation Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_svm_cvxopt(X, y, C, kernel_type='gaussian', hyperparam=1.0):\n",
    "    \"\"\"\n",
    "    Risolve il problema duale SVM usando CVXOPT.\n",
    "    \n",
    "    Args:\n",
    "        X: Matrice features (N_samples, N_features)\n",
    "        y: Vettore target {-1, 1} (N_samples,)\n",
    "        C: Parametro di regolarizzazione (Box constraint)\n",
    "        kernel_type: 'gaussian' o 'polynomial'\n",
    "        hyperparam: gamma (per gaussiano) o p (per polinomiale)\n",
    "        \n",
    "    Returns:\n",
    "        alphas: Moltiplicatori di Lagrange\n",
    "        b: Bias\n",
    "        support_vectors: Vettori di supporto\n",
    "        support_labels: Label dei vettori di supporto\n",
    "        support_alphas: Alpha dei vettori di supporto\n",
    "        iterations: Numero di iterazioni impiegate dal solver\n",
    "        dual_objective: Valore finale della funzione obiettivo duale (da massimizzare)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # 1. Calcolo Matrice Kernel\n",
    "    if kernel_type == 'gaussian':\n",
    "        K = gaussian_kernel(X, X, hyperparam)\n",
    "    elif kernel_type == 'polynomial':\n",
    "        K = polynomial_kernel(X, X, hyperparam)\n",
    "    else:\n",
    "        raise ValueError(\"Kernel non supportato. Scegli 'gaussian' o 'polynomial'.\")\n",
    "\n",
    "    # 2. Setup Matrici per CVXOPT (Minimizzazione Quadrata Standard)\n",
    "    # Min 1/2 x^T P x + q^T x\n",
    "    # SVM Duale: Max sum(alpha) - 1/2 alpha^T P alpha\n",
    "    # Equivalente a: Min 1/2 alpha^T P alpha - sum(alpha)\n",
    "    \n",
    "    # P = diag(y) * K * diag(y) -> P_ij = y_i * y_j * K_ij\n",
    "    P = matrix(np.outer(y, y) * K)\n",
    "    \n",
    "    # q = vettore colonna di -1\n",
    "    q = matrix(-np.ones((n_samples, 1)))\n",
    "    \n",
    "    # Vincoli Gx <= h\n",
    "    # a) -alpha <= 0  -> alpha >= 0\n",
    "    # b) alpha <= C\n",
    "    G_std = np.eye(n_samples) * -1\n",
    "    G_slack = np.eye(n_samples)\n",
    "    G = matrix(np.vstack((G_std, G_slack)))\n",
    "    \n",
    "    h_std = np.zeros((n_samples, 1))\n",
    "    h_slack = np.ones((n_samples, 1)) * C\n",
    "    h = matrix(np.vstack((h_std, h_slack)))\n",
    "    \n",
    "    # Vincoli Ax = b (Uguaglianza)\n",
    "    # sum(alpha_i * y_i) = 0\n",
    "    A = matrix(y.reshape(1, -1).astype('float'))\n",
    "    b_eq = matrix(np.zeros(1))\n",
    "\n",
    "    # 3. Risoluzione\n",
    "    try:\n",
    "        solution = solvers.qp(P, q, G, h, A, b_eq)\n",
    "    except ValueError as e:\n",
    "        print(f\"Errore ottimizzazione CVXOPT: {e}\")\n",
    "        return None, None, None, None, None, 0, 0\n",
    "\n",
    "    # 4. Estrazione Risultati\n",
    "    alphas = np.ravel(solution['x'])\n",
    "    iterations = solution['iterations']\n",
    "    \n",
    "    # Il valore restituito da CVXOPT è il valore del problema di MINIMIZZAZIONE.\n",
    "    # Il valore del problema DUALE SVM (Massimizzazione) è l'opposto.\n",
    "    # Nota: CVXOPT minimizza (1/2 aPa - sum(a)). Dual Obj SVM = (sum(a) - 1/2 aPa).\n",
    "    dual_objective = -1 * solution['primal objective'] \n",
    "\n",
    "    # 5. Identificazione Support Vectors\n",
    "    # Usiamo una soglia numerica per considerare alpha > 0\n",
    "    sv_indices = alphas > 1e-5\n",
    "    \n",
    "    support_alphas = alphas[sv_indices]\n",
    "    support_vectors = X[sv_indices]\n",
    "    support_labels = y[sv_indices]\n",
    "    \n",
    "    # 6. Calcolo Bias (b)\n",
    "    # Si usa la condizione KKT per i vettori sul margine (0 < alpha < C) per stabilità\n",
    "    margin_indices = np.logical_and(alphas > 1e-5, alphas < C - 1e-5)\n",
    "    \n",
    "    if np.sum(margin_indices) > 0:\n",
    "        b_subset_idx = margin_indices\n",
    "    else:\n",
    "        # Fallback se nessun punto è esattamente sul margine (raro ma possibile)\n",
    "        b_subset_idx = sv_indices\n",
    "\n",
    "    # Ricalcoliamo il kernel parziale necessario per b\n",
    "    if kernel_type == 'gaussian':\n",
    "        K_b = gaussian_kernel(support_vectors, X[b_subset_idx], hyperparam)\n",
    "    else:\n",
    "        K_b = polynomial_kernel(support_vectors, X[b_subset_idx], hyperparam)\n",
    "    \n",
    "    # b = mean( y_k - sum_i( alpha_i * y_i * K(x_i, x_k) ) )\n",
    "    # K_b shape: (n_support_vectors, n_margin_vectors)\n",
    "    preds_partial = np.sum(K_b * support_alphas.reshape(-1, 1) * support_labels.reshape(-1, 1), axis=0)\n",
    "    b_values = y[b_subset_idx] - preds_partial\n",
    "    b = np.mean(b_values)\n",
    "    \n",
    "    return alphas, b, support_vectors, support_labels, support_alphas, iterations, dual_objective\n",
    "\n",
    "def predict_svm(X_test, support_vectors, support_labels, support_alphas, b, kernel_type, hyperparam):\n",
    "    \"\"\"\n",
    "    Effettua predizioni su nuovi dati.\n",
    "    f(x) = sign( sum( alpha_i * y_i * K(x_i, x) ) + b )\n",
    "    \"\"\"\n",
    "    if kernel_type == 'gaussian':\n",
    "        K_test = gaussian_kernel(support_vectors, X_test, hyperparam)\n",
    "    else:\n",
    "        K_test = polynomial_kernel(support_vectors, X_test, hyperparam)\n",
    "        \n",
    "    # K_test shape: (n_support_vectors, n_test_samples)\n",
    "    decision = np.sum(K_test * support_alphas.reshape(-1, 1) * support_labels.reshape(-1, 1), axis=0) + b\n",
    "    \n",
    "    return np.sign(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target convertiti: [-1  1]\n",
      "Dimensioni Training Set: (800, 32)\n",
      "Dimensioni Test Set: (200, 32)\n",
      "Avvio Grid Search (5-Fold CV)...\n",
      "--------------------------------------------------\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0552e+02 -1.5173e+02  5e+03  3e+01  3e-15\n",
      " 1: -2.4554e+01 -1.4409e+02  2e+02  4e-01  3e-15\n",
      " 2: -2.4748e+01 -4.2503e+01  2e+01  1e-02  6e-16\n",
      " 3: -2.8239e+01 -3.1125e+01  3e+00  2e-03  7e-16\n",
      " 4: -2.9426e+01 -3.0046e+01  6e-01  3e-04  7e-16\n",
      " 5: -2.9696e+01 -2.9830e+01  1e-01  4e-05  7e-16\n",
      " 6: -2.9751e+01 -2.9784e+01  3e-02  9e-06  7e-16\n",
      " 7: -2.9766e+01 -2.9771e+01  6e-03  1e-06  7e-16\n",
      " 8: -2.9768e+01 -2.9769e+01  1e-03  1e-07  8e-16\n",
      " 9: -2.9769e+01 -2.9769e+01  3e-05  3e-09  8e-16\n",
      "10: -2.9769e+01 -2.9769e+01  4e-07  3e-11  8e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.1241e+02 -1.5146e+02  5e+03  3e+01  3e-15\n",
      " 1: -2.5046e+01 -1.4401e+02  2e+02  3e-01  3e-15\n",
      " 2: -2.5523e+01 -4.2887e+01  2e+01  2e-02  8e-16\n",
      " 3: -2.8977e+01 -3.1715e+01  3e+00  2e-03  7e-16\n",
      " 4: -3.0170e+01 -3.0729e+01  6e-01  3e-04  7e-16\n",
      " 5: -3.0405e+01 -3.0558e+01  2e-01  7e-05  7e-16\n",
      " 6: -3.0464e+01 -3.0510e+01  5e-02  2e-05  7e-16\n",
      " 7: -3.0485e+01 -3.0492e+01  7e-03  2e-06  8e-16\n",
      " 8: -3.0489e+01 -3.0489e+01  5e-04  1e-07  8e-16\n",
      " 9: -3.0489e+01 -3.0489e+01  6e-06  1e-09  9e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.1044e+02 -1.4447e+02  5e+03  3e+01  3e-15\n",
      " 1: -2.5102e+01 -1.3715e+02  2e+02  4e-01  3e-15\n",
      " 2: -2.5284e+01 -4.1486e+01  2e+01  1e-02  6e-16\n",
      " 3: -2.8642e+01 -3.1313e+01  3e+00  1e-03  7e-16\n",
      " 4: -2.9806e+01 -3.0304e+01  5e-01  2e-04  8e-16\n",
      " 5: -3.0028e+01 -3.0133e+01  1e-01  3e-05  8e-16\n",
      " 6: -3.0068e+01 -3.0100e+01  3e-02  7e-06  7e-16\n",
      " 7: -3.0081e+01 -3.0089e+01  8e-03  1e-06  8e-16\n",
      " 8: -3.0085e+01 -3.0086e+01  1e-03  9e-08  8e-16\n",
      " 9: -3.0085e+01 -3.0085e+01  1e-05  1e-09  9e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0680e+02 -1.5132e+02  5e+03  3e+01  3e-15\n",
      " 1: -2.4688e+01 -1.4375e+02  2e+02  4e-01  3e-15\n",
      " 2: -2.4772e+01 -4.2440e+01  2e+01  9e-03  8e-16\n",
      " 3: -2.8215e+01 -3.1005e+01  3e+00  1e-03  9e-16\n",
      " 4: -2.9407e+01 -3.0001e+01  6e-01  2e-04  9e-16\n",
      " 5: -2.9678e+01 -2.9794e+01  1e-01  3e-05  9e-16\n",
      " 6: -2.9727e+01 -2.9754e+01  3e-02  7e-06  8e-16\n",
      " 7: -2.9740e+01 -2.9744e+01  4e-03  4e-08  9e-16\n",
      " 8: -2.9742e+01 -2.9742e+01  3e-04  3e-09  9e-16\n",
      " 9: -2.9742e+01 -2.9742e+01  5e-06  3e-11  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.0560e+02 -1.5195e+02  5e+03  3e+01  3e-15\n",
      " 1: -2.4652e+01 -1.4439e+02  2e+02  4e-01  3e-15\n",
      " 2: -2.4656e+01 -4.2577e+01  2e+01  6e-03  6e-16\n",
      " 3: -2.8231e+01 -3.1112e+01  3e+00  8e-04  7e-16\n",
      " 4: -2.9521e+01 -3.0019e+01  5e-01  1e-04  8e-16\n",
      " 5: -2.9762e+01 -2.9845e+01  8e-02  9e-06  7e-16\n",
      " 6: -2.9801e+01 -2.9810e+01  9e-03  7e-07  8e-16\n",
      " 7: -2.9806e+01 -2.9806e+01  2e-04  2e-08  8e-16\n",
      " 8: -2.9806e+01 -2.9806e+01  3e-05  2e-09  8e-16\n",
      "Optimal solution found.\n",
      "C=0.1   Gamma=0.001 -> Avg Val Acc: 0.9062\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.1381e+01 -1.4008e+02  5e+03  3e+01  2e-15\n",
      " 1: -1.5338e+01 -1.3115e+02  2e+02  7e-01  2e-15\n",
      " 2: -1.1909e+01 -4.1664e+01  3e+01  1e-15  2e-15\n",
      " 3: -1.4532e+01 -1.9932e+01  5e+00  6e-16  9e-16\n",
      " 4: -1.5167e+01 -1.8221e+01  3e+00  2e-16  6e-16\n",
      " 5: -1.5519e+01 -1.7300e+01  2e+00  3e-16  4e-16\n",
      " 6: -1.5699e+01 -1.6858e+01  1e+00  2e-16  4e-16\n",
      " 7: -1.5866e+01 -1.6419e+01  6e-01  2e-16  5e-16\n",
      " 8: -1.5920e+01 -1.6300e+01  4e-01  6e-16  4e-16\n",
      " 9: -1.5995e+01 -1.6152e+01  2e-01  2e-15  4e-16\n",
      "10: -1.6020e+01 -1.6110e+01  9e-02  5e-16  4e-16\n",
      "11: -1.6032e+01 -1.6092e+01  6e-02  1e-15  4e-16\n",
      "12: -1.6045e+01 -1.6071e+01  3e-02  7e-16  5e-16\n",
      "13: -1.6053e+01 -1.6061e+01  9e-03  2e-16  5e-16\n",
      "14: -1.6055e+01 -1.6058e+01  3e-03  2e-16  5e-16\n",
      "15: -1.6057e+01 -1.6057e+01  1e-04  3e-16  6e-16\n",
      "16: -1.6057e+01 -1.6057e+01  5e-06  7e-16  6e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.9941e+01 -1.4031e+02  5e+03  3e+01  3e-15\n",
      " 1: -1.6385e+01 -1.3149e+02  2e+02  7e-01  2e-15\n",
      " 2: -1.2933e+01 -4.1886e+01  3e+01  7e-16  2e-15\n",
      " 3: -1.5614e+01 -2.0880e+01  5e+00  2e-16  9e-16\n",
      " 4: -1.6280e+01 -1.9256e+01  3e+00  6e-16  5e-16\n",
      " 5: -1.6719e+01 -1.8115e+01  1e+00  1e-15  5e-16\n",
      " 6: -1.6839e+01 -1.7858e+01  1e+00  2e-16  4e-16\n",
      " 7: -1.6982e+01 -1.7524e+01  5e-01  2e-16  4e-16\n",
      " 8: -1.7050e+01 -1.7388e+01  3e-01  2e-16  4e-16\n",
      " 9: -1.7120e+01 -1.7260e+01  1e-01  4e-16  5e-16\n",
      "10: -1.7161e+01 -1.7195e+01  3e-02  2e-16  5e-16\n",
      "11: -1.7171e+01 -1.7182e+01  1e-02  4e-16  6e-16\n",
      "12: -1.7175e+01 -1.7178e+01  4e-03  5e-16  5e-16\n",
      "13: -1.7176e+01 -1.7176e+01  3e-04  9e-16  6e-16\n",
      "14: -1.7176e+01 -1.7176e+01  3e-06  8e-16  7e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.8298e+01 -1.3847e+02  5e+03  3e+01  3e-15\n",
      " 1: -1.5962e+01 -1.2961e+02  2e+02  6e-01  2e-15\n",
      " 2: -1.2885e+01 -3.8091e+01  3e+01  2e-16  2e-15\n",
      " 3: -1.5419e+01 -2.0180e+01  5e+00  8e-16  9e-16\n",
      " 4: -1.5725e+01 -1.9501e+01  4e+00  7e-16  6e-16\n",
      " 5: -1.6252e+01 -1.8035e+01  2e+00  9e-16  5e-16\n",
      " 6: -1.6474e+01 -1.7469e+01  1e+00  6e-16  4e-16\n",
      " 7: -1.6591e+01 -1.7210e+01  6e-01  8e-16  4e-16\n",
      " 8: -1.6700e+01 -1.6985e+01  3e-01  2e-16  4e-16\n",
      " 9: -1.6736e+01 -1.6911e+01  2e-01  7e-16  4e-16\n",
      "10: -1.6755e+01 -1.6886e+01  1e-01  2e-16  4e-16\n",
      "11: -1.6783e+01 -1.6835e+01  5e-02  2e-16  6e-16\n",
      "12: -1.6797e+01 -1.6818e+01  2e-02  3e-16  5e-16\n",
      "13: -1.6803e+01 -1.6810e+01  7e-03  5e-16  6e-16\n",
      "14: -1.6806e+01 -1.6807e+01  5e-04  1e-15  6e-16\n",
      "15: -1.6807e+01 -1.6807e+01  6e-05  2e-16  6e-16\n",
      "16: -1.6807e+01 -1.6807e+01  7e-07  6e-16  6e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.3923e+01 -1.3975e+02  5e+03  3e+01  3e-15\n",
      " 1: -1.5544e+01 -1.3085e+02  2e+02  7e-01  2e-15\n",
      " 2: -1.2196e+01 -4.0772e+01  3e+01  5e-16  1e-15\n",
      " 3: -1.4780e+01 -2.0409e+01  6e+00  2e-16  9e-16\n",
      " 4: -1.5457e+01 -1.8562e+01  3e+00  4e-16  6e-16\n",
      " 5: -1.5832e+01 -1.7526e+01  2e+00  2e-16  4e-16\n",
      " 6: -1.5982e+01 -1.7145e+01  1e+00  2e-16  4e-16\n",
      " 7: -1.6175e+01 -1.6661e+01  5e-01  8e-16  4e-16\n",
      " 8: -1.6239e+01 -1.6528e+01  3e-01  2e-16  4e-16\n",
      " 9: -1.6290e+01 -1.6435e+01  1e-01  2e-16  4e-16\n",
      "10: -1.6308e+01 -1.6407e+01  1e-01  4e-16  4e-16\n",
      "11: -1.6334e+01 -1.6365e+01  3e-02  2e-16  5e-16\n",
      "12: -1.6342e+01 -1.6353e+01  1e-02  2e-16  5e-16\n",
      "13: -1.6346e+01 -1.6348e+01  2e-03  2e-16  6e-16\n",
      "14: -1.6347e+01 -1.6347e+01  5e-05  2e-16  7e-16\n",
      "15: -1.6347e+01 -1.6347e+01  5e-07  3e-16  7e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.1454e+01 -1.3982e+02  5e+03  3e+01  3e-15\n",
      " 1: -1.5364e+01 -1.3086e+02  2e+02  7e-01  2e-15\n",
      " 2: -1.1917e+01 -4.1857e+01  3e+01  2e-16  1e-15\n",
      " 3: -1.4603e+01 -2.0006e+01  5e+00  1e-15  9e-16\n",
      " 4: -1.5187e+01 -1.8564e+01  3e+00  8e-16  6e-16\n",
      " 5: -1.5636e+01 -1.7276e+01  2e+00  5e-16  4e-16\n",
      " 6: -1.5761e+01 -1.6989e+01  1e+00  1e-15  4e-16\n",
      " 7: -1.5899e+01 -1.6657e+01  8e-01  6e-16  4e-16\n",
      " 8: -1.6026e+01 -1.6375e+01  3e-01  2e-16  5e-16\n",
      " 9: -1.6073e+01 -1.6279e+01  2e-01  4e-16  4e-16\n",
      "10: -1.6095e+01 -1.6245e+01  1e-01  2e-16  4e-16\n",
      "11: -1.6125e+01 -1.6198e+01  7e-02  5e-16  4e-16\n",
      "12: -1.6141e+01 -1.6171e+01  3e-02  5e-16  5e-16\n",
      "13: -1.6149e+01 -1.6160e+01  1e-02  7e-16  5e-16\n",
      "14: -1.6153e+01 -1.6155e+01  1e-03  2e-16  5e-16\n",
      "15: -1.6154e+01 -1.6154e+01  2e-04  3e-16  6e-16\n",
      "16: -1.6154e+01 -1.6154e+01  8e-06  4e-16  6e-16\n",
      "Optimal solution found.\n",
      "C=0.1   Gamma=0.01  -> Avg Val Acc: 0.9150\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.8686e+01 -1.3917e+02  5e+03  3e+01  1e-15\n",
      " 1: -1.5400e+01 -1.3003e+02  2e+02  7e-01  1e-15\n",
      " 2: -1.2101e+01 -3.9788e+01  3e+01  4e-16  1e-15\n",
      " 3: -1.4014e+01 -1.8110e+01  4e+00  2e-16  8e-16\n",
      " 4: -1.4376e+01 -1.5483e+01  1e+00  3e-16  4e-16\n",
      " 5: -1.4409e+01 -1.5439e+01  1e+00  8e-16  4e-16\n",
      " 6: -1.4509e+01 -1.5024e+01  5e-01  9e-16  3e-16\n",
      " 7: -1.4557e+01 -1.4869e+01  3e-01  2e-16  3e-16\n",
      " 8: -1.4587e+01 -1.4775e+01  2e-01  2e-16  3e-16\n",
      " 9: -1.4609e+01 -1.4724e+01  1e-01  2e-16  3e-16\n",
      "10: -1.4626e+01 -1.4686e+01  6e-02  2e-16  3e-16\n",
      "11: -1.4637e+01 -1.4661e+01  2e-02  3e-16  3e-16\n",
      "12: -1.4644e+01 -1.4650e+01  6e-03  4e-16  4e-16\n",
      "13: -1.4646e+01 -1.4647e+01  8e-04  4e-16  4e-16\n",
      "14: -1.4646e+01 -1.4646e+01  1e-04  2e-16  5e-16\n",
      "15: -1.4646e+01 -1.4646e+01  4e-06  3e-16  6e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.7896e+01 -1.3968e+02  5e+03  3e+01  1e-15\n",
      " 1: -1.6718e+01 -1.3069e+02  2e+02  7e-01  1e-15\n",
      " 2: -1.3389e+01 -4.0763e+01  3e+01  2e-16  2e-15\n",
      " 3: -1.5389e+01 -1.8885e+01  3e+00  7e-16  8e-16\n",
      " 4: -1.5702e+01 -1.6744e+01  1e+00  3e-16  4e-16\n",
      " 5: -1.5752e+01 -1.6638e+01  9e-01  3e-16  3e-16\n",
      " 6: -1.5814e+01 -1.6390e+01  6e-01  1e-15  3e-16\n",
      " 7: -1.5873e+01 -1.6199e+01  3e-01  6e-16  3e-16\n",
      " 8: -1.5905e+01 -1.6107e+01  2e-01  2e-16  3e-16\n",
      " 9: -1.5930e+01 -1.6043e+01  1e-01  4e-16  3e-16\n",
      "10: -1.5944e+01 -1.6014e+01  7e-02  3e-16  3e-16\n",
      "11: -1.5958e+01 -1.5985e+01  3e-02  2e-16  4e-16\n",
      "12: -1.5964e+01 -1.5973e+01  9e-03  4e-16  4e-16\n",
      "13: -1.5966e+01 -1.5970e+01  4e-03  5e-16  3e-16\n",
      "14: -1.5968e+01 -1.5968e+01  8e-04  2e-16  4e-16\n",
      "15: -1.5968e+01 -1.5968e+01  7e-05  2e-16  6e-16\n",
      "16: -1.5968e+01 -1.5968e+01  1e-06  7e-16  5e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.6206e+01 -1.3901e+02  5e+03  3e+01  1e-15\n",
      " 1: -1.6494e+01 -1.2992e+02  2e+02  7e-01  1e-15\n",
      " 2: -1.3184e+01 -3.9929e+01  3e+01  2e-16  1e-15\n",
      " 3: -1.5083e+01 -1.8528e+01  3e+00  7e-16  7e-16\n",
      " 4: -1.5359e+01 -1.6670e+01  1e+00  1e-15  4e-16\n",
      " 5: -1.5449e+01 -1.6323e+01  9e-01  8e-16  3e-16\n",
      " 6: -1.5505e+01 -1.6101e+01  6e-01  2e-16  2e-16\n",
      " 7: -1.5568e+01 -1.5853e+01  3e-01  2e-16  3e-16\n",
      " 8: -1.5590e+01 -1.5804e+01  2e-01  8e-16  3e-16\n",
      " 9: -1.5615e+01 -1.5737e+01  1e-01  1e-15  3e-16\n",
      "10: -1.5636e+01 -1.5691e+01  5e-02  8e-16  3e-16\n",
      "11: -1.5646e+01 -1.5671e+01  2e-02  2e-16  4e-16\n",
      "12: -1.5653e+01 -1.5659e+01  6e-03  5e-16  4e-16\n",
      "13: -1.5655e+01 -1.5656e+01  1e-03  5e-16  4e-16\n",
      "14: -1.5656e+01 -1.5656e+01  5e-05  4e-16  5e-16\n",
      "15: -1.5656e+01 -1.5656e+01  2e-06  4e-16  5e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.2067e+01 -1.3914e+02  5e+03  3e+01  1e-15\n",
      " 1: -1.5809e+01 -1.3010e+02  2e+02  7e-01  1e-15\n",
      " 2: -1.2544e+01 -3.9755e+01  3e+01  4e-16  1e-15\n",
      " 3: -1.4485e+01 -1.8398e+01  4e+00  5e-16  7e-16\n",
      " 4: -1.4794e+01 -1.6164e+01  1e+00  4e-16  4e-16\n",
      " 5: -1.4876e+01 -1.5832e+01  1e+00  1e-16  3e-16\n",
      " 6: -1.4949e+01 -1.5510e+01  6e-01  2e-16  3e-16\n",
      " 7: -1.5006e+01 -1.5296e+01  3e-01  2e-16  3e-16\n",
      " 8: -1.5038e+01 -1.5205e+01  2e-01  7e-16  3e-16\n",
      " 9: -1.5055e+01 -1.5160e+01  1e-01  7e-16  3e-16\n",
      "10: -1.5072e+01 -1.5121e+01  5e-02  8e-16  3e-16\n",
      "11: -1.5082e+01 -1.5102e+01  2e-02  5e-16  4e-16\n",
      "12: -1.5087e+01 -1.5092e+01  5e-03  2e-16  4e-16\n",
      "13: -1.5089e+01 -1.5090e+01  7e-04  6e-16  5e-16\n",
      "14: -1.5090e+01 -1.5090e+01  4e-05  2e-15  5e-16\n",
      "15: -1.5090e+01 -1.5090e+01  6e-07  1e-15  7e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.8296e+01 -1.3876e+02  5e+03  3e+01  1e-15\n",
      " 1: -1.5574e+01 -1.2960e+02  2e+02  7e-01  1e-15\n",
      " 2: -1.2235e+01 -4.0576e+01  3e+01  9e-16  1e-15\n",
      " 3: -1.4212e+01 -1.8104e+01  4e+00  6e-16  8e-16\n",
      " 4: -1.4525e+01 -1.5854e+01  1e+00  2e-16  4e-16\n",
      " 5: -1.4581e+01 -1.5693e+01  1e+00  2e-16  3e-16\n",
      " 6: -1.4647e+01 -1.5366e+01  7e-01  2e-16  3e-16\n",
      " 7: -1.4727e+01 -1.5014e+01  3e-01  3e-16  3e-16\n",
      " 8: -1.4751e+01 -1.4963e+01  2e-01  9e-16  3e-16\n",
      " 9: -1.4774e+01 -1.4903e+01  1e-01  9e-16  2e-16\n",
      "10: -1.4797e+01 -1.4850e+01  5e-02  3e-16  3e-16\n",
      "11: -1.4806e+01 -1.4832e+01  3e-02  7e-16  4e-16\n",
      "12: -1.4813e+01 -1.4821e+01  9e-03  2e-16  3e-16\n",
      "13: -1.4815e+01 -1.4818e+01  3e-03  6e-16  3e-16\n",
      "14: -1.4816e+01 -1.4816e+01  5e-04  6e-16  4e-16\n",
      "15: -1.4816e+01 -1.4816e+01  1e-05  6e-16  5e-16\n",
      "Optimal solution found.\n",
      "C=0.1   Gamma=0.1   -> Avg Val Acc: 0.9175\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.6371e+01 -1.4668e+02  5e+03  3e+01  6e-16\n",
      " 1: -2.1794e+01 -1.3770e+02  2e+02  5e-01  7e-16\n",
      " 2: -2.0223e+01 -4.4131e+01  2e+01  2e-16  6e-16\n",
      " 3: -2.1789e+01 -2.6337e+01  5e+00  8e-16  4e-16\n",
      " 4: -2.2482e+01 -2.4042e+01  2e+00  5e-16  3e-16\n",
      " 5: -2.2755e+01 -2.3502e+01  7e-01  3e-16  3e-16\n",
      " 6: -2.2909e+01 -2.3204e+01  3e-01  2e-16  3e-16\n",
      " 7: -2.2991e+01 -2.3060e+01  7e-02  3e-16  3e-16\n",
      " 8: -2.3012e+01 -2.3028e+01  2e-02  8e-16  3e-16\n",
      " 9: -2.3018e+01 -2.3020e+01  1e-03  3e-16  4e-16\n",
      "10: -2.3019e+01 -2.3019e+01  6e-05  2e-16  3e-16\n",
      "11: -2.3019e+01 -2.3019e+01  1e-06  9e-16  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.4085e+01 -1.4929e+02  5e+03  3e+01  7e-16\n",
      " 1: -2.3187e+01 -1.3992e+02  2e+02  5e-01  7e-16\n",
      " 2: -2.1587e+01 -4.5078e+01  2e+01  1e-15  7e-16\n",
      " 3: -2.3321e+01 -2.7744e+01  4e+00  9e-16  4e-16\n",
      " 4: -2.4056e+01 -2.5471e+01  1e+00  4e-16  3e-16\n",
      " 5: -2.4324e+01 -2.4970e+01  6e-01  4e-16  3e-16\n",
      " 6: -2.4469e+01 -2.4710e+01  2e-01  2e-15  3e-16\n",
      " 7: -2.4531e+01 -2.4608e+01  8e-02  2e-15  3e-16\n",
      " 8: -2.4558e+01 -2.4567e+01  9e-03  2e-16  4e-16\n",
      " 9: -2.4562e+01 -2.4563e+01  5e-04  1e-15  4e-16\n",
      "10: -2.4562e+01 -2.4562e+01  1e-05  1e-15  4e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.2164e+01 -1.4617e+02  5e+03  3e+01  7e-16\n",
      " 1: -2.3344e+01 -1.3685e+02  2e+02  6e-01  8e-16\n",
      " 2: -2.1301e+01 -4.7876e+01  3e+01  6e-16  8e-16\n",
      " 3: -2.3004e+01 -2.7618e+01  5e+00  2e-16  5e-16\n",
      " 4: -2.3717e+01 -2.5338e+01  2e+00  8e-16  3e-16\n",
      " 5: -2.4081e+01 -2.4589e+01  5e-01  1e-15  3e-16\n",
      " 6: -2.4215e+01 -2.4374e+01  2e-01  5e-16  3e-16\n",
      " 7: -2.4265e+01 -2.4298e+01  3e-02  3e-16  3e-16\n",
      " 8: -2.4278e+01 -2.4281e+01  2e-03  1e-15  4e-16\n",
      " 9: -2.4279e+01 -2.4279e+01  9e-05  1e-15  5e-16\n",
      "10: -2.4279e+01 -2.4279e+01  3e-06  2e-16  3e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.9036e+01 -1.4792e+02  5e+03  3e+01  6e-16\n",
      " 1: -2.2400e+01 -1.3879e+02  2e+02  6e-01  7e-16\n",
      " 2: -2.0644e+01 -4.6758e+01  3e+01  2e-16  6e-16\n",
      " 3: -2.2384e+01 -2.6861e+01  4e+00  8e-16  4e-16\n",
      " 4: -2.3127e+01 -2.4583e+01  1e+00  6e-16  4e-16\n",
      " 5: -2.3423e+01 -2.4005e+01  6e-01  4e-16  3e-16\n",
      " 6: -2.3566e+01 -2.3769e+01  2e-01  2e-16  3e-16\n",
      " 7: -2.3626e+01 -2.3674e+01  5e-02  8e-16  3e-16\n",
      " 8: -2.3642e+01 -2.3651e+01  9e-03  2e-16  4e-16\n",
      " 9: -2.3646e+01 -2.3646e+01  4e-04  1e-15  4e-16\n",
      "10: -2.3646e+01 -2.3646e+01  1e-05  9e-16  3e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.5857e+01 -1.4722e+02  5e+03  3e+01  6e-16\n",
      " 1: -2.2073e+01 -1.3815e+02  2e+02  6e-01  8e-16\n",
      " 2: -2.0359e+01 -4.6433e+01  3e+01  5e-16  6e-16\n",
      " 3: -2.2033e+01 -2.6923e+01  5e+00  6e-16  4e-16\n",
      " 4: -2.2761e+01 -2.4407e+01  2e+00  2e-16  3e-16\n",
      " 5: -2.3114e+01 -2.3631e+01  5e-01  8e-16  3e-16\n",
      " 6: -2.3255e+01 -2.3397e+01  1e-01  2e-15  3e-16\n",
      " 7: -2.3296e+01 -2.3334e+01  4e-02  2e-16  3e-16\n",
      " 8: -2.3309e+01 -2.3315e+01  6e-03  2e-16  3e-16\n",
      " 9: -2.3311e+01 -2.3312e+01  9e-04  2e-16  4e-16\n",
      "10: -2.3312e+01 -2.3312e+01  5e-05  7e-16  4e-16\n",
      "11: -2.3312e+01 -2.3312e+01  1e-06  1e-15  3e-16\n",
      "Optimal solution found.\n",
      "C=0.1   Gamma=1     -> Avg Val Acc: 0.9088\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7533e+02 -1.5462e+03  8e+03  3e+00  5e-15\n",
      " 1: -1.0450e+02 -9.6507e+02  1e+03  1e-01  4e-15\n",
      " 2: -1.1982e+02 -2.3845e+02  1e+02  1e-02  3e-15\n",
      " 3: -1.4209e+02 -1.7659e+02  4e+01  3e-03  3e-15\n",
      " 4: -1.4689e+02 -1.6772e+02  2e+01  1e-03  3e-15\n",
      " 5: -1.5008e+02 -1.6177e+02  1e+01  7e-04  3e-15\n",
      " 6: -1.5155e+02 -1.5892e+02  7e+00  3e-04  3e-15\n",
      " 7: -1.5291e+02 -1.5631e+02  3e+00  1e-04  3e-15\n",
      " 8: -1.5303e+02 -1.5622e+02  3e+00  1e-04  3e-15\n",
      " 9: -1.5340e+02 -1.5537e+02  2e+00  5e-15  4e-15\n",
      "10: -1.5358e+02 -1.5519e+02  2e+00  4e-15  4e-15\n",
      "11: -1.5402e+02 -1.5464e+02  6e-01  4e-15  4e-15\n",
      "12: -1.5417e+02 -1.5444e+02  3e-01  2e-15  4e-15\n",
      "13: -1.5425e+02 -1.5434e+02  9e-02  5e-15  4e-15\n",
      "14: -1.5428e+02 -1.5430e+02  2e-02  9e-15  4e-15\n",
      "15: -1.5429e+02 -1.5429e+02  3e-04  2e-14  4e-15\n",
      "16: -1.5429e+02 -1.5429e+02  3e-06  1e-14  4e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8853e+02 -1.5480e+03  8e+03  3e+00  5e-15\n",
      " 1: -1.1311e+02 -9.6750e+02  1e+03  1e-01  4e-15\n",
      " 2: -1.3035e+02 -2.4997e+02  1e+02  1e-02  3e-15\n",
      " 3: -1.5133e+02 -1.9047e+02  4e+01  3e-03  3e-15\n",
      " 4: -1.5802e+02 -1.7690e+02  2e+01  1e-03  3e-15\n",
      " 5: -1.5960e+02 -1.7423e+02  1e+01  9e-04  3e-15\n",
      " 6: -1.6245e+02 -1.6857e+02  6e+00  3e-04  3e-15\n",
      " 7: -1.6328e+02 -1.6727e+02  4e+00  1e-04  3e-15\n",
      " 8: -1.6401e+02 -1.6608e+02  2e+00  3e-05  4e-15\n",
      " 9: -1.6438e+02 -1.6550e+02  1e+00  1e-05  3e-15\n",
      "10: -1.6442e+02 -1.6545e+02  1e+00  7e-06  3e-15\n",
      "11: -1.6466e+02 -1.6514e+02  5e-01  3e-06  3e-15\n",
      "12: -1.6479e+02 -1.6495e+02  2e-01  1e-07  4e-15\n",
      "13: -1.6480e+02 -1.6495e+02  2e-01  1e-07  4e-15\n",
      "14: -1.6484e+02 -1.6490e+02  7e-02  7e-10  4e-15\n",
      "15: -1.6485e+02 -1.6489e+02  4e-02  3e-10  4e-15\n",
      "16: -1.6486e+02 -1.6488e+02  2e-02  1e-14  4e-15\n",
      "17: -1.6487e+02 -1.6487e+02  5e-04  9e-15  4e-15\n",
      "18: -1.6487e+02 -1.6487e+02  8e-06  5e-15  4e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8501e+02 -1.4215e+03  7e+03  3e+00  6e-15\n",
      " 1: -1.1314e+02 -8.5862e+02  9e+02  1e-01  5e-15\n",
      " 2: -1.3001e+02 -2.3891e+02  1e+02  1e-02  4e-15\n",
      " 3: -1.4891e+02 -1.8464e+02  4e+01  3e-03  4e-15\n",
      " 4: -1.5441e+02 -1.7340e+02  2e+01  1e-03  4e-15\n",
      " 5: -1.5738e+02 -1.6758e+02  1e+01  7e-04  4e-15\n",
      " 6: -1.5887e+02 -1.6464e+02  6e+00  3e-04  4e-15\n",
      " 7: -1.5943e+02 -1.6367e+02  4e+00  2e-04  4e-15\n",
      " 8: -1.6002e+02 -1.6240e+02  2e+00  5e-15  5e-15\n",
      " 9: -1.6047e+02 -1.6175e+02  1e+00  5e-15  4e-15\n",
      "10: -1.6067e+02 -1.6147e+02  8e-01  1e-15  5e-15\n",
      "11: -1.6077e+02 -1.6134e+02  6e-01  7e-16  4e-15\n",
      "12: -1.6077e+02 -1.6134e+02  6e-01  7e-16  4e-15\n",
      "13: -1.6083e+02 -1.6129e+02  5e-01  1e-15  4e-15\n",
      "14: -1.6089e+02 -1.6122e+02  3e-01  8e-15  4e-15\n",
      "15: -1.6088e+02 -1.6120e+02  3e-01  3e-15  4e-15\n",
      "16: -1.6099e+02 -1.6109e+02  9e-02  3e-15  4e-15\n",
      "17: -1.6103e+02 -1.6104e+02  1e-02  1e-14  5e-15\n",
      "18: -1.6104e+02 -1.6104e+02  1e-04  5e-15  5e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7788e+02 -1.5410e+03  8e+03  3e+00  6e-15\n",
      " 1: -1.0605e+02 -9.6036e+02  1e+03  1e-01  5e-15\n",
      " 2: -1.2216e+02 -2.4403e+02  1e+02  1e-02  4e-15\n",
      " 3: -1.4325e+02 -1.8120e+02  4e+01  3e-03  4e-15\n",
      " 4: -1.4970e+02 -1.6859e+02  2e+01  1e-03  4e-15\n",
      " 5: -1.5102e+02 -1.6629e+02  2e+01  1e-03  4e-15\n",
      " 6: -1.5332e+02 -1.6171e+02  8e+00  5e-04  4e-15\n",
      " 7: -1.5451e+02 -1.5958e+02  5e+00  3e-04  4e-15\n",
      " 8: -1.5485e+02 -1.5910e+02  4e+00  2e-04  4e-15\n",
      " 9: -1.5528e+02 -1.5826e+02  3e+00  6e-05  4e-15\n",
      "10: -1.5575e+02 -1.5749e+02  2e+00  3e-05  4e-15\n",
      "11: -1.5608e+02 -1.5697e+02  9e-01  1e-05  4e-15\n",
      "12: -1.5618e+02 -1.5683e+02  7e-01  4e-06  4e-15\n",
      "13: -1.5641e+02 -1.5655e+02  1e-01  1e-07  5e-15\n",
      "14: -1.5644e+02 -1.5651e+02  6e-02  4e-08  4e-15\n",
      "15: -1.5647e+02 -1.5648e+02  1e-02  3e-15  5e-15\n",
      "16: -1.5647e+02 -1.5647e+02  2e-04  7e-15  5e-15\n",
      "17: -1.5647e+02 -1.5647e+02  2e-06  3e-15  5e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7553e+02 -1.5508e+03  8e+03  3e+00  5e-15\n",
      " 1: -1.0448e+02 -9.7017e+02  1e+03  1e-01  4e-15\n",
      " 2: -1.1976e+02 -2.3915e+02  1e+02  1e-02  3e-15\n",
      " 3: -1.4208e+02 -1.7906e+02  4e+01  3e-03  4e-15\n",
      " 4: -1.4887e+02 -1.6521e+02  2e+01  1e-03  4e-15\n",
      " 5: -1.5069e+02 -1.6236e+02  1e+01  7e-04  3e-15\n",
      " 6: -1.5252e+02 -1.5892e+02  6e+00  2e-04  3e-15\n",
      " 7: -1.5335e+02 -1.5766e+02  4e+00  1e-04  3e-15\n",
      " 8: -1.5380e+02 -1.5677e+02  3e+00  3e-05  4e-15\n",
      " 9: -1.5440e+02 -1.5581e+02  1e+00  1e-05  4e-15\n",
      "10: -1.5445e+02 -1.5577e+02  1e+00  9e-06  3e-15\n",
      "11: -1.5464e+02 -1.5552e+02  9e-01  6e-06  3e-15\n",
      "12: -1.5476e+02 -1.5533e+02  6e-01  2e-06  4e-15\n",
      "13: -1.5477e+02 -1.5532e+02  6e-01  2e-06  4e-15\n",
      "14: -1.5492e+02 -1.5513e+02  2e-01  4e-07  4e-15\n",
      "15: -1.5497e+02 -1.5506e+02  9e-02  2e-08  4e-15\n",
      "16: -1.5499e+02 -1.5504e+02  4e-02  7e-15  4e-15\n",
      "17: -1.5501e+02 -1.5502e+02  6e-03  3e-15  4e-15\n",
      "18: -1.5502e+02 -1.5502e+02  6e-05  2e-16  4e-15\n",
      "Optimal solution found.\n",
      "C=1     Gamma=0.001 -> Avg Val Acc: 0.9163\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6238e+02 -1.4619e+03  8e+03  3e+00  4e-15\n",
      " 1: -1.0005e+02 -8.5023e+02  1e+03  2e-01  3e-15\n",
      " 2: -1.0251e+02 -2.1369e+02  1e+02  2e-02  4e-15\n",
      " 3: -1.1318e+02 -1.4448e+02  3e+01  3e-03  2e-15\n",
      " 4: -1.1598e+02 -1.3192e+02  2e+01  1e-03  2e-15\n",
      " 5: -1.1717e+02 -1.2747e+02  1e+01  7e-04  2e-15\n",
      " 6: -1.1817e+02 -1.2368e+02  6e+00  3e-04  2e-15\n",
      " 7: -1.1870e+02 -1.2191e+02  3e+00  1e-04  2e-15\n",
      " 8: -1.1897e+02 -1.2127e+02  2e+00  6e-05  2e-15\n",
      " 9: -1.1907e+02 -1.2080e+02  2e+00  3e-05  2e-15\n",
      "10: -1.1918e+02 -1.2058e+02  1e+00  2e-05  2e-15\n",
      "11: -1.1936e+02 -1.2016e+02  8e-01  2e-06  3e-15\n",
      "12: -1.1950e+02 -1.1987e+02  4e-01  7e-07  3e-15\n",
      "13: -1.1954e+02 -1.1979e+02  3e-01  3e-07  2e-15\n",
      "14: -1.1958e+02 -1.1972e+02  1e-01  1e-07  2e-15\n",
      "15: -1.1961e+02 -1.1967e+02  6e-02  9e-15  3e-15\n",
      "16: -1.1963e+02 -1.1964e+02  1e-02  9e-15  3e-15\n",
      "17: -1.1964e+02 -1.1964e+02  4e-03  1e-14  2e-15\n",
      "18: -1.1964e+02 -1.1964e+02  3e-04  1e-14  3e-15\n",
      "19: -1.1964e+02 -1.1964e+02  3e-06  1e-15  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7823e+02 -1.4690e+03  8e+03  3e+00  5e-15\n",
      " 1: -1.1098e+02 -8.6086e+02  1e+03  1e-01  3e-15\n",
      " 2: -1.1461e+02 -2.2372e+02  1e+02  1e-02  4e-15\n",
      " 3: -1.2662e+02 -1.5403e+02  3e+01  2e-03  2e-15\n",
      " 4: -1.2803e+02 -1.4828e+02  2e+01  2e-03  2e-15\n",
      " 5: -1.3014e+02 -1.3792e+02  8e+00  4e-04  2e-15\n",
      " 6: -1.3084e+02 -1.3544e+02  5e+00  2e-04  2e-15\n",
      " 7: -1.3121e+02 -1.3423e+02  3e+00  9e-05  2e-15\n",
      " 8: -1.3143e+02 -1.3359e+02  2e+00  6e-05  2e-15\n",
      " 9: -1.3159e+02 -1.3308e+02  1e+00  3e-05  2e-15\n",
      "10: -1.3179e+02 -1.3253e+02  7e-01  6e-06  2e-15\n",
      "11: -1.3185e+02 -1.3244e+02  6e-01  4e-06  2e-15\n",
      "12: -1.3190e+02 -1.3232e+02  4e-01  8e-07  2e-15\n",
      "13: -1.3196e+02 -1.3222e+02  3e-01  1e-07  2e-15\n",
      "14: -1.3205e+02 -1.3210e+02  5e-02  5e-15  3e-15\n",
      "15: -1.3206e+02 -1.3208e+02  2e-02  2e-16  3e-15\n",
      "16: -1.3207e+02 -1.3207e+02  2e-03  7e-15  2e-15\n",
      "17: -1.3207e+02 -1.3207e+02  2e-04  8e-15  3e-15\n",
      "18: -1.3207e+02 -1.3207e+02  2e-06  2e-15  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7525e+02 -1.4356e+03  7e+03  3e+00  5e-15\n",
      " 1: -1.0875e+02 -8.2726e+02  9e+02  1e-01  3e-15\n",
      " 2: -1.1304e+02 -2.2451e+02  1e+02  1e-02  3e-15\n",
      " 3: -1.2323e+02 -1.5538e+02  3e+01  3e-03  2e-15\n",
      " 4: -1.2607e+02 -1.4063e+02  1e+01  1e-03  3e-15\n",
      " 5: -1.2694e+02 -1.3676e+02  1e+01  6e-04  3e-15\n",
      " 6: -1.2795e+02 -1.3192e+02  4e+00  1e-04  3e-15\n",
      " 7: -1.2814e+02 -1.3143e+02  3e+00  9e-05  3e-15\n",
      " 8: -1.2847e+02 -1.3024e+02  2e+00  3e-05  3e-15\n",
      " 9: -1.2864e+02 -1.2985e+02  1e+00  2e-05  3e-15\n",
      "10: -1.2882e+02 -1.2949e+02  7e-01  7e-06  3e-15\n",
      "11: -1.2892e+02 -1.2929e+02  4e-01  3e-07  3e-15\n",
      "12: -1.2894e+02 -1.2922e+02  3e-01  6e-08  3e-15\n",
      "13: -1.2896e+02 -1.2919e+02  2e-01  4e-08  3e-15\n",
      "14: -1.2904e+02 -1.2909e+02  4e-02  4e-09  3e-15\n",
      "15: -1.2906e+02 -1.2906e+02  2e-03  2e-10  3e-15\n",
      "16: -1.2906e+02 -1.2906e+02  9e-05  9e-12  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6719e+02 -1.4578e+03  8e+03  3e+00  5e-15\n",
      " 1: -1.0331e+02 -8.4688e+02  1e+03  1e-01  4e-15\n",
      " 2: -1.0658e+02 -2.1843e+02  1e+02  2e-02  3e-15\n",
      " 3: -1.1750e+02 -1.5101e+02  3e+01  3e-03  3e-15\n",
      " 4: -1.1905e+02 -1.4378e+02  3e+01  2e-03  2e-15\n",
      " 5: -1.2139e+02 -1.3083e+02  1e+01  7e-04  3e-15\n",
      " 6: -1.2215e+02 -1.2816e+02  6e+00  2e-04  3e-15\n",
      " 7: -1.2258e+02 -1.2675e+02  4e+00  1e-04  2e-15\n",
      " 8: -1.2303e+02 -1.2500e+02  2e+00  5e-05  3e-15\n",
      " 9: -1.2313e+02 -1.2487e+02  2e+00  3e-05  3e-15\n",
      "10: -1.2319e+02 -1.2452e+02  1e+00  8e-06  3e-15\n",
      "11: -1.2329e+02 -1.2430e+02  1e+00  3e-06  2e-15\n",
      "12: -1.2342e+02 -1.2400e+02  6e-01  6e-07  3e-15\n",
      "13: -1.2349e+02 -1.2387e+02  4e-01  4e-07  3e-15\n",
      "14: -1.2356e+02 -1.2374e+02  2e-01  6e-08  3e-15\n",
      "15: -1.2358e+02 -1.2369e+02  1e-01  1e-09  3e-15\n",
      "16: -1.2361e+02 -1.2366e+02  5e-02  6e-10  3e-15\n",
      "17: -1.2362e+02 -1.2364e+02  3e-02  2e-10  3e-15\n",
      "18: -1.2363e+02 -1.2363e+02  8e-03  7e-15  3e-15\n",
      "19: -1.2363e+02 -1.2363e+02  4e-04  6e-15  3e-15\n",
      "20: -1.2363e+02 -1.2363e+02  4e-06  1e-15  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6249e+02 -1.4571e+03  8e+03  3e+00  4e-15\n",
      " 1: -1.0081e+02 -8.4646e+02  1e+03  2e-01  3e-15\n",
      " 2: -1.0344e+02 -2.1283e+02  1e+02  1e-02  4e-15\n",
      " 3: -1.1447e+02 -1.4726e+02  3e+01  3e-03  3e-15\n",
      " 4: -1.1770e+02 -1.3249e+02  2e+01  1e-03  2e-15\n",
      " 5: -1.1905e+02 -1.2739e+02  8e+00  4e-04  2e-15\n",
      " 6: -1.1981e+02 -1.2454e+02  5e+00  2e-04  2e-15\n",
      " 7: -1.2031e+02 -1.2299e+02  3e+00  9e-05  2e-15\n",
      " 8: -1.2067e+02 -1.2198e+02  1e+00  3e-05  2e-15\n",
      " 9: -1.2078e+02 -1.2167e+02  9e-01  1e-05  2e-15\n",
      "10: -1.2092e+02 -1.2136e+02  4e-01  3e-07  2e-15\n",
      "11: -1.2101e+02 -1.2123e+02  2e-01  1e-07  2e-15\n",
      "12: -1.2108e+02 -1.2113e+02  6e-02  3e-10  3e-15\n",
      "13: -1.2110e+02 -1.2110e+02  6e-03  3e-11  3e-15\n",
      "14: -1.2110e+02 -1.2110e+02  4e-04  1e-12  3e-15\n",
      "15: -1.2110e+02 -1.2110e+02  4e-06  1e-14  3e-15\n",
      "Optimal solution found.\n",
      "C=1     Gamma=0.01  -> Avg Val Acc: 0.9163\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5586e+02 -1.4292e+03  7e+03  3e+00  2e-15\n",
      " 1: -9.5310e+01 -8.2243e+02  1e+03  1e-01  2e-15\n",
      " 2: -9.9940e+01 -2.1250e+02  1e+02  1e-02  4e-15\n",
      " 3: -1.0869e+02 -1.3471e+02  3e+01  3e-03  1e-15\n",
      " 4: -1.1098e+02 -1.1806e+02  7e+00  1e-04  1e-15\n",
      " 5: -1.1158e+02 -1.1490e+02  3e+00  3e-05  1e-15\n",
      " 6: -1.1190e+02 -1.1349e+02  2e+00  7e-06  1e-15\n",
      " 7: -1.1206e+02 -1.1292e+02  9e-01  2e-06  1e-15\n",
      " 8: -1.1218e+02 -1.1257e+02  4e-01  2e-07  1e-15\n",
      " 9: -1.1225e+02 -1.1240e+02  2e-01  6e-08  1e-15\n",
      "10: -1.1229e+02 -1.1233e+02  4e-02  6e-09  1e-15\n",
      "11: -1.1230e+02 -1.1231e+02  1e-02  2e-15  1e-15\n",
      "12: -1.1230e+02 -1.1230e+02  6e-04  9e-15  1e-15\n",
      "13: -1.1230e+02 -1.1230e+02  9e-06  2e-14  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7275e+02 -1.4399e+03  7e+03  3e+00  2e-15\n",
      " 1: -1.0674e+02 -8.3706e+02  9e+02  1e-01  2e-15\n",
      " 2: -1.1187e+02 -2.1613e+02  1e+02  1e-02  3e-15\n",
      " 3: -1.1919e+02 -1.4177e+02  2e+01  2e-03  2e-15\n",
      " 4: -1.2112e+02 -1.2837e+02  7e+00  6e-04  9e-16\n",
      " 5: -1.2182e+02 -1.2516e+02  3e+00  7e-15  1e-15\n",
      " 6: -1.2210e+02 -1.2372e+02  2e+00  2e-15  1e-15\n",
      " 7: -1.2236e+02 -1.2294e+02  6e-01  3e-15  1e-15\n",
      " 8: -1.2245e+02 -1.2271e+02  3e-01  3e-15  1e-15\n",
      " 9: -1.2250e+02 -1.2261e+02  1e-01  1e-15  1e-15\n",
      "10: -1.2251e+02 -1.2257e+02  5e-02  7e-16  1e-15\n",
      "11: -1.2253e+02 -1.2255e+02  2e-02  4e-15  1e-15\n",
      "12: -1.2254e+02 -1.2254e+02  5e-03  7e-15  1e-15\n",
      "13: -1.2254e+02 -1.2254e+02  4e-04  3e-15  1e-15\n",
      "14: -1.2254e+02 -1.2254e+02  7e-06  6e-15  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6951e+02 -1.4262e+03  7e+03  3e+00  2e-15\n",
      " 1: -1.0445e+02 -8.2268e+02  9e+02  1e-01  2e-15\n",
      " 2: -1.0903e+02 -2.1690e+02  1e+02  1e-02  2e-15\n",
      " 3: -1.1597e+02 -1.4009e+02  2e+01  2e-03  1e-15\n",
      " 4: -1.1796e+02 -1.2569e+02  8e+00  6e-04  1e-15\n",
      " 5: -1.1852e+02 -1.2318e+02  5e+00  3e-04  9e-16\n",
      " 6: -1.1898e+02 -1.2058e+02  2e+00  6e-05  1e-15\n",
      " 7: -1.1920e+02 -1.1990e+02  7e-01  1e-05  1e-15\n",
      " 8: -1.1930e+02 -1.1967e+02  4e-01  4e-06  1e-15\n",
      " 9: -1.1937e+02 -1.1949e+02  1e-01  1e-15  1e-15\n",
      "10: -1.1940e+02 -1.1944e+02  3e-02  4e-15  1e-15\n",
      "11: -1.1941e+02 -1.1942e+02  2e-02  1e-14  1e-15\n",
      "12: -1.1941e+02 -1.1942e+02  9e-04  1e-14  1e-15\n",
      "13: -1.1941e+02 -1.1941e+02  2e-05  7e-16  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6224e+02 -1.4310e+03  7e+03  3e+00  2e-15\n",
      " 1: -9.9295e+01 -8.2534e+02  9e+02  1e-01  2e-15\n",
      " 2: -1.0363e+02 -2.0615e+02  1e+02  1e-02  2e-15\n",
      " 3: -1.1120e+02 -1.3254e+02  2e+01  2e-03  1e-15\n",
      " 4: -1.1316e+02 -1.1960e+02  6e+00  2e-04  1e-15\n",
      " 5: -1.1366e+02 -1.1723e+02  4e+00  5e-05  1e-15\n",
      " 6: -1.1401e+02 -1.1538e+02  1e+00  1e-05  1e-15\n",
      " 7: -1.1414e+02 -1.1506e+02  9e-01  4e-06  1e-15\n",
      " 8: -1.1429e+02 -1.1459e+02  3e-01  3e-07  1e-15\n",
      " 9: -1.1434e+02 -1.1449e+02  1e-01  1e-14  1e-15\n",
      "10: -1.1438e+02 -1.1442e+02  5e-02  5e-15  1e-15\n",
      "11: -1.1438e+02 -1.1442e+02  4e-02  1e-15  1e-15\n",
      "12: -1.1439e+02 -1.1440e+02  6e-03  6e-15  1e-15\n",
      "13: -1.1439e+02 -1.1439e+02  1e-04  1e-15  1e-15\n",
      "14: -1.1439e+02 -1.1439e+02  3e-06  9e-15  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5496e+02 -1.4199e+03  7e+03  3e+00  2e-15\n",
      " 1: -9.5440e+01 -8.1425e+02  9e+02  1e-01  2e-15\n",
      " 2: -1.0030e+02 -2.1433e+02  1e+02  2e-02  3e-15\n",
      " 3: -1.0890e+02 -1.2904e+02  2e+01  2e-03  1e-15\n",
      " 4: -1.1035e+02 -1.2137e+02  1e+01  5e-04  1e-15\n",
      " 5: -1.1118e+02 -1.1521e+02  4e+00  2e-04  1e-15\n",
      " 6: -1.1159e+02 -1.1332e+02  2e+00  2e-05  1e-15\n",
      " 7: -1.1183e+02 -1.1262e+02  8e-01  6e-07  1e-15\n",
      " 8: -1.1192e+02 -1.1238e+02  5e-01  9e-08  1e-15\n",
      " 9: -1.1201e+02 -1.1215e+02  1e-01  2e-08  1e-15\n",
      "10: -1.1205e+02 -1.1208e+02  3e-02  5e-15  1e-15\n",
      "11: -1.1206e+02 -1.1207e+02  1e-02  1e-15  1e-15\n",
      "12: -1.1206e+02 -1.1206e+02  7e-04  9e-16  1e-15\n",
      "13: -1.1206e+02 -1.1206e+02  2e-05  7e-15  1e-15\n",
      "Optimal solution found.\n",
      "C=1     Gamma=0.1   -> Avg Val Acc: 0.9138\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3946e+02 -1.4438e+03  7e+03  3e+00  1e-15\n",
      " 1: -8.2233e+01 -8.5338e+02  9e+02  9e-02  9e-16\n",
      " 2: -1.0041e+02 -2.0865e+02  1e+02  1e-02  1e-15\n",
      " 3: -1.1140e+02 -1.3820e+02  3e+01  2e-03  6e-16\n",
      " 4: -1.1435e+02 -1.2343e+02  9e+00  4e-04  5e-16\n",
      " 5: -1.1567e+02 -1.1882e+02  3e+00  7e-05  5e-16\n",
      " 6: -1.1620e+02 -1.1733e+02  1e+00  2e-05  4e-16\n",
      " 7: -1.1646e+02 -1.1668e+02  2e-01  1e-14  5e-16\n",
      " 8: -1.1651e+02 -1.1658e+02  7e-02  4e-15  4e-16\n",
      " 9: -1.1653e+02 -1.1654e+02  6e-03  4e-15  5e-16\n",
      "10: -1.1653e+02 -1.1653e+02  1e-04  5e-15  6e-16\n",
      "11: -1.1653e+02 -1.1653e+02  1e-06  7e-16  5e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5281e+02 -1.4846e+03  7e+03  3e+00  1e-15\n",
      " 1: -9.2458e+01 -8.9803e+02  1e+03  1e-01  8e-16\n",
      " 2: -1.0915e+02 -2.2271e+02  1e+02  1e-02  8e-16\n",
      " 3: -1.2027e+02 -1.4806e+02  3e+01  2e-03  6e-16\n",
      " 4: -1.2348e+02 -1.3126e+02  8e+00  3e-04  5e-16\n",
      " 5: -1.2475e+02 -1.2719e+02  2e+00  3e-05  5e-16\n",
      " 6: -1.2521e+02 -1.2593e+02  7e-01  3e-06  5e-16\n",
      " 7: -1.2536e+02 -1.2556e+02  2e-01  5e-07  5e-16\n",
      " 8: -1.2542e+02 -1.2544e+02  2e-02  3e-08  6e-16\n",
      " 9: -1.2542e+02 -1.2543e+02  3e-03  5e-10  5e-16\n",
      "10: -1.2542e+02 -1.2542e+02  9e-05  1e-11  6e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.4923e+02 -1.4269e+03  7e+03  3e+00  1e-15\n",
      " 1: -9.0471e+01 -8.4402e+02  9e+02  8e-02  8e-16\n",
      " 2: -1.0773e+02 -2.1478e+02  1e+02  1e-02  8e-16\n",
      " 3: -1.1803e+02 -1.4454e+02  3e+01  2e-03  6e-16\n",
      " 4: -1.2122e+02 -1.2947e+02  8e+00  2e-04  5e-16\n",
      " 5: -1.2243e+02 -1.2537e+02  3e+00  4e-05  5e-16\n",
      " 6: -1.2295e+02 -1.2394e+02  1e+00  5e-06  5e-16\n",
      " 7: -1.2318e+02 -1.2338e+02  2e-01  8e-15  5e-16\n",
      " 8: -1.2323e+02 -1.2326e+02  3e-02  8e-15  5e-16\n",
      " 9: -1.2324e+02 -1.2325e+02  2e-03  7e-15  5e-16\n",
      "10: -1.2324e+02 -1.2324e+02  8e-05  6e-15  6e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.4408e+02 -1.4641e+03  7e+03  3e+00  1e-15\n",
      " 1: -8.5754e+01 -8.7481e+02  9e+02  1e-01  8e-16\n",
      " 2: -1.0209e+02 -2.1025e+02  1e+02  1e-02  9e-16\n",
      " 3: -1.1255e+02 -1.3725e+02  3e+01  2e-03  6e-16\n",
      " 4: -1.1532e+02 -1.2324e+02  8e+00  2e-04  5e-16\n",
      " 5: -1.1653e+02 -1.1941e+02  3e+00  2e-14  5e-16\n",
      " 6: -1.1695e+02 -1.1792e+02  1e+00  6e-15  5e-16\n",
      " 7: -1.1714e+02 -1.1738e+02  2e-01  2e-15  5e-16\n",
      " 8: -1.1720e+02 -1.1726e+02  6e-02  7e-15  6e-16\n",
      " 9: -1.1722e+02 -1.1722e+02  3e-03  1e-15  6e-16\n",
      "10: -1.1722e+02 -1.1722e+02  5e-05  1e-14  5e-16\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3821e+02 -1.4505e+03  7e+03  3e+00  1e-15\n",
      " 1: -8.2289e+01 -8.6416e+02  9e+02  1e-01  8e-16\n",
      " 2: -9.8966e+01 -2.0911e+02  1e+02  1e-02  7e-16\n",
      " 3: -1.0986e+02 -1.3872e+02  3e+01  2e-03  5e-16\n",
      " 4: -1.1326e+02 -1.2165e+02  8e+00  4e-04  4e-16\n",
      " 5: -1.1456e+02 -1.1749e+02  3e+00  6e-05  5e-16\n",
      " 6: -1.1509e+02 -1.1607e+02  1e+00  5e-15  5e-16\n",
      " 7: -1.1528e+02 -1.1558e+02  3e-01  6e-15  5e-16\n",
      " 8: -1.1535e+02 -1.1541e+02  5e-02  2e-15  5e-16\n",
      " 9: -1.1537e+02 -1.1537e+02  2e-03  5e-16  5e-16\n",
      "10: -1.1537e+02 -1.1537e+02  1e-04  7e-15  5e-16\n",
      "Optimal solution found.\n",
      "C=1     Gamma=1     -> Avg Val Acc: 0.9100\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.5430e+01 -2.6994e+04  5e+04  3e-01  3e-14\n",
      " 1: -3.1582e+02 -4.2920e+03  4e+03  5e-03  2e-14\n",
      " 2: -9.1622e+02 -2.0299e+03  1e+03  1e-03  2e-14\n",
      " 3: -1.0540e+03 -1.6531e+03  6e+02  5e-04  3e-14\n",
      " 4: -1.1206e+03 -1.4501e+03  3e+02  3e-04  3e-14\n",
      " 5: -1.1407e+03 -1.3948e+03  3e+02  2e-04  3e-14\n",
      " 6: -1.1599e+03 -1.3372e+03  2e+02  1e-04  3e-14\n",
      " 7: -1.1785e+03 -1.2814e+03  1e+02  5e-05  3e-14\n",
      " 8: -1.1820e+03 -1.2772e+03  1e+02  5e-05  3e-14\n",
      " 9: -1.1892e+03 -1.2616e+03  7e+01  3e-05  3e-14\n",
      "10: -1.1932e+03 -1.2506e+03  6e+01  2e-05  3e-14\n",
      "11: -1.1984e+03 -1.2371e+03  4e+01  1e-05  3e-14\n",
      "12: -1.2042e+03 -1.2258e+03  2e+01  2e-06  3e-14\n",
      "13: -1.2047e+03 -1.2233e+03  2e+01  1e-06  3e-14\n",
      "14: -1.2079e+03 -1.2180e+03  1e+01  8e-08  4e-14\n",
      "15: -1.2100e+03 -1.2142e+03  4e+00  2e-08  3e-14\n",
      "16: -1.2110e+03 -1.2128e+03  2e+00  3e-09  4e-14\n",
      "17: -1.2117e+03 -1.2119e+03  2e-01  3e-10  4e-14\n",
      "18: -1.2118e+03 -1.2118e+03  4e-02  5e-11  3e-14\n",
      "19: -1.2118e+03 -1.2118e+03  6e-03  5e-13  4e-14\n",
      "20: -1.2118e+03 -1.2118e+03  6e-05  8e-14  4e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6491e+02 -2.6827e+04  5e+04  3e-01  3e-14\n",
      " 1: -4.6241e+02 -4.3592e+03  4e+03  5e-03  2e-14\n",
      " 2: -1.0397e+03 -2.1369e+03  1e+03  1e-03  2e-14\n",
      " 3: -1.1763e+03 -1.8047e+03  6e+02  6e-04  2e-14\n",
      " 4: -1.2404e+03 -1.6000e+03  4e+02  3e-04  2e-14\n",
      " 5: -1.2709e+03 -1.5076e+03  2e+02  2e-04  2e-14\n",
      " 6: -1.2788e+03 -1.4865e+03  2e+02  1e-04  3e-14\n",
      " 7: -1.2948e+03 -1.4332e+03  1e+02  6e-05  3e-14\n",
      " 8: -1.3061e+03 -1.3954e+03  9e+01  4e-05  3e-14\n",
      " 9: -1.3117e+03 -1.3822e+03  7e+01  3e-05  3e-14\n",
      "10: -1.3185e+03 -1.3638e+03  5e+01  7e-06  3e-14\n",
      "11: -1.3244e+03 -1.3522e+03  3e+01  3e-06  3e-14\n",
      "12: -1.3242e+03 -1.3491e+03  2e+01  1e-06  3e-14\n",
      "13: -1.3285e+03 -1.3438e+03  2e+01  4e-07  3e-14\n",
      "14: -1.3321e+03 -1.3360e+03  4e+00  6e-08  3e-14\n",
      "15: -1.3329e+03 -1.3347e+03  2e+00  2e-08  3e-14\n",
      "16: -1.3331e+03 -1.3344e+03  1e+00  9e-09  3e-14\n",
      "17: -1.3334e+03 -1.3340e+03  6e-01  4e-09  3e-14\n",
      "18: -1.3335e+03 -1.3338e+03  3e-01  3e-12  4e-14\n",
      "19: -1.3337e+03 -1.3337e+03  4e-02  3e-13  4e-14\n",
      "20: -1.3337e+03 -1.3337e+03  8e-04  2e-14  4e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.4923e+02 -2.6937e+04  5e+04  3e-01  3e-14\n",
      " 1: -4.2437e+02 -4.3488e+03  4e+03  4e-03  3e-14\n",
      " 2: -1.0012e+03 -2.1572e+03  1e+03  1e-03  3e-14\n",
      " 3: -1.1414e+03 -1.7870e+03  6e+02  5e-04  3e-14\n",
      " 4: -1.2178e+03 -1.5384e+03  3e+02  2e-04  3e-14\n",
      " 5: -1.2459e+03 -1.4597e+03  2e+02  1e-04  3e-14\n",
      " 6: -1.2542e+03 -1.4384e+03  2e+02  1e-04  3e-14\n",
      " 7: -1.2729e+03 -1.3749e+03  1e+02  2e-05  3e-14\n",
      " 8: -1.2763e+03 -1.3648e+03  9e+01  2e-05  3e-14\n",
      " 9: -1.2824e+03 -1.3551e+03  7e+01  6e-06  3e-14\n",
      "10: -1.2907e+03 -1.3284e+03  4e+01  3e-06  3e-14\n",
      "11: -1.2928e+03 -1.3201e+03  3e+01  1e-06  3e-14\n",
      "12: -1.2942e+03 -1.3198e+03  3e+01  8e-07  3e-14\n",
      "13: -1.2989e+03 -1.3115e+03  1e+01  3e-07  3e-14\n",
      "14: -1.3013e+03 -1.3073e+03  6e+00  9e-08  3e-14\n",
      "15: -1.3029e+03 -1.3046e+03  2e+00  2e-08  4e-14\n",
      "16: -1.3035e+03 -1.3037e+03  3e-01  1e-09  4e-14\n",
      "17: -1.3036e+03 -1.3036e+03  7e-02  3e-10  4e-14\n",
      "18: -1.3036e+03 -1.3036e+03  2e-03  3e-12  4e-14\n",
      "19: -1.3036e+03 -1.3036e+03  2e-05  1e-13  4e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.1930e+01 -2.6317e+04  5e+04  3e-01  3e-14\n",
      " 1: -3.7910e+02 -4.1809e+03  4e+03  4e-03  3e-14\n",
      " 2: -9.6679e+02 -2.0363e+03  1e+03  9e-04  3e-14\n",
      " 3: -1.0844e+03 -1.7533e+03  7e+02  5e-04  3e-14\n",
      " 4: -1.1385e+03 -1.5810e+03  4e+02  3e-04  3e-14\n",
      " 5: -1.1704e+03 -1.4768e+03  3e+02  2e-04  3e-14\n",
      " 6: -1.1866e+03 -1.4257e+03  2e+02  1e-04  3e-14\n",
      " 7: -1.2034e+03 -1.3732e+03  2e+02  7e-05  3e-14\n",
      " 8: -1.2144e+03 -1.3343e+03  1e+02  4e-05  3e-14\n",
      " 9: -1.2214e+03 -1.3138e+03  9e+01  3e-05  3e-14\n",
      "10: -1.2268e+03 -1.2983e+03  7e+01  2e-05  3e-14\n",
      "11: -1.2323e+03 -1.2823e+03  5e+01  9e-06  3e-14\n",
      "12: -1.2383e+03 -1.2659e+03  3e+01  4e-06  3e-14\n",
      "13: -1.2398e+03 -1.2652e+03  3e+01  3e-06  3e-14\n",
      "14: -1.2399e+03 -1.2632e+03  2e+01  2e-06  3e-14\n",
      "15: -1.2435e+03 -1.2560e+03  1e+01  3e-14  4e-14\n",
      "16: -1.2457e+03 -1.2521e+03  6e+00  7e-15  4e-14\n",
      "17: -1.2466e+03 -1.2508e+03  4e+00  7e-14  4e-14\n",
      "18: -1.2480e+03 -1.2487e+03  8e-01  1e-14  4e-14\n",
      "19: -1.2483e+03 -1.2483e+03  4e-02  8e-14  4e-14\n",
      "20: -1.2483e+03 -1.2483e+03  7e-04  5e-14  4e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.9287e+01 -2.7140e+04  5e+04  3e-01  3e-14\n",
      " 1: -3.0787e+02 -4.3201e+03  4e+03  4e-03  2e-14\n",
      " 2: -9.1815e+02 -2.0408e+03  1e+03  1e-03  3e-14\n",
      " 3: -1.0568e+03 -1.6947e+03  6e+02  5e-04  3e-14\n",
      " 4: -1.1316e+03 -1.4669e+03  3e+02  2e-04  3e-14\n",
      " 5: -1.1569e+03 -1.3968e+03  2e+02  2e-04  3e-14\n",
      " 6: -1.1784e+03 -1.3362e+03  2e+02  9e-05  3e-14\n",
      " 7: -1.1916e+03 -1.3037e+03  1e+02  6e-05  3e-14\n",
      " 8: -1.2015e+03 -1.2790e+03  8e+01  3e-05  3e-14\n",
      " 9: -1.2093e+03 -1.2627e+03  5e+01  1e-05  3e-14\n",
      "10: -1.2149e+03 -1.2513e+03  4e+01  4e-06  3e-14\n",
      "11: -1.2155e+03 -1.2457e+03  3e+01  1e-06  3e-14\n",
      "12: -1.2184e+03 -1.2413e+03  2e+01  7e-07  3e-14\n",
      "13: -1.2194e+03 -1.2401e+03  2e+01  5e-07  3e-14\n",
      "14: -1.2226e+03 -1.2344e+03  1e+01  1e-07  3e-14\n",
      "15: -1.2252e+03 -1.2298e+03  5e+00  3e-08  3e-14\n",
      "16: -1.2258e+03 -1.2288e+03  3e+00  1e-08  3e-14\n",
      "17: -1.2266e+03 -1.2277e+03  1e+00  2e-09  3e-14\n",
      "18: -1.2271e+03 -1.2271e+03  9e-02  2e-10  4e-14\n",
      "19: -1.2271e+03 -1.2271e+03  1e-03  2e-12  4e-14\n",
      "20: -1.2271e+03 -1.2271e+03  1e-05  7e-14  4e-14\n",
      "Optimal solution found.\n",
      "C=10    Gamma=0.001 -> Avg Val Acc: 0.9150\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.8577e+02 -2.2595e+04  4e+04  3e-01  2e-14\n",
      " 1: -7.2909e+02 -3.6796e+03  3e+03  5e-03  2e-14\n",
      " 2: -9.8399e+02 -1.7566e+03  8e+02  9e-04  2e-14\n",
      " 3: -1.0596e+03 -1.3620e+03  3e+02  2e-04  2e-14\n",
      " 4: -1.0831e+03 -1.2385e+03  2e+02  1e-04  2e-14\n",
      " 5: -1.0970e+03 -1.1640e+03  7e+01  3e-05  2e-14\n",
      " 6: -1.1023e+03 -1.1430e+03  4e+01  1e-05  2e-14\n",
      " 7: -1.1068e+03 -1.1239e+03  2e+01  4e-06  2e-14\n",
      " 8: -1.1080e+03 -1.1204e+03  1e+01  2e-06  2e-14\n",
      " 9: -1.1097e+03 -1.1156e+03  6e+00  7e-07  3e-14\n",
      "10: -1.1104e+03 -1.1142e+03  4e+00  1e-07  3e-14\n",
      "11: -1.1111e+03 -1.1126e+03  2e+00  3e-08  3e-14\n",
      "12: -1.1114e+03 -1.1121e+03  7e-01  5e-09  2e-14\n",
      "13: -1.1116e+03 -1.1118e+03  3e-01  2e-09  2e-14\n",
      "14: -1.1117e+03 -1.1118e+03  1e-01  4e-10  2e-14\n",
      "15: -1.1117e+03 -1.1117e+03  4e-03  1e-11  3e-14\n",
      "16: -1.1117e+03 -1.1117e+03  6e-05  1e-13  3e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.8909e+02 -2.3003e+04  4e+04  3e-01  3e-14\n",
      " 1: -8.3171e+02 -3.8887e+03  3e+03  6e-03  2e-14\n",
      " 2: -1.0954e+03 -1.8673e+03  8e+02  1e-03  2e-14\n",
      " 3: -1.1612e+03 -1.5221e+03  4e+02  4e-04  2e-14\n",
      " 4: -1.1849e+03 -1.3881e+03  2e+02  2e-04  2e-14\n",
      " 5: -1.2007e+03 -1.2998e+03  1e+02  5e-05  2e-14\n",
      " 6: -1.2072e+03 -1.2669e+03  6e+01  2e-05  2e-14\n",
      " 7: -1.2106e+03 -1.2506e+03  4e+01  1e-05  2e-14\n",
      " 8: -1.2123e+03 -1.2434e+03  3e+01  9e-06  2e-14\n",
      " 9: -1.2144e+03 -1.2339e+03  2e+01  4e-06  2e-14\n",
      "10: -1.2160e+03 -1.2275e+03  1e+01  2e-06  2e-14\n",
      "11: -1.2169e+03 -1.2242e+03  7e+00  1e-06  2e-14\n",
      "12: -1.2179e+03 -1.2211e+03  3e+00  3e-07  2e-14\n",
      "13: -1.2183e+03 -1.2205e+03  2e+00  6e-08  2e-14\n",
      "14: -1.2187e+03 -1.2198e+03  1e+00  3e-08  2e-14\n",
      "15: -1.2189e+03 -1.2194e+03  5e-01  1e-08  2e-14\n",
      "16: -1.2190e+03 -1.2192e+03  2e-01  1e-09  2e-14\n",
      "17: -1.2191e+03 -1.2191e+03  4e-02  9e-11  2e-14\n",
      "18: -1.2191e+03 -1.2191e+03  5e-04  1e-12  2e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.7263e+02 -2.1167e+04  3e+04  2e-01  3e-14\n",
      " 1: -8.2691e+02 -3.5383e+03  3e+03  7e-03  2e-14\n",
      " 2: -1.0758e+03 -1.8292e+03  8e+02  1e-03  2e-14\n",
      " 3: -1.1282e+03 -1.5345e+03  4e+02  5e-04  2e-14\n",
      " 4: -1.1598e+03 -1.3336e+03  2e+02  2e-04  2e-14\n",
      " 5: -1.1691e+03 -1.2863e+03  1e+02  1e-04  2e-14\n",
      " 6: -1.1741e+03 -1.2620e+03  9e+01  8e-05  2e-14\n",
      " 7: -1.1813e+03 -1.2240e+03  4e+01  3e-05  2e-14\n",
      " 8: -1.1830e+03 -1.2188e+03  4e+01  2e-05  2e-14\n",
      " 9: -1.1855e+03 -1.2101e+03  2e+01  9e-06  2e-14\n",
      "10: -1.1872e+03 -1.2019e+03  1e+01  8e-14  3e-14\n",
      "11: -1.1893e+03 -1.1958e+03  7e+00  5e-15  2e-14\n",
      "12: -1.1897e+03 -1.1952e+03  5e+00  5e-15  2e-14\n",
      "13: -1.1902e+03 -1.1942e+03  4e+00  2e-14  2e-14\n",
      "14: -1.1906e+03 -1.1934e+03  3e+00  3e-14  2e-14\n",
      "15: -1.1907e+03 -1.1932e+03  3e+00  9e-14  2e-14\n",
      "16: -1.1908e+03 -1.1928e+03  2e+00  2e-13  3e-14\n",
      "17: -1.1912e+03 -1.1922e+03  1e+00  7e-14  3e-14\n",
      "18: -1.1913e+03 -1.1920e+03  7e-01  3e-14  2e-14\n",
      "19: -1.1915e+03 -1.1917e+03  2e-01  7e-14  3e-14\n",
      "20: -1.1916e+03 -1.1916e+03  9e-03  2e-13  3e-14\n",
      "21: -1.1916e+03 -1.1916e+03  2e-04  7e-14  3e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -7.2273e+02 -2.2382e+04  4e+04  3e-01  3e-14\n",
      " 1: -7.6668e+02 -3.7197e+03  3e+03  6e-03  2e-14\n",
      " 2: -1.0209e+03 -1.7785e+03  8e+02  1e-03  2e-14\n",
      " 3: -1.0905e+03 -1.4156e+03  3e+02  2e-04  3e-14\n",
      " 4: -1.1029e+03 -1.3518e+03  2e+02  1e-04  2e-14\n",
      " 5: -1.1223e+03 -1.2267e+03  1e+02  4e-05  2e-14\n",
      " 6: -1.1269e+03 -1.2033e+03  8e+01  3e-05  2e-14\n",
      " 7: -1.1324e+03 -1.1768e+03  4e+01  1e-05  2e-14\n",
      " 8: -1.1371e+03 -1.1567e+03  2e+01  2e-06  3e-14\n",
      " 9: -1.1386e+03 -1.1519e+03  1e+01  1e-06  2e-14\n",
      "10: -1.1396e+03 -1.1479e+03  8e+00  5e-07  2e-14\n",
      "11: -1.1407e+03 -1.1450e+03  4e+00  2e-07  3e-14\n",
      "12: -1.1414e+03 -1.1436e+03  2e+00  2e-08  3e-14\n",
      "13: -1.1418e+03 -1.1429e+03  1e+00  7e-09  3e-14\n",
      "14: -1.1419e+03 -1.1426e+03  7e-01  3e-09  3e-14\n",
      "15: -1.1421e+03 -1.1423e+03  3e-01  8e-10  3e-14\n",
      "16: -1.1421e+03 -1.1423e+03  1e-01  3e-14  3e-14\n",
      "17: -1.1422e+03 -1.1422e+03  2e-02  7e-14  3e-14\n",
      "18: -1.1422e+03 -1.1422e+03  1e-03  8e-14  3e-14\n",
      "19: -1.1422e+03 -1.1422e+03  3e-05  1e-13  3e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.8549e+02 -2.2331e+04  4e+04  3e-01  3e-14\n",
      " 1: -7.2368e+02 -3.7081e+03  3e+03  7e-03  2e-14\n",
      " 2: -9.8705e+02 -1.7443e+03  8e+02  1e-03  2e-14\n",
      " 3: -1.0378e+03 -1.5511e+03  5e+02  7e-04  2e-14\n",
      " 4: -1.0857e+03 -1.2686e+03  2e+02  2e-04  2e-14\n",
      " 5: -1.1028e+03 -1.1763e+03  7e+01  7e-05  2e-14\n",
      " 6: -1.1079e+03 -1.1597e+03  5e+01  3e-05  2e-14\n",
      " 7: -1.1127e+03 -1.1423e+03  3e+01  1e-05  2e-14\n",
      " 8: -1.1155e+03 -1.1337e+03  2e+01  3e-06  2e-14\n",
      " 9: -1.1166e+03 -1.1301e+03  1e+01  2e-06  2e-14\n",
      "10: -1.1183e+03 -1.1245e+03  6e+00  4e-07  2e-14\n",
      "11: -1.1189e+03 -1.1234e+03  5e+00  2e-07  2e-14\n",
      "12: -1.1192e+03 -1.1227e+03  3e+00  6e-08  2e-14\n",
      "13: -1.1196e+03 -1.1219e+03  2e+00  3e-08  2e-14\n",
      "14: -1.1200e+03 -1.1212e+03  1e+00  1e-08  2e-14\n",
      "15: -1.1203e+03 -1.1206e+03  3e-01  1e-09  3e-14\n",
      "16: -1.1204e+03 -1.1205e+03  6e-02  3e-10  2e-14\n",
      "17: -1.1204e+03 -1.1204e+03  3e-03  5e-12  3e-14\n",
      "18: -1.1204e+03 -1.1204e+03  1e-04  2e-13  3e-14\n",
      "Optimal solution found.\n",
      "C=10    Gamma=0.01  -> Avg Val Acc: 0.9138\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.6106e+02 -2.0708e+04  4e+04  2e-01  1e-14\n",
      " 1: -6.2432e+02 -3.7724e+03  4e+03  1e-02  8e-15\n",
      " 2: -8.9366e+02 -1.8904e+03  1e+03  3e-03  9e-15\n",
      " 3: -1.0087e+03 -1.3295e+03  3e+02  4e-04  1e-14\n",
      " 4: -1.0398e+03 -1.1542e+03  1e+02  5e-05  1e-14\n",
      " 5: -1.0517e+03 -1.0928e+03  4e+01  5e-06  1e-14\n",
      " 6: -1.0562e+03 -1.0707e+03  1e+01  9e-07  1e-14\n",
      " 7: -1.0577e+03 -1.0654e+03  8e+00  3e-14  1e-14\n",
      " 8: -1.0587e+03 -1.0613e+03  3e+00  3e-14  9e-15\n",
      " 9: -1.0589e+03 -1.0605e+03  2e+00  3e-14  9e-15\n",
      "10: -1.0591e+03 -1.0599e+03  8e-01  2e-14  9e-15\n",
      "11: -1.0593e+03 -1.0596e+03  3e-01  4e-14  1e-14\n",
      "12: -1.0593e+03 -1.0594e+03  1e-01  6e-15  1e-14\n",
      "13: -1.0593e+03 -1.0594e+03  9e-02  5e-14  9e-15\n",
      "14: -1.0594e+03 -1.0594e+03  1e-02  4e-14  1e-14\n",
      "15: -1.0594e+03 -1.0594e+03  4e-03  2e-16  1e-14\n",
      "16: -1.0594e+03 -1.0594e+03  6e-05  2e-14  1e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.6194e+02 -2.1306e+04  4e+04  2e-01  1e-14\n",
      " 1: -7.3695e+02 -4.0136e+03  4e+03  1e-02  8e-15\n",
      " 2: -1.0052e+03 -1.8638e+03  9e+02  2e-03  9e-15\n",
      " 3: -1.1089e+03 -1.3319e+03  2e+02  4e-04  9e-15\n",
      " 4: -1.1389e+03 -1.2133e+03  7e+01  3e-05  1e-14\n",
      " 5: -1.1473e+03 -1.1708e+03  2e+01  4e-06  1e-14\n",
      " 6: -1.1504e+03 -1.1572e+03  7e+00  2e-07  1e-14\n",
      " 7: -1.1513e+03 -1.1542e+03  3e+00  3e-08  1e-14\n",
      " 8: -1.1516e+03 -1.1534e+03  2e+00  7e-09  9e-15\n",
      " 9: -1.1519e+03 -1.1524e+03  5e-01  7e-10  1e-14\n",
      "10: -1.1520e+03 -1.1522e+03  2e-01  1e-10  1e-14\n",
      "11: -1.1521e+03 -1.1521e+03  7e-02  2e-11  1e-14\n",
      "12: -1.1521e+03 -1.1521e+03  2e-02  3e-12  1e-14\n",
      "13: -1.1521e+03 -1.1521e+03  6e-04  4e-14  1e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.3480e+02 -2.0542e+04  3e+04  2e-01  1e-14\n",
      " 1: -7.0998e+02 -4.1188e+03  4e+03  2e-02  9e-15\n",
      " 2: -9.4712e+02 -1.8664e+03  1e+03  3e-03  1e-14\n",
      " 3: -1.0592e+03 -1.3029e+03  2e+02  5e-04  1e-14\n",
      " 4: -1.0923e+03 -1.1933e+03  1e+02  1e-04  1e-14\n",
      " 5: -1.1057e+03 -1.1354e+03  3e+01  2e-05  1e-14\n",
      " 6: -1.1100e+03 -1.1193e+03  9e+00  3e-06  1e-14\n",
      " 7: -1.1113e+03 -1.1154e+03  4e+00  8e-08  1e-14\n",
      " 8: -1.1119e+03 -1.1132e+03  1e+00  2e-08  1e-14\n",
      " 9: -1.1120e+03 -1.1129e+03  8e-01  5e-09  1e-14\n",
      "10: -1.1122e+03 -1.1125e+03  4e-01  2e-09  1e-14\n",
      "11: -1.1122e+03 -1.1124e+03  1e-01  3e-14  1e-14\n",
      "12: -1.1123e+03 -1.1123e+03  3e-02  5e-14  1e-14\n",
      "13: -1.1123e+03 -1.1123e+03  9e-04  1e-13  1e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.0861e+02 -2.0822e+04  3e+04  2e-01  1e-14\n",
      " 1: -6.7839e+02 -3.6893e+03  3e+03  1e-02  1e-14\n",
      " 2: -9.3897e+02 -1.7622e+03  8e+02  2e-03  1e-14\n",
      " 3: -1.0149e+03 -1.3863e+03  4e+02  5e-04  1e-14\n",
      " 4: -1.0516e+03 -1.1478e+03  1e+02  1e-04  1e-14\n",
      " 5: -1.0628e+03 -1.1020e+03  4e+01  8e-06  1e-14\n",
      " 6: -1.0666e+03 -1.0806e+03  1e+01  2e-06  9e-15\n",
      " 7: -1.0683e+03 -1.0730e+03  5e+00  3e-07  1e-14\n",
      " 8: -1.0688e+03 -1.0717e+03  3e+00  8e-08  1e-14\n",
      " 9: -1.0692e+03 -1.0702e+03  1e+00  2e-08  1e-14\n",
      "10: -1.0693e+03 -1.0698e+03  5e-01  9e-10  1e-14\n",
      "11: -1.0694e+03 -1.0696e+03  1e-01  2e-10  1e-14\n",
      "12: -1.0694e+03 -1.0696e+03  1e-01  8e-11  1e-14\n",
      "13: -1.0695e+03 -1.0695e+03  3e-02  2e-11  1e-14\n",
      "14: -1.0695e+03 -1.0695e+03  3e-02  1e-11  1e-14\n",
      "15: -1.0695e+03 -1.0695e+03  8e-03  5e-12  1e-14\n",
      "16: -1.0695e+03 -1.0695e+03  8e-03  2e-12  1e-14\n",
      "17: -1.0695e+03 -1.0695e+03  2e-03  4e-13  1e-14\n",
      "18: -1.0695e+03 -1.0695e+03  6e-04  8e-14  1e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.4553e+02 -2.0189e+04  3e+04  2e-01  1e-14\n",
      " 1: -6.1846e+02 -3.5345e+03  3e+03  1e-02  8e-15\n",
      " 2: -9.0016e+02 -1.6971e+03  8e+02  1e-03  1e-14\n",
      " 3: -9.9794e+02 -1.2242e+03  2e+02  2e-04  9e-15\n",
      " 4: -1.0258e+03 -1.1053e+03  8e+01  4e-05  1e-14\n",
      " 5: -1.0356e+03 -1.0718e+03  4e+01  2e-06  1e-14\n",
      " 6: -1.0397e+03 -1.0506e+03  1e+01  3e-07  1e-14\n",
      " 7: -1.0411e+03 -1.0450e+03  4e+00  5e-08  1e-14\n",
      " 8: -1.0417e+03 -1.0433e+03  2e+00  9e-10  1e-14\n",
      " 9: -1.0420e+03 -1.0426e+03  6e-01  2e-10  1e-14\n",
      "10: -1.0421e+03 -1.0422e+03  1e-01  1e-11  1e-14\n",
      "11: -1.0421e+03 -1.0422e+03  5e-02  3e-12  1e-14\n",
      "12: -1.0421e+03 -1.0422e+03  1e-02  3e-14  1e-14\n",
      "13: -1.0422e+03 -1.0422e+03  2e-04  3e-14  1e-14\n",
      "Optimal solution found.\n",
      "C=10    Gamma=0.1   -> Avg Val Acc: 0.9138\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.9968e+02 -2.0842e+04  3e+04  2e-01  6e-15\n",
      " 1: -1.4683e+02 -3.2204e+03  3e+03  1e-02  4e-15\n",
      " 2: -5.5923e+02 -1.4240e+03  9e+02  2e-03  3e-15\n",
      " 3: -6.9490e+02 -1.0107e+03  3e+02  3e-04  3e-15\n",
      " 4: -7.4067e+02 -8.4657e+02  1e+02  1e-05  3e-15\n",
      " 5: -7.5553e+02 -7.7980e+02  2e+01  8e-07  3e-15\n",
      " 6: -7.5922e+02 -7.6761e+02  8e+00  1e-07  3e-15\n",
      " 7: -7.6066e+02 -7.6329e+02  3e+00  2e-08  3e-15\n",
      " 8: -7.6121e+02 -7.6196e+02  7e-01  3e-14  3e-15\n",
      " 9: -7.6140e+02 -7.6150e+02  1e-01  2e-14  3e-15\n",
      "10: -7.6144e+02 -7.6144e+02  3e-03  6e-14  3e-15\n",
      "11: -7.6144e+02 -7.6144e+02  6e-05  7e-14  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.6342e+02 -2.3052e+04  4e+04  2e-01  6e-15\n",
      " 1: -1.5905e+02 -3.4720e+03  4e+03  8e-03  3e-15\n",
      " 2: -5.6535e+02 -1.5782e+03  1e+03  2e-03  3e-15\n",
      " 3: -7.1443e+02 -1.0429e+03  3e+02  5e-04  3e-15\n",
      " 4: -7.7044e+02 -8.7038e+02  1e+02  3e-05  3e-15\n",
      " 5: -7.8453e+02 -8.1834e+02  3e+01  5e-06  3e-15\n",
      " 6: -7.8992e+02 -7.9922e+02  9e+00  1e-07  3e-15\n",
      " 7: -7.9156e+02 -7.9427e+02  3e+00  8e-09  3e-15\n",
      " 8: -7.9207e+02 -7.9291e+02  8e-01  9e-10  3e-15\n",
      " 9: -7.9228e+02 -7.9243e+02  1e-01  2e-14  3e-15\n",
      "10: -7.9233e+02 -7.9233e+02  7e-03  4e-14  3e-15\n",
      "11: -7.9233e+02 -7.9233e+02  4e-04  1e-14  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.8811e+02 -1.9872e+04  3e+04  2e-01  7e-15\n",
      " 1: -2.2067e+02 -3.0953e+03  3e+03  1e-02  4e-15\n",
      " 2: -5.8118e+02 -1.5130e+03  1e+03  3e-03  3e-15\n",
      " 3: -7.3895e+02 -1.0657e+03  3e+02  2e-04  3e-15\n",
      " 4: -7.7901e+02 -9.1011e+02  1e+02  4e-05  3e-15\n",
      " 5: -7.9681e+02 -8.3385e+02  4e+01  8e-07  3e-15\n",
      " 6: -8.0199e+02 -8.1376e+02  1e+01  1e-07  3e-15\n",
      " 7: -8.0380e+02 -8.0767e+02  4e+00  2e-08  3e-15\n",
      " 8: -8.0447e+02 -8.0573e+02  1e+00  5e-14  3e-15\n",
      " 9: -8.0475e+02 -8.0503e+02  3e-01  1e-14  3e-15\n",
      "10: -8.0482e+02 -8.0488e+02  6e-02  1e-13  3e-15\n",
      "11: -8.0483e+02 -8.0484e+02  8e-03  1e-13  3e-15\n",
      "12: -8.0484e+02 -8.0484e+02  3e-04  9e-14  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.8660e+02 -2.1946e+04  4e+04  2e-01  6e-15\n",
      " 1: -1.3062e+02 -3.4330e+03  4e+03  1e-02  4e-15\n",
      " 2: -5.1882e+02 -1.6408e+03  1e+03  3e-03  3e-15\n",
      " 3: -6.8032e+02 -1.1042e+03  4e+02  4e-04  3e-15\n",
      " 4: -7.4297e+02 -8.5919e+02  1e+02  3e-14  3e-15\n",
      " 5: -7.5833e+02 -7.9154e+02  3e+01  3e-15  3e-15\n",
      " 6: -7.6358e+02 -7.7329e+02  1e+01  2e-14  3e-15\n",
      " 7: -7.6514e+02 -7.6858e+02  3e+00  6e-15  3e-15\n",
      " 8: -7.6574e+02 -7.6703e+02  1e+00  2e-14  4e-15\n",
      " 9: -7.6604e+02 -7.6630e+02  3e-01  2e-14  3e-15\n",
      "10: -7.6610e+02 -7.6617e+02  7e-02  7e-15  3e-15\n",
      "11: -7.6613e+02 -7.6613e+02  2e-03  6e-14  4e-15\n",
      "12: -7.6613e+02 -7.6613e+02  2e-05  9e-14  3e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.2376e+02 -2.1195e+04  3e+04  2e-01  6e-15\n",
      " 1: -1.1240e+02 -3.3139e+03  3e+03  1e-02  3e-15\n",
      " 2: -5.3877e+02 -1.4627e+03  9e+02  2e-03  4e-15\n",
      " 3: -6.9285e+02 -1.0370e+03  3e+02  1e-04  3e-15\n",
      " 4: -7.3519e+02 -8.4327e+02  1e+02  2e-05  3e-15\n",
      " 5: -7.4947e+02 -7.8090e+02  3e+01  2e-06  3e-15\n",
      " 6: -7.5429e+02 -7.6330e+02  9e+00  8e-08  4e-15\n",
      " 7: -7.5572e+02 -7.5924e+02  4e+00  3e-09  3e-15\n",
      " 8: -7.5629e+02 -7.5752e+02  1e+00  2e-16  3e-15\n",
      " 9: -7.5660e+02 -7.5680e+02  2e-01  2e-14  3e-15\n",
      "10: -7.5666e+02 -7.5667e+02  1e-02  1e-14  3e-15\n",
      "11: -7.5666e+02 -7.5666e+02  2e-04  7e-14  3e-15\n",
      "Optimal solution found.\n",
      "C=10    Gamma=1     -> Avg Val Acc: 0.8863\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  8.2942e+04 -1.9465e+06  3e+06  2e-01  3e-13\n",
      " 1:  3.0511e+04 -2.3745e+05  3e+05  8e-03  2e-13\n",
      " 2: -3.2511e+03 -5.7081e+04  5e+04  1e-03  2e-13\n",
      " 3: -8.5807e+03 -3.0906e+04  2e+04  2e-04  2e-13\n",
      " 4: -9.8133e+03 -2.1999e+04  1e+04  1e-04  2e-13\n",
      " 5: -1.0530e+04 -1.5992e+04  5e+03  4e-05  3e-13\n",
      " 6: -1.0952e+04 -1.2723e+04  2e+03  1e-05  3e-13\n",
      " 7: -1.1010e+04 -1.2509e+04  1e+03  7e-06  3e-13\n",
      " 8: -1.1105e+04 -1.2061e+04  1e+03  4e-06  3e-13\n",
      " 9: -1.1188e+04 -1.1668e+04  5e+02  2e-06  3e-13\n",
      "10: -1.1193e+04 -1.1638e+04  4e+02  1e-06  3e-13\n",
      "11: -1.1211e+04 -1.1631e+04  4e+02  9e-07  3e-13\n",
      "12: -1.1251e+04 -1.1511e+04  3e+02  5e-07  3e-13\n",
      "13: -1.1247e+04 -1.1478e+04  2e+02  3e-07  3e-13\n",
      "14: -1.1252e+04 -1.1476e+04  2e+02  3e-07  3e-13\n",
      "15: -1.1269e+04 -1.1441e+04  2e+02  1e-07  3e-13\n",
      "16: -1.1274e+04 -1.1434e+04  2e+02  1e-07  2e-13\n",
      "17: -1.1288e+04 -1.1405e+04  1e+02  9e-09  3e-13\n",
      "18: -1.1306e+04 -1.1370e+04  6e+01  4e-09  3e-13\n",
      "19: -1.1322e+04 -1.1342e+04  2e+01  5e-10  3e-13\n",
      "20: -1.1323e+04 -1.1339e+04  2e+01  3e-10  3e-13\n",
      "21: -1.1327e+04 -1.1334e+04  7e+00  1e-10  3e-13\n",
      "22: -1.1329e+04 -1.1330e+04  7e-01  1e-12  3e-13\n",
      "23: -1.1330e+04 -1.1330e+04  1e-02  6e-13  3e-13\n",
      "24: -1.1330e+04 -1.1330e+04  2e-04  3e-13  3e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  7.8552e+04 -1.9316e+06  3e+06  2e-01  3e-13\n",
      " 1:  2.3960e+04 -2.2761e+05  3e+05  7e-03  2e-13\n",
      " 2: -4.7252e+03 -5.7523e+04  5e+04  1e-03  2e-13\n",
      " 3: -9.7057e+03 -3.2660e+04  2e+04  2e-04  2e-13\n",
      " 4: -1.0897e+04 -2.3658e+04  1e+04  1e-04  2e-13\n",
      " 5: -1.1743e+04 -1.5756e+04  4e+03  3e-05  2e-13\n",
      " 6: -1.1826e+04 -1.5419e+04  4e+03  2e-05  2e-13\n",
      " 7: -1.2080e+04 -1.3732e+04  2e+03  6e-06  2e-13\n",
      " 8: -1.2147e+04 -1.3340e+04  1e+03  3e-06  2e-13\n",
      " 9: -1.2184e+04 -1.3268e+04  1e+03  2e-06  2e-13\n",
      "10: -1.2272e+04 -1.2801e+04  5e+02  8e-07  2e-13\n",
      "11: -1.2311e+04 -1.2598e+04  3e+02  2e-07  3e-13\n",
      "12: -1.2327e+04 -1.2558e+04  2e+02  1e-07  3e-13\n",
      "13: -1.2355e+04 -1.2499e+04  1e+02  7e-08  3e-13\n",
      "14: -1.2372e+04 -1.2472e+04  1e+02  4e-08  3e-13\n",
      "15: -1.2377e+04 -1.2451e+04  7e+01  1e-09  3e-13\n",
      "16: -1.2391e+04 -1.2427e+04  4e+01  2e-10  3e-13\n",
      "17: -1.2400e+04 -1.2414e+04  1e+01  7e-11  3e-13\n",
      "18: -1.2400e+04 -1.2413e+04  1e+01  4e-11  3e-13\n",
      "19: -1.2404e+04 -1.2407e+04  3e+00  1e-11  3e-13\n",
      "20: -1.2406e+04 -1.2406e+04  2e-01  2e-13  3e-13\n",
      "21: -1.2406e+04 -1.2406e+04  2e-03  3e-13  3e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  7.8360e+04 -1.9417e+06  3e+06  2e-01  3e-13\n",
      " 1:  2.5735e+04 -2.3267e+05  3e+05  7e-03  3e-13\n",
      " 2: -3.8193e+03 -6.2553e+04  6e+04  1e-03  3e-13\n",
      " 3: -9.2561e+03 -3.2676e+04  2e+04  3e-04  3e-13\n",
      " 4: -1.0575e+04 -2.3634e+04  1e+04  1e-04  3e-13\n",
      " 5: -1.1460e+04 -1.5827e+04  4e+03  4e-05  3e-13\n",
      " 6: -1.1634e+04 -1.4742e+04  3e+03  2e-05  3e-13\n",
      " 7: -1.1811e+04 -1.3740e+04  2e+03  9e-06  3e-13\n",
      " 8: -1.1928e+04 -1.2882e+04  1e+03  2e-06  3e-13\n",
      " 9: -1.2006e+04 -1.2521e+04  5e+02  8e-07  3e-13\n",
      "10: -1.2038e+04 -1.2404e+04  4e+02  4e-07  3e-13\n",
      "11: -1.2068e+04 -1.2277e+04  2e+02  2e-07  3e-13\n",
      "12: -1.2090e+04 -1.2234e+04  1e+02  7e-08  3e-13\n",
      "13: -1.2105e+04 -1.2210e+04  1e+02  4e-08  3e-13\n",
      "14: -1.2117e+04 -1.2184e+04  7e+01  2e-08  4e-13\n",
      "15: -1.2119e+04 -1.2175e+04  6e+01  7e-09  3e-13\n",
      "16: -1.2132e+04 -1.2152e+04  2e+01  2e-09  3e-13\n",
      "17: -1.2136e+04 -1.2146e+04  1e+01  7e-10  3e-13\n",
      "18: -1.2140e+04 -1.2141e+04  9e-01  5e-11  4e-13\n",
      "19: -1.2140e+04 -1.2141e+04  5e-02  3e-12  4e-13\n",
      "20: -1.2141e+04 -1.2141e+04  9e-04  4e-13  4e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  8.1556e+04 -1.8845e+06  3e+06  2e-01  3e-13\n",
      " 1:  2.6231e+04 -2.2496e+05  3e+05  7e-03  3e-13\n",
      " 2: -4.0669e+03 -5.5136e+04  5e+04  1e-03  2e-13\n",
      " 3: -7.9717e+03 -3.8495e+04  3e+04  4e-04  3e-13\n",
      " 4: -1.0197e+04 -2.0497e+04  1e+04  1e-04  3e-13\n",
      " 5: -1.0773e+04 -1.6449e+04  6e+03  5e-05  3e-13\n",
      " 6: -1.1068e+04 -1.4599e+04  4e+03  3e-05  3e-13\n",
      " 7: -1.1246e+04 -1.3491e+04  2e+03  1e-05  3e-13\n",
      " 8: -1.1325e+04 -1.3071e+04  2e+03  1e-05  3e-13\n",
      " 9: -1.1388e+04 -1.2690e+04  1e+03  6e-06  3e-13\n",
      "10: -1.1471e+04 -1.2133e+04  7e+02  3e-06  3e-13\n",
      "11: -1.1502e+04 -1.1986e+04  5e+02  2e-06  3e-13\n",
      "12: -1.1542e+04 -1.1805e+04  3e+02  7e-07  3e-13\n",
      "13: -1.1565e+04 -1.1745e+04  2e+02  4e-07  4e-13\n",
      "14: -1.1580e+04 -1.1714e+04  1e+02  2e-07  4e-13\n",
      "15: -1.1585e+04 -1.1695e+04  1e+02  8e-08  4e-13\n",
      "16: -1.1604e+04 -1.1654e+04  5e+01  3e-08  3e-13\n",
      "17: -1.1606e+04 -1.1647e+04  4e+01  2e-08  3e-13\n",
      "18: -1.1618e+04 -1.1628e+04  9e+00  2e-09  4e-13\n",
      "19: -1.1622e+04 -1.1623e+04  1e+00  2e-10  4e-13\n",
      "20: -1.1622e+04 -1.1622e+04  2e-02  3e-12  4e-13\n",
      "21: -1.1622e+04 -1.1622e+04  2e-04  2e-12  4e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  8.2654e+04 -1.9600e+06  3e+06  2e-01  3e-13\n",
      " 1:  3.1481e+04 -2.3875e+05  3e+05  8e-03  2e-13\n",
      " 2: -3.0424e+03 -5.4086e+04  5e+04  1e-03  2e-13\n",
      " 3: -7.7109e+03 -3.4586e+04  3e+04  4e-04  2e-13\n",
      " 4: -1.0059e+04 -1.8876e+04  9e+03  1e-04  3e-13\n",
      " 5: -1.0661e+04 -1.5609e+04  5e+03  4e-05  3e-13\n",
      " 6: -1.0942e+04 -1.3770e+04  3e+03  2e-05  3e-13\n",
      " 7: -1.1132e+04 -1.2355e+04  1e+03  6e-06  3e-13\n",
      " 8: -1.1153e+04 -1.2210e+04  1e+03  5e-06  3e-13\n",
      " 9: -1.1187e+04 -1.2158e+04  1e+03  3e-06  3e-13\n",
      "10: -1.1273e+04 -1.1684e+04  4e+02  9e-07  3e-13\n",
      "11: -1.1304e+04 -1.1617e+04  3e+02  5e-07  3e-13\n",
      "12: -1.1316e+04 -1.1582e+04  3e+02  3e-07  3e-13\n",
      "13: -1.1355e+04 -1.1482e+04  1e+02  3e-08  3e-13\n",
      "14: -1.1350e+04 -1.1475e+04  1e+02  3e-08  3e-13\n",
      "15: -1.1356e+04 -1.1468e+04  1e+02  2e-08  3e-13\n",
      "16: -1.1379e+04 -1.1432e+04  5e+01  8e-09  3e-13\n",
      "17: -1.1389e+04 -1.1414e+04  2e+01  2e-09  3e-13\n",
      "18: -1.1396e+04 -1.1404e+04  7e+00  6e-13  3e-13\n",
      "19: -1.1399e+04 -1.1401e+04  2e+00  6e-13  3e-13\n",
      "20: -1.1399e+04 -1.1400e+04  2e-01  3e-13  3e-13\n",
      "21: -1.1400e+04 -1.1400e+04  2e-03  2e-13  3e-13\n",
      "Optimal solution found.\n",
      "C=100   Gamma=0.001 -> Avg Val Acc: 0.9138\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2738e+04 -1.0373e+06  1e+06  3e-02  2e-13\n",
      " 1: -4.1504e+03 -8.8315e+04  9e+04  2e-03  2e-13\n",
      " 2: -8.0892e+03 -2.3185e+04  2e+04  2e-04  2e-13\n",
      " 3: -9.5957e+03 -1.7102e+04  8e+03  8e-05  2e-13\n",
      " 4: -1.0135e+04 -1.4419e+04  4e+03  4e-05  2e-13\n",
      " 5: -1.0560e+04 -1.1976e+04  1e+03  1e-05  2e-13\n",
      " 6: -1.0726e+04 -1.1196e+04  5e+02  3e-06  2e-13\n",
      " 7: -1.0775e+04 -1.1070e+04  3e+02  9e-07  2e-13\n",
      " 8: -1.0808e+04 -1.0910e+04  1e+02  2e-07  2e-13\n",
      " 9: -1.0823e+04 -1.0869e+04  5e+01  2e-08  2e-13\n",
      "10: -1.0829e+04 -1.0849e+04  2e+01  7e-09  2e-13\n",
      "11: -1.0831e+04 -1.0845e+04  1e+01  3e-09  2e-13\n",
      "12: -1.0833e+04 -1.0839e+04  5e+00  9e-10  2e-13\n",
      "13: -1.0834e+04 -1.0837e+04  3e+00  1e-10  2e-13\n",
      "14: -1.0835e+04 -1.0836e+04  1e+00  3e-11  2e-13\n",
      "15: -1.0835e+04 -1.0835e+04  5e-01  1e-12  2e-13\n",
      "16: -1.0835e+04 -1.0835e+04  9e-02  2e-13  2e-13\n",
      "17: -1.0835e+04 -1.0835e+04  3e-03  1e-12  2e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.0310e+04 -1.1262e+06  1e+06  4e-02  2e-13\n",
      " 1: -5.1988e+03 -9.5668e+04  9e+04  2e-03  1e-13\n",
      " 2: -9.1395e+03 -2.7379e+04  2e+04  2e-04  1e-13\n",
      " 3: -1.0978e+04 -1.6444e+04  5e+03  5e-05  2e-13\n",
      " 4: -1.1487e+04 -1.3137e+04  2e+03  1e-05  2e-13\n",
      " 5: -1.1642e+04 -1.2415e+04  8e+02  4e-06  2e-13\n",
      " 6: -1.1719e+04 -1.2110e+04  4e+02  1e-06  2e-13\n",
      " 7: -1.1754e+04 -1.1938e+04  2e+02  5e-07  2e-13\n",
      " 8: -1.1776e+04 -1.1839e+04  6e+01  6e-08  2e-13\n",
      " 9: -1.1783e+04 -1.1818e+04  3e+01  2e-08  2e-13\n",
      "10: -1.1789e+04 -1.1805e+04  2e+01  5e-09  2e-13\n",
      "11: -1.1792e+04 -1.1797e+04  5e+00  5e-10  2e-13\n",
      "12: -1.1793e+04 -1.1796e+04  2e+00  2e-11  2e-13\n",
      "13: -1.1794e+04 -1.1795e+04  9e-01  2e-13  2e-13\n",
      "14: -1.1794e+04 -1.1794e+04  7e-02  3e-13  2e-13\n",
      "15: -1.1794e+04 -1.1794e+04  1e-03  2e-13  2e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.0464e+04 -1.1020e+06  1e+06  3e-02  3e-13\n",
      " 1: -4.7702e+03 -9.9376e+04  1e+05  2e-03  2e-13\n",
      " 2: -8.6747e+03 -2.6152e+04  2e+04  2e-04  2e-13\n",
      " 3: -1.0515e+04 -1.8008e+04  8e+03  5e-05  2e-13\n",
      " 4: -1.1137e+04 -1.3084e+04  2e+03  1e-05  2e-13\n",
      " 5: -1.1328e+04 -1.2175e+04  8e+02  2e-06  2e-13\n",
      " 6: -1.1421e+04 -1.1710e+04  3e+02  5e-07  2e-13\n",
      " 7: -1.1454e+04 -1.1626e+04  2e+02  2e-07  2e-13\n",
      " 8: -1.1475e+04 -1.1550e+04  8e+01  6e-08  2e-13\n",
      " 9: -1.1484e+04 -1.1523e+04  4e+01  1e-08  2e-13\n",
      "10: -1.1491e+04 -1.1508e+04  2e+01  5e-09  2e-13\n",
      "11: -1.1493e+04 -1.1502e+04  8e+00  4e-10  3e-13\n",
      "12: -1.1496e+04 -1.1498e+04  2e+00  8e-11  2e-13\n",
      "13: -1.1496e+04 -1.1497e+04  9e-01  1e-11  2e-13\n",
      "14: -1.1497e+04 -1.1497e+04  6e-02  5e-13  3e-13\n",
      "15: -1.1497e+04 -1.1497e+04  1e-03  6e-13  3e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.1436e+04 -1.0902e+06  1e+06  4e-02  2e-13\n",
      " 1: -4.0139e+03 -9.5093e+04  9e+04  2e-03  2e-13\n",
      " 2: -8.1928e+03 -2.8663e+04  2e+04  3e-04  2e-13\n",
      " 3: -1.0206e+04 -1.4983e+04  5e+03  4e-05  2e-13\n",
      " 4: -1.0587e+04 -1.3154e+04  3e+03  2e-05  2e-13\n",
      " 5: -1.0841e+04 -1.1780e+04  9e+02  5e-06  2e-13\n",
      " 6: -1.0901e+04 -1.1583e+04  7e+02  2e-06  2e-13\n",
      " 7: -1.0977e+04 -1.1173e+04  2e+02  4e-07  3e-13\n",
      " 8: -1.0991e+04 -1.1137e+04  1e+02  2e-07  2e-13\n",
      " 9: -1.1007e+04 -1.1073e+04  7e+01  6e-08  2e-13\n",
      "10: -1.1018e+04 -1.1040e+04  2e+01  5e-09  3e-13\n",
      "11: -1.1022e+04 -1.1032e+04  1e+01  1e-09  3e-13\n",
      "12: -1.1023e+04 -1.1028e+04  4e+00  3e-10  3e-13\n",
      "13: -1.1024e+04 -1.1026e+04  2e+00  6e-11  3e-13\n",
      "14: -1.1025e+04 -1.1025e+04  5e-01  1e-11  3e-13\n",
      "15: -1.1025e+04 -1.1025e+04  5e-02  8e-13  3e-13\n",
      "16: -1.1025e+04 -1.1025e+04  2e-03  4e-13  3e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  1.2825e+04 -1.0393e+06  1e+06  3e-02  2e-13\n",
      " 1: -3.7515e+03 -8.9415e+04  9e+04  2e-03  2e-13\n",
      " 2: -8.0885e+03 -2.7164e+04  2e+04  2e-04  2e-13\n",
      " 3: -9.7123e+03 -1.7089e+04  7e+03  6e-05  2e-13\n",
      " 4: -1.0254e+04 -1.3663e+04  3e+03  2e-05  2e-13\n",
      " 5: -1.0567e+04 -1.1822e+04  1e+03  4e-06  2e-13\n",
      " 6: -1.0644e+04 -1.1560e+04  9e+02  2e-06  2e-13\n",
      " 7: -1.0693e+04 -1.1313e+04  6e+02  5e-07  2e-13\n",
      " 8: -1.0750e+04 -1.0972e+04  2e+02  2e-07  2e-13\n",
      " 9: -1.0769e+04 -1.0882e+04  1e+02  6e-08  2e-13\n",
      "10: -1.0783e+04 -1.0831e+04  5e+01  7e-09  2e-13\n",
      "11: -1.0789e+04 -1.0817e+04  3e+01  2e-09  2e-13\n",
      "12: -1.0794e+04 -1.0804e+04  1e+01  5e-10  2e-13\n",
      "13: -1.0796e+04 -1.0800e+04  4e+00  7e-11  2e-13\n",
      "14: -1.0797e+04 -1.0798e+04  2e+00  2e-11  2e-13\n",
      "15: -1.0797e+04 -1.0798e+04  4e-01  5e-13  2e-13\n",
      "16: -1.0797e+04 -1.0797e+04  2e-02  1e-12  2e-13\n",
      "17: -1.0797e+04 -1.0797e+04  2e-04  1e-12  2e-13\n",
      "Optimal solution found.\n",
      "C=100   Gamma=0.01  -> Avg Val Acc: 0.9138\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.2044e+04 -9.3503e+05  1e+06  1e-14  1e-13\n",
      " 1: -1.2703e+03 -9.6500e+04  1e+05  1e-13  9e-14\n",
      " 2: -5.6840e+03 -2.9207e+04  2e+04  2e-13  9e-14\n",
      " 3: -7.8425e+03 -1.7076e+04  9e+03  6e-13  9e-14\n",
      " 4: -8.7957e+03 -1.1836e+04  3e+03  5e-13  1e-13\n",
      " 5: -9.1543e+03 -1.0355e+04  1e+03  2e-13  9e-14\n",
      " 6: -9.3027e+03 -9.6998e+03  4e+02  5e-14  1e-13\n",
      " 7: -9.3606e+03 -9.4998e+03  1e+02  2e-13  1e-13\n",
      " 8: -9.3845e+03 -9.4265e+03  4e+01  3e-13  1e-13\n",
      " 9: -9.3909e+03 -9.4126e+03  2e+01  2e-13  1e-13\n",
      "10: -9.3944e+03 -9.4043e+03  1e+01  8e-14  1e-13\n",
      "11: -9.3963e+03 -9.3999e+03  4e+00  2e-13  1e-13\n",
      "12: -9.3973e+03 -9.3982e+03  9e-01  2e-13  1e-13\n",
      "13: -9.3975e+03 -9.3979e+03  4e-01  8e-13  9e-14\n",
      "14: -9.3976e+03 -9.3977e+03  3e-02  6e-13  1e-13\n",
      "15: -9.3977e+03 -9.3977e+03  5e-04  2e-14  1e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.0384e+04 -1.0347e+06  1e+06  2e-02  1e-13\n",
      " 1: -9.0576e+02 -1.1672e+05  1e+05  2e-03  7e-14\n",
      " 2: -6.2341e+03 -2.9720e+04  2e+04  3e-04  7e-14\n",
      " 3: -8.4609e+03 -2.1707e+04  1e+04  6e-05  8e-14\n",
      " 4: -9.6045e+03 -1.4282e+04  5e+03  2e-05  9e-14\n",
      " 5: -1.0084e+04 -1.1892e+04  2e+03  5e-06  9e-14\n",
      " 6: -1.0316e+04 -1.0892e+04  6e+02  8e-07  9e-14\n",
      " 7: -1.0401e+04 -1.0567e+04  2e+02  9e-08  1e-13\n",
      " 8: -1.0425e+04 -1.0500e+04  7e+01  8e-10  1e-13\n",
      " 9: -1.0438e+04 -1.0462e+04  2e+01  7e-13  1e-13\n",
      "10: -1.0442e+04 -1.0453e+04  1e+01  2e-13  1e-13\n",
      "11: -1.0443e+04 -1.0450e+04  7e+00  4e-13  1e-13\n",
      "12: -1.0445e+04 -1.0447e+04  2e+00  7e-13  1e-13\n",
      "13: -1.0445e+04 -1.0446e+04  7e-01  8e-13  1e-13\n",
      "14: -1.0445e+04 -1.0446e+04  2e-01  2e-13  1e-13\n",
      "15: -1.0445e+04 -1.0445e+04  1e-02  3e-13  1e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.1488e+04 -1.0571e+06  1e+06  2e-02  1e-13\n",
      " 1:  1.8722e+03 -1.2900e+05  1e+05  2e-03  9e-14\n",
      " 2: -4.8880e+03 -3.1949e+04  3e+04  4e-04  8e-14\n",
      " 3: -7.6035e+03 -2.2013e+04  1e+04  9e-05  9e-14\n",
      " 4: -9.0323e+03 -1.3934e+04  5e+03  2e-05  1e-13\n",
      " 5: -9.6431e+03 -1.1379e+04  2e+03  6e-06  1e-13\n",
      " 6: -9.8769e+03 -1.0576e+04  7e+02  1e-06  1e-13\n",
      " 7: -9.9866e+03 -1.0192e+04  2e+02  8e-08  1e-13\n",
      " 8: -1.0021e+04 -1.0092e+04  7e+01  1e-08  1e-13\n",
      " 9: -1.0033e+04 -1.0062e+04  3e+01  2e-13  1e-13\n",
      "10: -1.0038e+04 -1.0047e+04  9e+00  7e-13  1e-13\n",
      "11: -1.0039e+04 -1.0046e+04  7e+00  6e-13  1e-13\n",
      "12: -1.0040e+04 -1.0043e+04  3e+00  5e-13  1e-13\n",
      "13: -1.0041e+04 -1.0042e+04  1e+00  9e-13  1e-13\n",
      "14: -1.0041e+04 -1.0041e+04  3e-01  5e-13  1e-13\n",
      "15: -1.0041e+04 -1.0041e+04  7e-02  1e-12  1e-13\n",
      "16: -1.0041e+04 -1.0041e+04  1e-02  5e-13  1e-13\n",
      "17: -1.0041e+04 -1.0041e+04  5e-04  2e-13  1e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.0460e+04 -9.8222e+05  1e+06  2e-02  1e-13\n",
      " 1: -5.6967e+02 -1.0994e+05  1e+05  2e-03  9e-14\n",
      " 2: -5.7317e+03 -3.2704e+04  3e+04  3e-04  9e-14\n",
      " 3: -8.1058e+03 -1.8119e+04  1e+04  4e-05  9e-14\n",
      " 4: -9.0147e+03 -1.3742e+04  5e+03  2e-05  9e-14\n",
      " 5: -9.5667e+03 -1.0885e+04  1e+03  3e-06  1e-13\n",
      " 6: -9.7645e+03 -1.0148e+04  4e+02  3e-07  1e-13\n",
      " 7: -9.8225e+03 -9.9761e+03  2e+02  5e-08  1e-13\n",
      " 8: -9.8507e+03 -9.8918e+03  4e+01  3e-09  1e-13\n",
      " 9: -9.8593e+03 -9.8728e+03  1e+01  6e-13  1e-13\n",
      "10: -9.8622e+03 -9.8666e+03  4e+00  8e-13  1e-13\n",
      "11: -9.8631e+03 -9.8649e+03  2e+00  1e-13  1e-13\n",
      "12: -9.8635e+03 -9.8642e+03  7e-01  3e-14  1e-13\n",
      "13: -9.8638e+03 -9.8639e+03  8e-02  3e-14  1e-13\n",
      "14: -9.8638e+03 -9.8638e+03  2e-03  3e-13  1e-13\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  2.3157e+04 -9.6897e+05  1e+06  2e-02  1e-13\n",
      " 1:  1.5789e+03 -1.2192e+05  1e+05  2e-03  8e-14\n",
      " 2: -4.7227e+03 -3.0430e+04  3e+04  3e-04  8e-14\n",
      " 3: -7.1975e+03 -2.0707e+04  1e+04  7e-05  8e-14\n",
      " 4: -8.5129e+03 -1.2873e+04  4e+03  2e-05  9e-14\n",
      " 5: -9.0701e+03 -1.0601e+04  2e+03  3e-06  1e-13\n",
      " 6: -9.2786e+03 -9.7432e+03  5e+02  3e-07  1e-13\n",
      " 7: -9.3412e+03 -9.5343e+03  2e+02  2e-08  1e-13\n",
      " 8: -9.3704e+03 -9.4390e+03  7e+01  4e-09  1e-13\n",
      " 9: -9.3810e+03 -9.4085e+03  3e+01  1e-09  1e-13\n",
      "10: -9.3855e+03 -9.3963e+03  1e+01  3e-10  9e-14\n",
      "11: -9.3868e+03 -9.3934e+03  7e+00  5e-11  1e-13\n",
      "12: -9.3883e+03 -9.3901e+03  2e+00  1e-11  1e-13\n",
      "13: -9.3888e+03 -9.3894e+03  6e-01  3e-13  1e-13\n",
      "14: -9.3890e+03 -9.3891e+03  1e-01  1e-13  1e-13\n",
      "15: -9.3890e+03 -9.3890e+03  1e-02  4e-13  1e-13\n",
      "16: -9.3890e+03 -9.3890e+03  6e-04  4e-13  1e-13\n",
      "Optimal solution found.\n",
      "C=100   Gamma=0.1   -> Avg Val Acc: 0.9025\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  9.0596e+04 -1.1743e+06  1e+06  4e-02  6e-14\n",
      " 1:  2.7463e+04 -1.6608e+05  2e+05  4e-03  4e-14\n",
      " 2:  6.7069e+03 -4.2489e+04  5e+04  8e-04  3e-14\n",
      " 3:  2.7691e+02 -1.9834e+04  2e+04  1e-04  2e-14\n",
      " 4: -2.2141e+03 -7.2384e+03  5e+03  2e-05  2e-14\n",
      " 5: -3.0360e+03 -4.3326e+03  1e+03  1e-06  2e-14\n",
      " 6: -3.2632e+03 -3.6562e+03  4e+02  1e-07  3e-14\n",
      " 7: -3.3373e+03 -3.4253e+03  9e+01  6e-09  3e-14\n",
      " 8: -3.3566e+03 -3.3733e+03  2e+01  1e-09  3e-14\n",
      " 9: -3.3603e+03 -3.3641e+03  4e+00  1e-10  3e-14\n",
      "10: -3.3610e+03 -3.3625e+03  1e+00  3e-11  2e-14\n",
      "11: -3.3614e+03 -3.3619e+03  5e-01  2e-13  3e-14\n",
      "12: -3.3615e+03 -3.3616e+03  7e-02  3e-14  2e-14\n",
      "13: -3.3615e+03 -3.3615e+03  2e-03  8e-14  2e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  9.3675e+04 -1.2588e+06  2e+06  4e-02  7e-14\n",
      " 1:  2.9392e+04 -1.7345e+05  2e+05  4e-03  4e-14\n",
      " 2:  8.2333e+03 -5.1379e+04  6e+04  9e-04  3e-14\n",
      " 3:  7.6597e+02 -1.7376e+04  2e+04  2e-04  2e-14\n",
      " 4: -1.7421e+03 -7.3269e+03  6e+03  2e-05  2e-14\n",
      " 5: -2.5569e+03 -4.3012e+03  2e+03  4e-06  2e-14\n",
      " 6: -2.8383e+03 -3.3158e+03  5e+02  5e-07  2e-14\n",
      " 7: -2.9229e+03 -3.0539e+03  1e+02  8e-08  2e-14\n",
      " 8: -2.9508e+03 -2.9732e+03  2e+01  6e-09  2e-14\n",
      " 9: -2.9562e+03 -2.9603e+03  4e+00  4e-13  2e-14\n",
      "10: -2.9571e+03 -2.9582e+03  1e+00  2e-13  2e-14\n",
      "11: -2.9573e+03 -2.9577e+03  4e-01  1e-13  2e-14\n",
      "12: -2.9574e+03 -2.9575e+03  8e-02  3e-13  2e-14\n",
      "13: -2.9574e+03 -2.9575e+03  1e-02  2e-13  2e-14\n",
      "14: -2.9574e+03 -2.9574e+03  8e-04  4e-14  2e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  9.4359e+04 -1.2862e+06  2e+06  5e-02  6e-14\n",
      " 1:  3.2235e+04 -1.9181e+05  2e+05  6e-03  4e-14\n",
      " 2:  8.3441e+03 -5.0840e+04  6e+04  1e-03  3e-14\n",
      " 3:  5.0982e+02 -1.7395e+04  2e+04  2e-04  2e-14\n",
      " 4: -2.2835e+03 -6.7821e+03  4e+03  3e-06  3e-14\n",
      " 5: -3.0050e+03 -4.3144e+03  1e+03  2e-07  2e-14\n",
      " 6: -3.2190e+03 -3.5583e+03  3e+02  3e-08  2e-14\n",
      " 7: -3.2815e+03 -3.3566e+03  8e+01  5e-09  2e-14\n",
      " 8: -3.2962e+03 -3.3123e+03  2e+01  8e-10  2e-14\n",
      " 9: -3.2992e+03 -3.3046e+03  5e+00  5e-11  2e-14\n",
      "10: -3.3002e+03 -3.3020e+03  2e+00  9e-12  2e-14\n",
      "11: -3.3006e+03 -3.3011e+03  5e-01  8e-13  2e-14\n",
      "12: -3.3007e+03 -3.3008e+03  9e-02  3e-13  2e-14\n",
      "13: -3.3008e+03 -3.3008e+03  1e-02  2e-13  2e-14\n",
      "14: -3.3008e+03 -3.3008e+03  4e-04  2e-13  2e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  9.1608e+04 -1.2340e+06  2e+06  5e-02  5e-14\n",
      " 1:  3.0363e+04 -1.6687e+05  2e+05  5e-03  4e-14\n",
      " 2:  7.8179e+03 -4.5731e+04  5e+04  5e-04  3e-14\n",
      " 3:  6.1521e+02 -2.0187e+04  2e+04  4e-05  3e-14\n",
      " 4: -1.7120e+03 -7.0995e+03  5e+03  9e-06  2e-14\n",
      " 5: -2.5239e+03 -4.1579e+03  2e+03  2e-07  2e-14\n",
      " 6: -2.7750e+03 -3.1987e+03  4e+02  4e-08  3e-14\n",
      " 7: -2.8592e+03 -2.9231e+03  6e+01  2e-09  3e-14\n",
      " 8: -2.8730e+03 -2.8890e+03  2e+01  4e-10  2e-14\n",
      " 9: -2.8760e+03 -2.8824e+03  6e+00  8e-11  3e-14\n",
      "10: -2.8774e+03 -2.8795e+03  2e+00  2e-13  3e-14\n",
      "11: -2.8779e+03 -2.8783e+03  4e-01  2e-13  3e-14\n",
      "12: -2.8780e+03 -2.8781e+03  5e-02  2e-13  3e-14\n",
      "13: -2.8781e+03 -2.8781e+03  7e-04  2e-13  3e-14\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  9.2398e+04 -1.1644e+06  1e+06  4e-02  5e-14\n",
      " 1:  2.8592e+04 -1.6893e+05  2e+05  4e-03  4e-14\n",
      " 2:  7.8736e+03 -4.9541e+04  6e+04  1e-03  3e-14\n",
      " 3:  5.0715e+01 -1.4388e+04  1e+04  1e-04  2e-14\n",
      " 4: -2.0706e+03 -8.0324e+03  6e+03  2e-13  2e-14\n",
      " 5: -2.7469e+03 -4.7552e+03  2e+03  3e-14  2e-14\n",
      " 6: -3.0385e+03 -3.6608e+03  6e+02  4e-13  2e-14\n",
      " 7: -3.1411e+03 -3.3107e+03  2e+02  3e-13  2e-14\n",
      " 8: -3.1735e+03 -3.2134e+03  4e+01  1e-13  3e-14\n",
      " 9: -3.1822e+03 -3.1920e+03  1e+01  3e-13  2e-14\n",
      "10: -3.1842e+03 -3.1873e+03  3e+00  2e-14  2e-14\n",
      "11: -3.1848e+03 -3.1860e+03  1e+00  1e-13  2e-14\n",
      "12: -3.1851e+03 -3.1854e+03  3e-01  1e-14  2e-14\n",
      "13: -3.1852e+03 -3.1852e+03  3e-02  2e-14  3e-14\n",
      "14: -3.1852e+03 -3.1852e+03  1e-03  1e-13  2e-14\n",
      "Optimal solution found.\n",
      "C=100   Gamma=1     -> Avg Val Acc: 0.8463\n",
      "--------------------------------------------------\n",
      "Migliori parametri trovati: {'C': 0.1, 'gamma': 0.1}\n",
      "Miglior Validation Accuracy: 0.9175\n",
      "Addestramento modello finale...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.1580e+02 -1.7360e+02  6e+03  3e+01  2e-15\n",
      " 1: -1.9474e+01 -1.6203e+02  3e+02  7e-01  1e-15\n",
      " 2: -1.5015e+01 -5.0259e+01  4e+01  3e-16  2e-15\n",
      " 3: -1.7468e+01 -2.2337e+01  5e+00  2e-16  9e-16\n",
      " 4: -1.7855e+01 -1.9436e+01  2e+00  4e-16  5e-16\n",
      " 5: -1.7929e+01 -1.9207e+01  1e+00  7e-16  4e-16\n",
      " 6: -1.8026e+01 -1.8719e+01  7e-01  6e-16  3e-16\n",
      " 7: -1.8093e+01 -1.8464e+01  4e-01  3e-16  4e-16\n",
      " 8: -1.8122e+01 -1.8387e+01  3e-01  4e-16  4e-16\n",
      " 9: -1.8155e+01 -1.8303e+01  1e-01  7e-16  4e-16\n",
      "10: -1.8172e+01 -1.8263e+01  9e-02  7e-16  3e-16\n",
      "11: -1.8184e+01 -1.8232e+01  5e-02  3e-16  3e-16\n",
      "12: -1.8193e+01 -1.8214e+01  2e-02  2e-16  5e-16\n",
      "13: -1.8199e+01 -1.8206e+01  7e-03  8e-16  4e-16\n",
      "14: -1.8201e+01 -1.8202e+01  1e-03  9e-16  4e-16\n",
      "15: -1.8202e+01 -1.8202e+01  1e-04  2e-16  5e-16\n",
      "16: -1.8202e+01 -1.8202e+01  2e-06  3e-16  6e-16\n",
      "Optimal solution found.\n",
      "\n",
      "========================================\n",
      " REPORT - PART 2 (SVM Q2)\n",
      "========================================\n",
      "1. Hyperparameters Settings:\n",
      "   - Kernel: gaussian\n",
      "   - C:      0.1\n",
      "   - Gamma:  0.1\n",
      "\n",
      "2. Machine Learning Performance:\n",
      "   - Training Accuracy: 0.9163\n",
      "   - Test Accuracy:     0.9250\n",
      "\n",
      "3. Confusion Matrix (Test Set):\n",
      "[[93  7]\n",
      " [ 8 92]]\n",
      "\n",
      "4. Optimization Time:\n",
      "   - 0.7181 seconds\n",
      "\n",
      "5. Optimization Iterations:\n",
      "   - 16\n",
      "\n",
      "6. Final Dual SVM Objective Value:\n",
      "   - 18.2017\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Estrazione Features e Target\n",
    "X = df.drop('gt', axis=1).values\n",
    "y = df['gt'].values\n",
    "\n",
    "# --- PREPROCESSING CRITICO ---\n",
    "# 1. Conversione Target: SVM richiede y in {-1, 1}, il dataset ha {0, 1}\n",
    "y_svm = np.where(y == 0, -1, 1)\n",
    "print(f\"Target convertiti: {np.unique(y_svm)}\")\n",
    "\n",
    "# 2. Split Train/Test\n",
    "# Riserviamo il 20% per il test finale come richiesto per valutare la 'Test Accuracy'\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y_svm, test_size=0.2, random_state=42, stratify=y_svm\n",
    ")\n",
    "\n",
    "print(f\"Dimensioni Training Set: {X_train_full.shape}\")\n",
    "print(f\"Dimensioni Test Set: {X_test.shape}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Grid Search with K-Fold Cross Validation\n",
    "# Ricerca dei migliori iperparametri $C$ e $\\gamma$ (kernel gaussiano).\n",
    "\n",
    "# %%\n",
    "# Parametri Grid Search\n",
    "C_list = [0.1, 1, 10, 100]           # Box constraint\n",
    "gamma_list = [0.001, 0.01, 0.1, 1]   # Kernel width parameter\n",
    "kernel_chosen = 'gaussian'\n",
    "k_folds = 5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_params = {'C': 1, 'gamma': 0.1} # Default fallback\n",
    "\n",
    "results_log = []\n",
    "\n",
    "print(f\"Avvio Grid Search ({k_folds}-Fold CV)...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for C in C_list:\n",
    "    for gamma in gamma_list:\n",
    "        fold_accuracies = []\n",
    "        \n",
    "        # Inizio Cross Validation\n",
    "        for train_idx, val_idx in skf.split(X_train_full, y_train_full):\n",
    "            # Split Fold\n",
    "            X_fold_train, X_fold_val = X_train_full[train_idx], X_train_full[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train_full[train_idx], y_train_full[val_idx]\n",
    "            \n",
    "            # --- SCALING ---\n",
    "            # FIT solo sul training del fold per evitare Data Leakage\n",
    "            scaler = StandardScaler()\n",
    "            X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "            X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "            \n",
    "            # Training\n",
    "            alphas, b, sv, sv_y, sv_a, _, _ = train_svm_cvxopt(\n",
    "                X_fold_train_scaled, y_fold_train, C, kernel_chosen, gamma\n",
    "            )\n",
    "            \n",
    "            if alphas is None:\n",
    "                continue # Skip se ottimizzazione fallisce\n",
    "            \n",
    "            # Validation\n",
    "            preds = predict_svm(X_fold_val_scaled, sv, sv_y, sv_a, b, kernel_chosen, gamma)\n",
    "            acc = accuracy_score(y_fold_val, preds)\n",
    "            fold_accuracies.append(acc)\n",
    "        \n",
    "        # Media Accuracy per coppia (C, gamma)\n",
    "        if fold_accuracies:\n",
    "            avg_acc = np.mean(fold_accuracies)\n",
    "            print(f\"C={C:<5} Gamma={gamma:<5} -> Avg Val Acc: {avg_acc:.4f}\")\n",
    "            results_log.append([C, gamma, avg_acc])\n",
    "            \n",
    "            if avg_acc > best_val_acc:\n",
    "                best_val_acc = avg_acc\n",
    "                best_params = {'C': C, 'gamma': gamma}\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Migliori parametri trovati: {best_params}\")\n",
    "print(f\"Miglior Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Final Training and Report Generation\n",
    "# Addestramento sul training set completo usando i parametri ottimi.\n",
    "\n",
    "# %%\n",
    "# Recupero parametri ottimi\n",
    "final_C = best_params['C']\n",
    "final_gamma = best_params['gamma']\n",
    "\n",
    "# Scaling Finale (Fit su tutto il training, transform su test)\n",
    "final_scaler = StandardScaler()\n",
    "X_train_final_scaled = final_scaler.fit_transform(X_train_full)\n",
    "X_test_final_scaled = final_scaler.transform(X_test)\n",
    "\n",
    "print(\"Addestramento modello finale...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# --- TRAINING FINALE ---\n",
    "# Nota: La funzione ora restituisce 7 valori\n",
    "alphas, b, sv, sv_y, sv_a, n_iters, final_dual_obj = train_svm_cvxopt(\n",
    "    X_train_final_scaled, y_train_full, final_C, kernel_chosen, final_gamma\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "opt_time = end_time - start_time\n",
    "\n",
    "# --- VALUTAZIONE ---\n",
    "# Train Accuracy\n",
    "train_preds = predict_svm(X_train_final_scaled, sv, sv_y, sv_a, b, kernel_chosen, final_gamma)\n",
    "train_acc = accuracy_score(y_train_full, train_preds)\n",
    "\n",
    "# Test Accuracy\n",
    "test_preds = predict_svm(X_test_final_scaled, sv, sv_y, sv_a, b, kernel_chosen, final_gamma)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Official Results Output\n",
    "# Output formattato come richiesto nelle \"Instructions for Python code\".\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\" REPORT - PART 2 (SVM Q2)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# [cite_start]1. Setting values of the hyperparameters [cite: 146]\n",
    "print(f\"1. Hyperparameters Settings:\")\n",
    "print(f\"   - Kernel: {kernel_chosen}\")\n",
    "print(f\"   - C:      {final_C}\")\n",
    "print(f\"   - Gamma:  {final_gamma}\")\n",
    "\n",
    "# [cite_start]2. Classification rate [cite: 147]\n",
    "print(f\"\\n2. Machine Learning Performance:\")\n",
    "print(f\"   - Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"   - Test Accuracy:     {test_acc:.4f}\")\n",
    "\n",
    "# [cite_start]3. The confusion matrix [cite: 148]\n",
    "print(f\"\\n3. Confusion Matrix (Test Set):\")\n",
    "print(cm)\n",
    "# (Riga 0: Classe -1, Riga 1: Classe 1)\n",
    "\n",
    "# [cite_start]4. Optimization time [cite: 149]\n",
    "print(f\"\\n4. Optimization Time:\")\n",
    "print(f\"   - {opt_time:.4f} seconds\")\n",
    "\n",
    "# [cite_start]5. Number of optimization iterations [cite: 150]\n",
    "print(f\"\\n5. Optimization Iterations:\")\n",
    "print(f\"   - {n_iters}\")\n",
    "\n",
    "# [cite_start]6. Final value of the dual SVM objective [cite: 152]\n",
    "print(f\"\\n6. Final Dual SVM Objective Value:\")\n",
    "print(f\"   - {final_dual_obj:.4f}\")\n",
    "\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versione FIXATA della classe MVP_SVM_Solver\n",
    "# Correzione: Moltiplicazione per self.y nell'aggiornamento dei gradienti\n",
    "\n",
    "class MVP_SVM_Solver:\n",
    "    def __init__(self, X, y, C, kernel_type, hyperparam, tol=1e-3, max_iter=10000):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.C = C\n",
    "        self.kernel_type = kernel_type\n",
    "        self.hyperparam = hyperparam\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        self.n_samples = X.shape[0]\n",
    "        self.alphas = np.zeros(self.n_samples)\n",
    "        self.b = 0\n",
    "        \n",
    "        if kernel_type == 'gaussian':\n",
    "            self.K = gaussian_kernel(X, X, hyperparam)\n",
    "        else:\n",
    "            self.K = polynomial_kernel(X, X, hyperparam)\n",
    "            \n",
    "        # Gradiente iniziale: 1 per tutti (dato che alpha=0)\n",
    "        self.gradients = np.ones(self.n_samples)\n",
    "\n",
    "    def select_working_set_mvp(self):\n",
    "        # Criterio di selezione basato su y*G\n",
    "        yG = self.y * self.gradients\n",
    "        eps = 1e-5\n",
    "        \n",
    "        mask_up = np.logical_or(\n",
    "            np.logical_and(self.y == 1, self.alphas < self.C - eps),\n",
    "            np.logical_and(self.y == -1, self.alphas > eps)\n",
    "        )\n",
    "        \n",
    "        mask_down = np.logical_or(\n",
    "            np.logical_and(self.y == 1, self.alphas > eps),\n",
    "            np.logical_and(self.y == -1, self.alphas < self.C - eps)\n",
    "        )\n",
    "        \n",
    "        if not np.any(mask_up) or not np.any(mask_down):\n",
    "            return -1, -1\n",
    "\n",
    "        valid_yG_up = yG.copy()\n",
    "        valid_yG_up[~mask_up] = -np.inf\n",
    "        \n",
    "        valid_yG_down = yG.copy()\n",
    "        valid_yG_down[~mask_down] = np.inf\n",
    "        \n",
    "        i = np.argmax(valid_yG_up)\n",
    "        j = np.argmin(valid_yG_down)\n",
    "        \n",
    "        violation = valid_yG_up[i] - valid_yG_down[j]\n",
    "        if violation < self.tol:\n",
    "            return -1, -1\n",
    "            \n",
    "        return i, j\n",
    "\n",
    "    def solve(self):\n",
    "        print(f\"DEBUG: Start MVP Optimization. C={self.C}, Gamma={self.hyperparam}\")\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            i, j = self.select_working_set_mvp()\n",
    "            \n",
    "            if i == -1:\n",
    "                break\n",
    "                \n",
    "            a_i_old = self.alphas[i]\n",
    "            a_j_old = self.alphas[j]\n",
    "            y_i = self.y[i]\n",
    "            y_j = self.y[j]\n",
    "            \n",
    "            eta = self.K[i, i] + self.K[j, j] - 2 * self.K[i, j]\n",
    "            if eta <= 1e-12: eta = 1e-12 \n",
    "            \n",
    "            # Step update (segno corretto per massimizzazione)\n",
    "            step = self.y[j] * (self.y[j] * self.gradients[j] - self.y[i] * self.gradients[i]) / eta\n",
    "            \n",
    "            a_j_unc = a_j_old + step\n",
    "            \n",
    "            # Clipping\n",
    "            if y_i != y_j:\n",
    "                L = max(0, a_j_old - a_i_old)\n",
    "                H = min(self.C, self.C + a_j_old - a_i_old)\n",
    "            else:\n",
    "                L = max(0, a_j_old + a_i_old - self.C)\n",
    "                H = min(self.C, a_j_old + a_i_old)\n",
    "            \n",
    "            if H <= L + 1e-12:\n",
    "                continue\n",
    "                \n",
    "            a_j_new = np.clip(a_j_unc, L, H)\n",
    "            \n",
    "            if abs(a_j_new - a_j_old) < 1e-8:\n",
    "                continue\n",
    "                \n",
    "            a_i_new = a_i_old + y_i * y_j * (a_j_old - a_j_new)\n",
    "            \n",
    "            # --- FIX CRITICO QUI SOTTO ---\n",
    "            # Aggiornamento Gradienti: deve includere self.y!\n",
    "            # G_new = G_old - y_k * (delta_sum_term)\n",
    "            delta_i = a_i_new - a_i_old\n",
    "            delta_j = a_j_new - a_j_old\n",
    "            \n",
    "            kernel_update = delta_i * y_i * self.K[i, :] + delta_j * y_j * self.K[j, :]\n",
    "            self.gradients -= self.y * kernel_update\n",
    "            \n",
    "            self.alphas[i] = a_i_new\n",
    "            self.alphas[j] = a_j_new\n",
    "            \n",
    "        # Calcolo Bias\n",
    "        sv_idx = np.logical_and(self.alphas > 1e-5, self.alphas < self.C - 1e-5)\n",
    "        if np.any(sv_idx):\n",
    "            support_indices = np.where(self.alphas > 1e-5)[0]\n",
    "            _alphas = self.alphas[support_indices]\n",
    "            _y = self.y[support_indices]\n",
    "            k_subset = self.K[np.ix_(sv_idx, support_indices)]\n",
    "            preds_no_b = np.dot(k_subset, _alphas * _y)\n",
    "            self.b = np.mean(self.y[sv_idx] - preds_no_b)\n",
    "        else:\n",
    "            self.b = 0.0\n",
    "\n",
    "        # Calcolo Dual Objective\n",
    "        term1 = np.sum(self.alphas)\n",
    "        ay = self.alphas * self.y\n",
    "        term2 = 0.5 * np.dot(ay, np.dot(self.K, ay))\n",
    "        dual_obj = term1 - term2\n",
    "        \n",
    "        return self.alphas, self.b, iteration, dual_obj\n",
    "\n",
    "# Wrapper\n",
    "def train_svm_mvp(X, y, C, kernel_type, hyperparam, max_iter=5000):\n",
    "    solver = MVP_SVM_Solver(X, y, C, kernel_type, hyperparam, max_iter=max_iter)\n",
    "    alphas, b, iters, obj = solver.solve()\n",
    "    sv_idx = alphas > 1e-5\n",
    "    return alphas, b, X[sv_idx], y[sv_idx], alphas[sv_idx], iters, obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MVP SVM with C=0.1, Gamma=0.1...\n",
      "DEBUG: Start MVP Optimization. C=0.1, Gamma=0.1\n",
      "MVP Training completed in 0.0559s\n",
      "Iterations: 529\n",
      "Dual Objective: 18.2017\n",
      "Train Accuracy: 0.9163\n",
      "Test Accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training MVP SVM with C={best_params['C']}, Gamma={best_params['gamma']}...\")\n",
    "\n",
    "# Esegui Training\n",
    "start_time_mvp = time.time()\n",
    "alphas_mvp, b_mvp, sv_mvp, sv_y_mvp, sv_a_mvp, iters_mvp, obj_mvp = train_svm_mvp(\n",
    "    X_train_final_scaled, y_train_full, best_params['C'], kernel_chosen, best_params['gamma'], max_iter=10000\n",
    ")\n",
    "end_time_mvp = time.time()\n",
    "time_mvp = end_time_mvp - start_time_mvp\n",
    "\n",
    "# Prediction & Evaluation\n",
    "train_preds_mvp = predict_svm(X_train_final_scaled, sv_mvp, sv_y_mvp, sv_a_mvp, b_mvp, kernel_chosen, best_params['gamma'])\n",
    "test_preds_mvp = predict_svm(X_test_final_scaled, sv_mvp, sv_y_mvp, sv_a_mvp, b_mvp, kernel_chosen, best_params['gamma'])\n",
    "\n",
    "acc_train_mvp = accuracy_score(y_train_full, train_preds_mvp)\n",
    "acc_test_mvp = accuracy_score(y_test, test_preds_mvp)\n",
    "\n",
    "print(f\"MVP Training completed in {time_mvp:.4f}s\")\n",
    "print(f\"Iterations: {iters_mvp}\")\n",
    "print(f\"Dual Objective: {obj_mvp:.4f}\")\n",
    "print(f\"Train Accuracy: {acc_train_mvp:.4f}\")\n",
    "print(f\"Test Accuracy: {acc_test_mvp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_24</th>\n",
       "      <th>feat_25</th>\n",
       "      <th>feat_26</th>\n",
       "      <th>feat_27</th>\n",
       "      <th>feat_28</th>\n",
       "      <th>feat_29</th>\n",
       "      <th>feat_30</th>\n",
       "      <th>feat_31</th>\n",
       "      <th>feat_32</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.502792</td>\n",
       "      <td>-0.349373</td>\n",
       "      <td>-0.068018</td>\n",
       "      <td>-0.627533</td>\n",
       "      <td>0.130331</td>\n",
       "      <td>0.373488</td>\n",
       "      <td>-0.491088</td>\n",
       "      <td>0.416753</td>\n",
       "      <td>-0.046255</td>\n",
       "      <td>0.325566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374539</td>\n",
       "      <td>-0.605601</td>\n",
       "      <td>0.579226</td>\n",
       "      <td>-0.119241</td>\n",
       "      <td>0.185425</td>\n",
       "      <td>-0.255194</td>\n",
       "      <td>0.705415</td>\n",
       "      <td>-0.027761</td>\n",
       "      <td>0.581276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.741646</td>\n",
       "      <td>-0.240194</td>\n",
       "      <td>-0.006548</td>\n",
       "      <td>-0.639129</td>\n",
       "      <td>-0.059524</td>\n",
       "      <td>0.457087</td>\n",
       "      <td>-0.500733</td>\n",
       "      <td>0.345128</td>\n",
       "      <td>-0.040395</td>\n",
       "      <td>0.334946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353851</td>\n",
       "      <td>-0.651629</td>\n",
       "      <td>0.129014</td>\n",
       "      <td>-0.034723</td>\n",
       "      <td>0.392575</td>\n",
       "      <td>-0.289928</td>\n",
       "      <td>0.380891</td>\n",
       "      <td>-0.157109</td>\n",
       "      <td>0.742231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.836617</td>\n",
       "      <td>1.256781</td>\n",
       "      <td>2.227900</td>\n",
       "      <td>-0.603728</td>\n",
       "      <td>0.200403</td>\n",
       "      <td>1.366685</td>\n",
       "      <td>-0.666864</td>\n",
       "      <td>1.156750</td>\n",
       "      <td>-0.004516</td>\n",
       "      <td>0.767839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265453</td>\n",
       "      <td>-0.762246</td>\n",
       "      <td>-0.331476</td>\n",
       "      <td>0.051758</td>\n",
       "      <td>1.255034</td>\n",
       "      <td>-0.529064</td>\n",
       "      <td>1.412283</td>\n",
       "      <td>1.182029</td>\n",
       "      <td>3.046791</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.576996</td>\n",
       "      <td>1.784919</td>\n",
       "      <td>3.166102</td>\n",
       "      <td>-0.539903</td>\n",
       "      <td>0.108954</td>\n",
       "      <td>1.773622</td>\n",
       "      <td>-0.708488</td>\n",
       "      <td>2.679638</td>\n",
       "      <td>-0.313469</td>\n",
       "      <td>1.817676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295006</td>\n",
       "      <td>-0.866968</td>\n",
       "      <td>2.203584</td>\n",
       "      <td>-0.428082</td>\n",
       "      <td>-0.368396</td>\n",
       "      <td>-0.398743</td>\n",
       "      <td>4.043780</td>\n",
       "      <td>2.281445</td>\n",
       "      <td>5.065996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.614504</td>\n",
       "      <td>1.103544</td>\n",
       "      <td>0.755605</td>\n",
       "      <td>-0.561434</td>\n",
       "      <td>-0.551708</td>\n",
       "      <td>0.936586</td>\n",
       "      <td>-0.575693</td>\n",
       "      <td>0.533321</td>\n",
       "      <td>0.033991</td>\n",
       "      <td>0.980126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155448</td>\n",
       "      <td>-0.825574</td>\n",
       "      <td>-0.627675</td>\n",
       "      <td>0.060466</td>\n",
       "      <td>0.815617</td>\n",
       "      <td>-0.434781</td>\n",
       "      <td>0.040881</td>\n",
       "      <td>-0.152861</td>\n",
       "      <td>2.191638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>1.177667</td>\n",
       "      <td>-0.354565</td>\n",
       "      <td>0.162559</td>\n",
       "      <td>-0.679878</td>\n",
       "      <td>0.404861</td>\n",
       "      <td>0.346877</td>\n",
       "      <td>-0.553733</td>\n",
       "      <td>0.246781</td>\n",
       "      <td>0.048537</td>\n",
       "      <td>-0.178114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504359</td>\n",
       "      <td>-0.509224</td>\n",
       "      <td>-0.086053</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.898129</td>\n",
       "      <td>-0.317467</td>\n",
       "      <td>0.366268</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.474631</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>0.452397</td>\n",
       "      <td>0.127534</td>\n",
       "      <td>-0.227879</td>\n",
       "      <td>-0.432106</td>\n",
       "      <td>-0.115514</td>\n",
       "      <td>0.151127</td>\n",
       "      <td>-0.464220</td>\n",
       "      <td>-0.616083</td>\n",
       "      <td>0.124265</td>\n",
       "      <td>0.053483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316485</td>\n",
       "      <td>-0.488984</td>\n",
       "      <td>-0.792142</td>\n",
       "      <td>0.147497</td>\n",
       "      <td>0.945197</td>\n",
       "      <td>-0.424854</td>\n",
       "      <td>-0.695024</td>\n",
       "      <td>-0.504614</td>\n",
       "      <td>-0.191776</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>-0.059290</td>\n",
       "      <td>-0.364770</td>\n",
       "      <td>-0.565849</td>\n",
       "      <td>-0.505793</td>\n",
       "      <td>-0.071319</td>\n",
       "      <td>-0.170053</td>\n",
       "      <td>-0.171233</td>\n",
       "      <td>-0.537452</td>\n",
       "      <td>0.015178</td>\n",
       "      <td>-0.304598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479153</td>\n",
       "      <td>-0.346231</td>\n",
       "      <td>-0.632762</td>\n",
       "      <td>0.125567</td>\n",
       "      <td>0.824368</td>\n",
       "      <td>-0.195254</td>\n",
       "      <td>-0.574200</td>\n",
       "      <td>-0.558383</td>\n",
       "      <td>-0.464065</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>0.700352</td>\n",
       "      <td>1.173251</td>\n",
       "      <td>1.135091</td>\n",
       "      <td>-0.394454</td>\n",
       "      <td>-0.450751</td>\n",
       "      <td>0.946441</td>\n",
       "      <td>-0.551620</td>\n",
       "      <td>0.928455</td>\n",
       "      <td>-0.118895</td>\n",
       "      <td>1.698370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137823</td>\n",
       "      <td>-0.824867</td>\n",
       "      <td>0.625824</td>\n",
       "      <td>-0.212100</td>\n",
       "      <td>-0.404575</td>\n",
       "      <td>-0.394925</td>\n",
       "      <td>1.206820</td>\n",
       "      <td>0.405817</td>\n",
       "      <td>2.605891</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>0.273901</td>\n",
       "      <td>-0.273564</td>\n",
       "      <td>-0.296626</td>\n",
       "      <td>-0.576465</td>\n",
       "      <td>-0.156409</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>-0.434261</td>\n",
       "      <td>-0.031111</td>\n",
       "      <td>-0.007019</td>\n",
       "      <td>0.325198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306803</td>\n",
       "      <td>-0.636117</td>\n",
       "      <td>-0.084282</td>\n",
       "      <td>-0.033043</td>\n",
       "      <td>0.245196</td>\n",
       "      <td>-0.283487</td>\n",
       "      <td>-0.051340</td>\n",
       "      <td>-0.362853</td>\n",
       "      <td>0.270606</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
       "0     0.502792 -0.349373 -0.068018 -0.627533  0.130331  0.373488 -0.491088   \n",
       "1     0.741646 -0.240194 -0.006548 -0.639129 -0.059524  0.457087 -0.500733   \n",
       "2     2.836617  1.256781  2.227900 -0.603728  0.200403  1.366685 -0.666864   \n",
       "3     2.576996  1.784919  3.166102 -0.539903  0.108954  1.773622 -0.708488   \n",
       "4     1.614504  1.103544  0.755605 -0.561434 -0.551708  0.936586 -0.575693   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2495  1.177667 -0.354565  0.162559 -0.679878  0.404861  0.346877 -0.553733   \n",
       "2496  0.452397  0.127534 -0.227879 -0.432106 -0.115514  0.151127 -0.464220   \n",
       "2497 -0.059290 -0.364770 -0.565849 -0.505793 -0.071319 -0.170053 -0.171233   \n",
       "2498  0.700352  1.173251  1.135091 -0.394454 -0.450751  0.946441 -0.551620   \n",
       "2499  0.273901 -0.273564 -0.296626 -0.576465 -0.156409  0.270355 -0.434261   \n",
       "\n",
       "        feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26  \\\n",
       "0     0.416753 -0.046255  0.325566  ...  0.374539 -0.605601  0.579226   \n",
       "1     0.345128 -0.040395  0.334946  ...  0.353851 -0.651629  0.129014   \n",
       "2     1.156750 -0.004516  0.767839  ...  0.265453 -0.762246 -0.331476   \n",
       "3     2.679638 -0.313469  1.817676  ...  0.295006 -0.866968  2.203584   \n",
       "4     0.533321  0.033991  0.980126  ...  0.155448 -0.825574 -0.627675   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2495  0.246781  0.048537 -0.178114  ...  0.504359 -0.509224 -0.086053   \n",
       "2496 -0.616083  0.124265  0.053483  ...  0.316485 -0.488984 -0.792142   \n",
       "2497 -0.537452  0.015178 -0.304598  ...  0.479153 -0.346231 -0.632762   \n",
       "2498  0.928455 -0.118895  1.698370  ...  0.137823 -0.824867  0.625824   \n",
       "2499 -0.031111 -0.007019  0.325198  ...  0.306803 -0.636117 -0.084282   \n",
       "\n",
       "       feat_27   feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
       "0    -0.119241  0.185425 -0.255194  0.705415 -0.027761  0.581276   0  \n",
       "1    -0.034723  0.392575 -0.289928  0.380891 -0.157109  0.742231   0  \n",
       "2     0.051758  1.255034 -0.529064  1.412283  1.182029  3.046791   0  \n",
       "3    -0.428082 -0.368396 -0.398743  4.043780  2.281445  5.065996   0  \n",
       "4     0.060466  0.815617 -0.434781  0.040881 -0.152861  2.191638   0  \n",
       "...        ...       ...       ...       ...       ...       ...  ..  \n",
       "2495  0.089109  0.898129 -0.317467  0.366268  0.003271  0.474631   4  \n",
       "2496  0.147497  0.945197 -0.424854 -0.695024 -0.504614 -0.191776   4  \n",
       "2497  0.125567  0.824368 -0.195254 -0.574200 -0.558383 -0.464065   4  \n",
       "2498 -0.212100 -0.404575 -0.394925  1.206820  0.405817  2.605891   4  \n",
       "2499 -0.033043  0.245196 -0.283487 -0.051340 -0.362853  0.270606   4  \n",
       "\n",
       "[2500 rows x 33 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='/Users/jacopocaldana/Desktop/Università/Optimization/final_project/dataset/ETHNICITY_CLASSIFICATION.csv'\n",
    "df_eth=pd.read_csv('/Users/jacopocaldana/Desktop/Università/Optimization/final_project/dataset/ETHNICITY_CLASSIFICATION.csv')\n",
    "df_eth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt\n",
      "0    500\n",
      "1    500\n",
      "2    500\n",
      "3    500\n",
      "4    500\n",
      "Name: count, dtype: int64\n",
      "Dataset ridotto a 3 classi: (1500, 32)\n",
      "Inizio training Multiclass OVA...\n",
      "  Training Model for Class 0 vs Rest...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.0998e+02 -2.6452e+03  1e+04  3e+00  2e-15\n",
      " 1: -1.3039e+02 -1.5173e+03  2e+03  2e-01  1e-15\n",
      " 2: -1.3884e+02 -3.5825e+02  2e+02  2e-02  3e-15\n",
      " 3: -1.6097e+02 -2.2914e+02  7e+01  4e-03  2e-15\n",
      " 4: -1.6884e+02 -1.9379e+02  3e+01  9e-04  1e-15\n",
      " 5: -1.7201e+02 -1.8071e+02  9e+00  2e-04  1e-15\n",
      " 6: -1.7342e+02 -1.7650e+02  3e+00  1e-05  1e-15\n",
      " 7: -1.7392e+02 -1.7520e+02  1e+00  6e-15  1e-15\n",
      " 8: -1.7421e+02 -1.7455e+02  3e-01  7e-16  1e-15\n",
      " 9: -1.7428e+02 -1.7442e+02  1e-01  3e-15  1e-15\n",
      "10: -1.7432e+02 -1.7435e+02  3e-02  4e-15  1e-15\n",
      "11: -1.7433e+02 -1.7433e+02  2e-03  1e-14  1e-15\n",
      "12: -1.7433e+02 -1.7433e+02  4e-05  1e-14  1e-15\n",
      "Optimal solution found.\n",
      "  Training Model for Class 1 vs Rest...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.8109e+02 -2.5370e+03  1e+04  3e+00  2e-15\n",
      " 1: -1.0895e+02 -1.3981e+03  2e+03  1e-01  1e-15\n",
      " 2: -1.1929e+02 -3.2657e+02  2e+02  2e-02  2e-15\n",
      " 3: -1.3755e+02 -2.0590e+02  7e+01  4e-03  1e-15\n",
      " 4: -1.4494e+02 -1.6763e+02  2e+01  9e-04  1e-15\n",
      " 5: -1.4789e+02 -1.5690e+02  9e+00  6e-05  1e-15\n",
      " 6: -1.4936e+02 -1.5180e+02  2e+00  6e-06  1e-15\n",
      " 7: -1.4984e+02 -1.5067e+02  8e-01  1e-14  1e-15\n",
      " 8: -1.5001e+02 -1.5032e+02  3e-01  1e-15  1e-15\n",
      " 9: -1.5011e+02 -1.5015e+02  5e-02  3e-15  1e-15\n",
      "10: -1.5012e+02 -1.5013e+02  2e-02  3e-15  1e-15\n",
      "11: -1.5012e+02 -1.5013e+02  2e-03  1e-14  1e-15\n",
      "12: -1.5012e+02 -1.5012e+02  4e-05  7e-15  1e-15\n",
      "Optimal solution found.\n",
      "  Training Model for Class 2 vs Rest...\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.7870e+02 -2.6318e+03  1e+04  3e+00  2e-15\n",
      " 1: -1.1038e+02 -1.4914e+03  2e+03  2e-01  1e-15\n",
      " 2: -1.1707e+02 -3.7891e+02  3e+02  2e-02  1e-15\n",
      " 3: -1.3976e+02 -2.1435e+02  8e+01  4e-03  2e-15\n",
      " 4: -1.4735e+02 -1.7993e+02  3e+01  1e-03  1e-15\n",
      " 5: -1.5161e+02 -1.6223e+02  1e+01  3e-04  1e-15\n",
      " 6: -1.5298e+02 -1.5852e+02  6e+00  2e-05  1e-15\n",
      " 7: -1.5392e+02 -1.5587e+02  2e+00  7e-06  1e-15\n",
      " 8: -1.5429e+02 -1.5504e+02  7e-01  1e-06  1e-15\n",
      " 9: -1.5447e+02 -1.5469e+02  2e-01  1e-07  1e-15\n",
      "10: -1.5453e+02 -1.5460e+02  7e-02  8e-15  1e-15\n",
      "11: -1.5456e+02 -1.5456e+02  5e-03  4e-15  1e-15\n",
      "12: -1.5456e+02 -1.5456e+02  8e-05  6e-15  1e-15\n",
      "Optimal solution found.\n",
      "Training completato in 3.3481s\n",
      "\n",
      "=== REPORT BONUS QUESTION 4 ===\n",
      "Classes used: [0, 1, 2]\n",
      "Multiclass Accuracy: 0.8867\n",
      "Confusion Matrix:\n",
      "[[90  5  5]\n",
      " [ 6 90  4]\n",
      " [10  4 86]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Bonus Question 4: Multiclass SVM (Ethnicity)\n",
    "# **Strategy:** One-Against-All (OVA)\n",
    "# **Classes:** Selecting 3 specific ethnicities.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 2. Selezione di 3 Classi\n",
    "# Vediamo la distribuzione\n",
    "print(df_eth['gt'].value_counts())\n",
    "\n",
    "# Selezioniamo le classi 0, 1 e 2 (o quelle più numerose)\n",
    "target_classes = [0, 1, 2]\n",
    "df_3class = df_eth[df_eth['gt'].isin(target_classes)].copy()\n",
    "\n",
    "X = df_3class.drop('gt', axis=1).values\n",
    "y = df_3class['gt'].values\n",
    "\n",
    "print(f\"Dataset ridotto a 3 classi: {X.shape}\")\n",
    "\n",
    "# 3. Split e Scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Training One-Against-All (OVA)\n",
    "# Train 3 binary classifiers:\n",
    "# * Model 0: Class 0 vs (1, 2)\n",
    "# * Model 1: Class 1 vs (0, 2)\n",
    "# * Model 2: Class 2 vs (0, 1)\n",
    "\n",
    "# %%\n",
    "# Parametri (riusiamo quelli buoni della Q2 o valori standard)\n",
    "C_multi = 1.0     \n",
    "gamma_multi = 0.1 \n",
    "kernel_multi = 'gaussian'\n",
    "\n",
    "models = {}\n",
    "\n",
    "print(\"Inizio training Multiclass OVA...\")\n",
    "start_time_total = time.time()\n",
    "\n",
    "for cls in target_classes:\n",
    "    print(f\"  Training Model for Class {cls} vs Rest...\")\n",
    "    \n",
    "    # Preparazione Target Binario: 1 se è la classe corrente, -1 altrimenti\n",
    "    y_binary = np.where(y_train == cls, 1, -1)\n",
    "    \n",
    "    # Train\n",
    "    alphas, b, sv, sv_y, sv_a, iters, obj = train_svm_cvxopt(\n",
    "        X_train_scaled, y_binary, C=C_multi, kernel_type=kernel_multi, hyperparam=gamma_multi\n",
    "    )\n",
    "    \n",
    "    # Salviamo il modello\n",
    "    models[cls] = {\n",
    "        'alphas': sv_a,\n",
    "        'b': b,\n",
    "        'sv': sv,\n",
    "        'sv_y': sv_y\n",
    "    }\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"Training completato in {end_time_total - start_time_total:.4f}s\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Prediction & Evaluation\n",
    "# Per ogni punto di test, calcoliamo il punteggio (decision function) di tutti e 3 i modelli.\n",
    "# La classe predetta è quella con il punteggio più alto (argmax).\n",
    "\n",
    "# %%\n",
    "def predict_multiclass(X, models, kernel_type, hyperparam):\n",
    "    n_samples = X.shape[0]\n",
    "    scores = np.zeros((n_samples, len(models)))\n",
    "    classes = list(models.keys())\n",
    "    \n",
    "    # Calcolo score per ogni classe\n",
    "    # Score = sum(alpha * y * K(sv, x)) + b\n",
    "    # Nota: NON usiamo il segno (sign), ma il valore grezzo (distanza dall'iperpiano)\n",
    "    \n",
    "    for idx, cls in enumerate(classes):\n",
    "        model = models[cls]\n",
    "        \n",
    "        if kernel_type == 'gaussian':\n",
    "            # Riusiamo la funzione kernel definita nel file .py o qui\n",
    "            \n",
    "            K = gaussian_kernel(model['sv'], X, hyperparam)\n",
    "        else:\n",
    "            \n",
    "            K = polynomial_kernel(model['sv'], X, hyperparam)\n",
    "            \n",
    "        # Decision function: (1, n_sv) @ (n_sv, n_samples) -> (1, n_samples)\n",
    "        # Broadcasting corretto\n",
    "        decision = np.sum(K * model['alphas'].reshape(-1, 1) * model['sv_y'].reshape(-1, 1), axis=0) + model['b']\n",
    "        scores[:, idx] = decision\n",
    "        \n",
    "    # Argmax per scegliere la classe vincente\n",
    "    predictions_indices = np.argmax(scores, axis=1)\n",
    "    return [classes[i] for i in predictions_indices]\n",
    "\n",
    "# Esecuzione predizione\n",
    "y_pred_multi = predict_multiclass(X_test_scaled, models, kernel_multi, gamma_multi)\n",
    "\n",
    "# Metriche\n",
    "acc_multi = accuracy_score(y_test, y_pred_multi)\n",
    "conf_mat_multi = confusion_matrix(y_test, y_pred_multi)\n",
    "\n",
    "print(\"\\n=== REPORT BONUS QUESTION 4 ===\")\n",
    "print(f\"Classes used: {target_classes}\")\n",
    "print(f\"Multiclass Accuracy: {acc_multi:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat_multi)\n",
    "\n",
    "# Visualizzazione (Opzionale)\n",
    "sns.heatmap(conf_mat_multi, annot=True, fmt='d', cmap='Greens', xticklabels=target_classes, yticklabels=target_classes)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Multiclass SVM Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
